From f7bb9338c953deaae6cbadf10ec9f77fb9cc667c Mon Sep 17 00:00:00 2001
From: Miguel Bernal Marin <miguel.bernal.marin@linux.intel.com>
Date: Mon, 20 Aug 2018 11:56:13 -0500
Subject: [PATCH 8008/8021] misc: intel: sepdk: add C source files from vtsspp

---
 drivers/misc/intel/sepdk/vtsspp/apic.c        |  375 ++
 drivers/misc/intel/sepdk/vtsspp/bts.c         |  283 ++
 drivers/misc/intel/sepdk/vtsspp/collector.c   | 3155 +++++++++++++++++
 drivers/misc/intel/sepdk/vtsspp/cpuevents.c   | 1071 ++++++
 .../misc/intel/sepdk/vtsspp/cpuevents_knx.c   |  170 +
 .../misc/intel/sepdk/vtsspp/cpuevents_p6.c    |  255 ++
 .../misc/intel/sepdk/vtsspp/cpuevents_sys.c   |  343 ++
 .../sepdk/vtsspp/cpumask_parselist_user.c     |  162 +
 drivers/misc/intel/sepdk/vtsspp/dsa.c         |  321 ++
 drivers/misc/intel/sepdk/vtsspp/globals.c     |  506 +++
 drivers/misc/intel/sepdk/vtsspp/ipt.c         |  867 +++++
 drivers/misc/intel/sepdk/vtsspp/lbr.c         |  385 ++
 drivers/misc/intel/sepdk/vtsspp/memory_pool.c |  746 ++++
 drivers/misc/intel/sepdk/vtsspp/module.c      |  941 +++++
 drivers/misc/intel/sepdk/vtsspp/nmiwd.c       |  236 ++
 drivers/misc/intel/sepdk/vtsspp/pebs.c        |  308 ++
 drivers/misc/intel/sepdk/vtsspp/procfs.c      | 1115 ++++++
 drivers/misc/intel/sepdk/vtsspp/record.c      |  834 +++++
 drivers/misc/intel/sepdk/vtsspp/stack.c       |  899 +++++
 drivers/misc/intel/sepdk/vtsspp/task_map.c    |  326 ++
 drivers/misc/intel/sepdk/vtsspp/transport.c   | 2187 ++++++++++++
 drivers/misc/intel/sepdk/vtsspp/uec.c         |  633 ++++
 drivers/misc/intel/sepdk/vtsspp/unwind.c      |  685 ++++
 drivers/misc/intel/sepdk/vtsspp/user_vm.c     |  736 ++++
 24 files changed, 17539 insertions(+)
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/apic.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/bts.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/collector.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/cpuevents.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/cpuevents_knx.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/cpuevents_p6.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/cpuevents_sys.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/cpumask_parselist_user.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/dsa.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/globals.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/ipt.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/lbr.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/memory_pool.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/module.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/nmiwd.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/pebs.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/procfs.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/record.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/stack.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/task_map.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/transport.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/uec.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/unwind.c
 create mode 100644 drivers/misc/intel/sepdk/vtsspp/user_vm.c

diff --git a/drivers/misc/intel/sepdk/vtsspp/apic.c b/drivers/misc/intel/sepdk/vtsspp/apic.c
new file mode 100644
index 000000000000..056a1e586489
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/apic.c
@@ -0,0 +1,375 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "apic.h"
+#include "globals.h"
+
+#include <asm/io.h>
+
+/* APIC base MSR */
+#define VTSS_APIC_BASE_MSR       0x01b
+
+/* x2APIC MSRs */
+#define VTSS_APIC_LCL_ID_MSR     0x802
+#define VTSS_APIC_LCL_TSKPRI_MSR 0x808
+#define VTSS_APIC_LCL_PPR_MSR    0x80a
+#define VTSS_APIC_LCL_EOI_MSR    0x80b
+#define VTSS_APIC_LCL_LDEST_MSR  0x80d
+#define VTSS_APIC_LCL_DSTFMT_MSR 0x80e
+#define VTSS_APIC_LCL_SVR_MSR    0x80f
+#define VTSS_APIC_LCL_ICR_MSR    0x830
+#define VTSS_APIC_LVT_TIMER_MSR  0x832
+#define VTSS_APIC_LVT_PMI_MSR    0x834
+#define VTSS_APIC_LVT_LINT0_MSR  0x835
+#define VTSS_APIC_LVT_LINT1_MSR  0x836
+#define VTSS_APIC_LVT_ERROR_MSR  0x837
+
+/* APIC registers */
+#define VTSS_APIC_LCL_ID     0x0020
+#define VTSS_APIC_LCL_TSKPRI 0x0080
+#define VTSS_APIC_LCL_PPR    0x00a0
+#define VTSS_APIC_LCL_EOI    0x00b0
+#define VTSS_APIC_LCL_LDEST  0x00d0
+#define VTSS_APIC_LCL_DSTFMT 0x00e0
+#define VTSS_APIC_LCL_SVR    0x00f0
+#define VTSS_APIC_LCL_ICR    0x0300
+#define VTSS_APIC_LVT_TIMER  0x0320
+#define VTSS_APIC_LVT_PMI    0x0340
+#define VTSS_APIC_LVT_LINT0  0x0350
+#define VTSS_APIC_LVT_LINT1  0x0360
+#define VTSS_APIC_LVT_ERROR  0x0370
+
+/* masks for LVT */
+#define LVT_MASK   0x10000
+#define LVT_EDGE   0x00000
+#define LVT_LEVEL  0x08000
+#define LVT_EXTINT 0x00700
+#define LVT_NMI    0x00400
+
+/* task priorities */
+#define VTSS_APIC_TSKPRI_LO 0x0000
+#define VTSS_APIC_TSKPRI_HI 0x00f0
+
+#define VTSS_X2APIC_ENABLED 0x0c00ULL
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+#ifdef CONFIG_RESOURCES_64BIT
+typedef u64 resource_size_t;
+#else
+typedef u32 resource_size_t;
+#endif
+#endif /* 2.6.27 */
+
+#ifndef cpu_has_x2apic
+#define cpu_has_x2apic          boot_cpu_has(X86_FEATURE_X2APIC)
+#endif
+
+static int vtss_x2apic_mode = 0;
+static void* vtss_apic_physical_addr = NULL;
+static char* vtss_apic_linear_addr = NULL;
+static atomic_t  vtss_apic_pmi_state_active = ATOMIC_INIT(0);
+
+extern void vtss_io_delay(void);
+
+/* unmask PMI */
+void vtss_pmi_enable(void)
+{
+#ifdef VTSS_USE_NMI
+    apic_write(APIC_LVTPC, APIC_DM_NMI);
+#else
+    if (atomic_read(&vtss_apic_pmi_state_active)){
+        if (vtss_x2apic_mode) {
+            long long msr_val;
+
+            rdmsrl(VTSS_APIC_LVT_PMI_MSR, msr_val);
+            msr_val &= ~LVT_MASK;
+            wrmsrl(VTSS_APIC_LVT_PMI_MSR, msr_val);
+        } else {
+            char *apic = (char*)pcb_cpu.apic_linear_addr;
+
+            if (apic != NULL) {
+                *((int*)&apic[VTSS_APIC_LVT_PMI]) &= ~LVT_MASK;
+            }
+        }
+        if (!atomic_read(&vtss_apic_pmi_state_active)){
+            INFO("ERROR!!! possibly enabled after stopping");
+        }
+    }
+#endif
+}
+
+/* mask PMI */
+void vtss_pmi_disable(void)
+{
+#ifndef VTSS_USE_NMI
+    if (vtss_x2apic_mode) {
+        wrmsrl(VTSS_APIC_LVT_PMI_MSR, (CPU_PERF_VECTOR | LVT_MASK));
+    } else {
+        char *apic = (char*)pcb_cpu.apic_linear_addr;
+
+        if (apic != NULL) {
+            *((int*)&apic[VTSS_APIC_LVT_PMI]) = (CPU_PERF_VECTOR | LVT_MASK);
+        }
+    }
+#endif
+}
+
+unsigned long long  vtss_apic_read_lvt_pmi(void)
+{
+    unsigned long long val = (unsigned long long)-1;
+    if (vtss_x2apic_mode) {
+        rdmsrl(VTSS_APIC_LVT_PMI_MSR, val);
+    } else {
+        char *apic = (char*)pcb_cpu.apic_linear_addr;
+
+        if (apic != NULL) {
+            val = *((int*)&apic[VTSS_APIC_LVT_PMI]);
+        }
+    }
+    return val;
+}
+
+static void vtss_apic_pmi_disable_on_cpu(void *ctx)
+{
+    vtss_pmi_disable();
+}
+
+void vtss_apic_pmi_init(void)
+{
+    atomic_set(&vtss_apic_pmi_state_active, 1);
+
+    preempt_disable();
+    vtss_pmi_enable();
+    preempt_enable_no_resched();
+}
+
+void vtss_apic_pmi_fini(void)
+{
+    atomic_set(&vtss_apic_pmi_state_active, 0);
+    on_each_cpu(vtss_apic_pmi_disable_on_cpu, NULL, SMP_CALL_FUNCTION_ARGS);
+//    vtss_apic_pmi_disable_on_cpu(NULL);
+}
+
+/* acknowledge end of interrupt */
+void vtss_apic_ack_eoi(void)
+{
+    if (vtss_x2apic_mode) {
+        wrmsrl(VTSS_APIC_LCL_EOI_MSR, 0LL);
+    } else {
+        char *apic = (char*)pcb_cpu.apic_linear_addr;
+
+        if (apic != NULL) {
+            *((int*)&apic[VTSS_APIC_LCL_EOI]) = 0;
+        }
+    }
+}
+unsigned long long  vtss_apic_read_eoi(void)
+{
+    unsigned long long val = (unsigned long long)-1;
+    if (vtss_x2apic_mode) {
+        rdmsrl(VTSS_APIC_LCL_EOI_MSR, val);
+    } else {
+        char *apic = (char*)pcb_cpu.apic_linear_addr;
+
+        if (apic != NULL) {
+            val = *((int*)&apic[VTSS_APIC_LCL_EOI]);
+        }
+    }
+    return val;
+}
+
+/*void ack_apic_pmi()
+{
+    if(vtss_x2apic_mode){
+        wrmsrl(VTSS_APIC_LVT_PMI_MSR, CPU_PERF_VECTOR);
+    }
+    else{
+        char* apic = (char*)pcb[curr_cpu()].apic_linear_addr;
+        *(int*)&apic[VTSS_APIC_LVT_PMI] = CPU_PERF_VECTOR;
+    }
+}*/
+/* read the contents of Processor Priority Register */
+int vtss_apic_read_priority(void)
+{
+    if (vtss_x2apic_mode) {
+        long long msr_val;
+
+        rdmsrl(VTSS_APIC_LCL_PPR_MSR, msr_val);
+        return (int)msr_val;
+    } else {
+        char *apic = (char*)pcb_cpu.apic_linear_addr;
+
+        return (apic != NULL) ? *((int*)&apic[VTSS_APIC_LCL_PPR]) : 0;
+    }
+}
+
+/* collect APIC physical addresses */
+static void vtss_apic_init_phase1(void *ctx)
+{
+    unsigned long long addr;
+
+    rdmsrl(VTSS_APIC_BASE_MSR, addr);
+    if (cpu_has_x2apic && (addr & VTSS_X2APIC_ENABLED) == VTSS_X2APIC_ENABLED) {
+        vtss_x2apic_mode = 1;
+        rdmsrl(VTSS_APIC_LCL_ID_MSR, addr);
+        /* the most significant byte contains the ID */
+        pcb_cpu.apic_id = (addr >> 24);
+        TRACE("x2apic_enabled=1, apic_id=%d", pcb_cpu.apic_id);
+    } else {
+        pcb_cpu.apic_physical_addr = (void*)(size_t)addr;
+        TRACE("apic_physical_addr=0x%p", pcb_cpu.apic_physical_addr);
+        if (vtss_apic_physical_addr == NULL) {
+            vtss_apic_physical_addr = (void*)(size_t)addr;
+        }
+    }
+}
+
+/* map APIC physical address to logical address space */
+static void vtss_apic_init_phase2(void *ctx)
+{
+    if (vtss_apic_linear_addr != NULL) {
+        pcb_cpu.apic_linear_addr = vtss_apic_linear_addr;
+        /* the most significant byte contains the ID */
+        pcb_cpu.apic_id = *((int*)&apic[VTSS_APIC_LCL_ID]) >> 24;
+        TRACE("apic_linear_addr=0x%p, apic_id=%d", pcb_cpu.apic_linear_addr, pcb_cpu.apic_id);
+    } else {
+        ERROR("ioremap_nocache() failed");
+    }
+}
+
+/* enable local APIC for SP systems */
+#if 0
+static void vtss_apic_init_phase3(void* ctx)
+{
+    unsigned long flags;
+    char *apic = (char*)pcb_cpu.apic_linear_addr;
+    unsigned long long addr = (unsigned long long)(size_t)pcb_cpu.apic_physical_addr;
+
+    if (!(VTSS_APIC_BASE_GLOBAL_ENABLED(addr)) || !(VTSS_APIC_VIRTUAL_WIRE_ENABLED(*((int *)&apic[VTSS_APIC_LCL_SVR])))) {
+        char pic0, pic1;
+
+        /* setup virtual wire */
+        local_irq_save(flags);
+
+        /* mask PICs */
+        pic0 = inb(0x21);
+        vtss_io_delay();
+        pic1 = inb(0xA1);
+        vtss_io_delay();
+        outb(0xFF, 0xA1);
+        vtss_io_delay();
+        outb(0xFF, 0x21);
+
+        local_irq_enable();
+        vtss_io_delay();
+        vtss_io_delay();
+        local_irq_disable();
+
+        /* enable via VTSS_APIC_BASE_MSR */
+        addr |= (1 << 11);
+        TRACE("set APIC_BASE_MSR to 0x%llx", addr);
+        wrmsrl(VTSS_APIC_BASE_MSR, addr);
+
+        /* enable in SVR and set VW */
+        *((int*)&apic[VTSS_APIC_LCL_SVR])    = 0x01FF;
+        *((int*)&apic[VTSS_APIC_LCL_TSKPRI]) = VTSS_APIC_TSKPRI_HI;
+        *((int*)&apic[VTSS_APIC_LCL_ID])     = 0;
+        *((int*)&apic[VTSS_APIC_LCL_LDEST])  = 0;        /* set local dest. id */
+        *((int*)&apic[VTSS_APIC_LCL_DSTFMT]) = -1;       /* set dest. format   */
+        *((int*)&apic[VTSS_APIC_LVT_TIMER])  = LVT_MASK; /* mask local timer   */
+        *((int*)&apic[VTSS_APIC_LVT_ERROR])  = LVT_MASK; /* mask error         */
+        /* INTs are redirected to PICs */
+        *((int*)&apic[VTSS_APIC_LVT_LINT0])  = LVT_EXTINT + LVT_EDGE;
+        *((int*)&apic[VTSS_APIC_LVT_LINT1])  = LVT_NMI + LVT_LEVEL;
+        *((int*)&apic[VTSS_APIC_LCL_TSKPRI]) = VTSS_APIC_TSKPRI_LO;
+
+        /* unmask PICs */
+        outb(pic0, 0x21);
+        outb(pic1, 0xA1);
+
+        local_irq_restore(flags);
+    }
+}
+#endif
+
+void vtss_apic_map(void)
+{
+    if (vtss_apic_linear_addr == NULL) {
+        TRACE("Map apic now!");
+        vtss_apic_linear_addr =
+            ioremap_nocache((resource_size_t)((size_t)vtss_apic_physical_addr & 0x00000000fffff000ULL),
+                            (resource_size_t)0x1000);
+    }
+}
+
+void vtss_apic_init(void)
+{
+    TRACE("IDT vector 0x%u is used for PMU interrupts.", CPU_PERF_VECTOR);
+    /* 1. collect APIC physical addresses or check for x2apic */
+    on_each_cpu(vtss_apic_init_phase1, NULL, SMP_CALL_FUNCTION_ARGS);
+
+    /* 2.a map APIC physical address to logical address space */    
+    /* 2.b initialize PMI entry in LVT for MP systems */
+    if (!vtss_x2apic_mode) {
+        vtss_apic_map();
+        on_each_cpu(vtss_apic_init_phase2, NULL, SMP_CALL_FUNCTION_ARGS);
+    }
+
+    /* 3. enable local APIC for SP systems */
+    //on_each_cpu(vtss_apic_init_phase3, NULL, SMP_CALL_FUNCTION_ARGS);
+    /* 4. disable PMI on each CPU */
+    on_each_cpu(vtss_apic_pmi_disable_on_cpu, NULL, SMP_CALL_FUNCTION_ARGS);
+
+    TRACE("APIC init");
+}
+
+void vtss_apic_unmap()
+{
+    TRACE("Unmap apic now!\n");
+    vtss_apic_physical_addr = NULL;
+    iounmap(vtss_apic_linear_addr);
+    vtss_apic_linear_addr = NULL;
+}
+
+/* unmap APIC logical address range */
+static void vtss_apic_fini_phase1(void *ctx)
+{
+    pcb_cpu.apic_linear_addr = NULL;
+}
+
+void vtss_apic_fini(void)
+{
+    on_each_cpu(vtss_apic_pmi_disable_on_cpu, NULL, SMP_CALL_FUNCTION_ARGS);
+    if (!vtss_x2apic_mode) {
+        TRACE("!vtss_x2apic_mode");
+        /* unmap APIC logical address range */
+        on_each_cpu(vtss_apic_fini_phase1, NULL, SMP_CALL_FUNCTION_ARGS);
+        vtss_apic_unmap();
+    }
+    TRACE("APIC fini");
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/bts.c b/drivers/misc/intel/sepdk/vtsspp/bts.c
new file mode 100644
index 000000000000..ef53ca80407e
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/bts.c
@@ -0,0 +1,283 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "bts.h"
+#include "dsa.h"
+#include "globals.h"
+#include "record.h"
+
+#include <linux/slab.h>
+
+#define DEBUGCTL_MSR        0x01d9
+#define BTS_ENABLE_MASK_P4  0x003c
+#define BTS_ENABLE_MASK_P6  0x03c0  ///0x01c0
+
+static int vtss_bts_count = VTSS_BTS_MIN;
+static DEFINE_PER_CPU_SHARED_ALIGNED(vtss_bts_t*, vtss_bts_per_cpu);
+
+int vtss_bts_overflowed(int cpu)
+{
+    vtss_dsa_t* dsa = vtss_dsa_get(cpu);
+
+    if (IS_DSA_64ON32) {
+        return (dsa->v32.bts_index >= dsa->v32.bts_threshold);
+    } else {
+        return (dsa->v64.bts_index >= dsa->v64.bts_threshold);
+    }
+}
+
+void vtss_bts_enable(void)
+{
+    unsigned long long msr_val;
+    if (hardcfg.family == 0x06 || hardcfg.family == 0x0f) {
+        rdmsrl(DEBUGCTL_MSR, msr_val);
+        msr_val |= (hardcfg.family == 0x0f) ? BTS_ENABLE_MASK_P4 : BTS_ENABLE_MASK_P6;
+        wrmsrl(DEBUGCTL_MSR, msr_val);
+    }
+}
+
+void vtss_bts_disable(void)
+{
+    unsigned long long msr_val;
+
+    if (hardcfg.family == 0x06 || hardcfg.family == 0x0f) {
+        rdmsrl(DEBUGCTL_MSR, msr_val);
+        msr_val &= (hardcfg.family == 0x0f) ? ~BTS_ENABLE_MASK_P4 : ~BTS_ENABLE_MASK_P6;
+        wrmsrl(DEBUGCTL_MSR, msr_val);
+    }
+}
+
+unsigned short vtss_bts_dump(unsigned char *bts_buff)
+{
+    unsigned char *dst;
+    size_t offset, value;
+    int i, j, sign, prefix, cpu;
+    char *src, *src_end;
+    vtss_dsa_t *dsa;
+
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+    dsa = vtss_dsa_get(cpu);
+
+    src     = (char*)(IS_DSA_64ON32 ? dsa->v32.bts_base  : dsa->v64.bts_base);
+    src_end = (char*)(IS_DSA_64ON32 ? dsa->v32.bts_index : dsa->v64.bts_index);
+    for (dst = bts_buff, offset = 0; ((dst - bts_buff) < (VTSS_BTS_MAX*sizeof(vtss_bts_t) - sizeof(size_t))) && (src < src_end); src = (IS_DSA_64ON32) ? src + 6*sizeof(void*) : src + 3*sizeof(void*)) {
+        for (i = 0; i < 2; i++) {
+            /// BTS structures are always 64-bit on Merom
+            if (IS_DSA_64ON32) {
+                value  = ((size_t*)src)[i << 1];
+                value -= offset;
+                offset = ((size_t*)src)[i << 1];
+                prefix = (int)((size_t)((vtss_bts_t*)src)->v32.prediction << 3);
+            } else {
+                value  = ((size_t*)src)[i];
+                value -= offset;
+                offset = ((size_t*)src)[i];
+                prefix = (int)((size_t)((vtss_bts_t*)src)->v64.prediction << 3);
+            }
+            sign = (value & (((size_t)1) << ((sizeof(size_t) << 3) - 1))) ? 0xff : 0;
+            for (j = sizeof(size_t) - 1; j >= 0; j--) {
+                if (((value >> (j << 3)) & 0xff) != sign) {
+                    break;
+                }
+            }
+            prefix |= sign ? 0x40 : 0;
+            prefix |= j + 1;
+            *dst++ = (unsigned char)prefix;
+            for (; j >= 0; j--) {
+                *dst++ = (unsigned char)(value & 0xff);
+                value >>= 8;
+            }
+        }
+    }
+    return (unsigned short)(size_t)(dst-bts_buff);
+}
+
+#ifdef VTSS_CONFIG_KPTI
+#include <asm/cpu_entry_area.h>
+
+#define BTS_BUFFER_SIZE (PAGE_SIZE << 4)
+
+static DEFINE_PER_CPU(void*, vtss_bts_vaddr);
+
+static int vtss_bts_alloc_buffer(int cpu)
+{
+    void *cea;
+    void *buffer;
+
+    per_cpu(vtss_bts_vaddr, cpu) = NULL;
+    per_cpu(vtss_bts_per_cpu, cpu) = NULL;
+    cea = &get_cpu_entry_area(cpu)->cpu_debug_buffers.bts_buffer;
+    buffer = vtss_cea_alloc_pages(BTS_BUFFER_SIZE, GFP_KERNEL, cpu);
+    if (unlikely(!buffer)) {
+        ERROR("Cannot allocate BTS buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+    per_cpu(vtss_bts_vaddr, cpu) = buffer;
+    vtss_cea_update(cea, buffer, BTS_BUFFER_SIZE, PAGE_KERNEL);
+    per_cpu(vtss_bts_per_cpu, cpu) = (vtss_bts_t*)cea;
+    TRACE("allocated buffer for %d cpu cea=%p, vaddr=%p", cpu, cea, buffer);
+    return 0;
+}
+
+static void vtss_bts_release_buffer(int cpu)
+{
+    void *cea;
+    void *buffer;
+
+    buffer = per_cpu(vtss_bts_vaddr, cpu);
+    cea = &get_cpu_entry_area(cpu)->cpu_debug_buffers.bts_buffer;
+    vtss_cea_clear(cea, BTS_BUFFER_SIZE);
+    vtss_cea_free_pages(buffer, BTS_BUFFER_SIZE);
+    TRACE("released buffer for %d cpu cea=%p, vaddr=%p", cpu, cea, buffer);
+}
+
+#elif defined(VTSS_CONFIG_KAISER)
+
+static int vtss_bts_alloc_buffer(int cpu)
+{
+    void *buffer;
+
+    per_cpu(vtss_bts_per_cpu, cpu) = NULL;
+    buffer = vtss_kaiser_alloc_pages(vtss_bts_count*sizeof(vtss_bts_t), GFP_KERNEL, cpu);
+    if (unlikely(!buffer)) {
+        ERROR("Cannot allocate BTS buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+    per_cpu(vtss_bts_per_cpu, cpu) = buffer;
+    TRACE("allocated buffer for %d cpu, buffer=%p", cpu, buffer);
+    return 0;
+}
+
+static void vtss_bts_release_buffer(int cpu)
+{
+    void *buffer;
+
+    buffer = per_cpu(vtss_bts_per_cpu, cpu);
+    vtss_kaiser_free_pages(buffer, vtss_bts_count*sizeof(vtss_bts_t));
+    TRACE("released buffer for %d cpu, buffer=%p", cpu, buffer);
+}
+
+#else
+
+static int vtss_bts_alloc_buffer(int cpu)
+{
+    per_cpu(vtss_bts_per_cpu, cpu) = NULL;
+    if ((per_cpu(vtss_bts_per_cpu, cpu) = (vtss_bts_t*)kmalloc_node(
+            vtss_bts_count*sizeof(vtss_bts_t), (GFP_KERNEL | __GFP_ZERO), cpu_to_node(cpu))) == NULL)
+    {
+        ERROR("Cannot allocate BTS buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+    return 0;
+}
+
+static void vtss_bts_release_buffer(int cpu)
+{
+    if (per_cpu(vtss_bts_per_cpu, cpu) != NULL)
+        kfree(per_cpu(vtss_bts_per_cpu, cpu));
+}
+#endif
+
+/* initialize BTS in DSA for the processor */
+void vtss_bts_init_dsa(void)
+{
+    size_t recsize;
+    void *threshold, *absmax;
+    int cpu;
+    vtss_dsa_t* dsa;
+    vtss_bts_t* bts;
+
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+    dsa = vtss_dsa_get(cpu);
+    bts = per_cpu(vtss_bts_per_cpu, cpu);
+
+    /* BTS structures are always 64-bit on Merom */
+    recsize   = IS_DSA_64ON32 ? sizeof(bts->v32) : sizeof(bts->v64);
+    absmax    = (void*)((size_t)bts + (vtss_bts_count - 1) * recsize + 1);
+    threshold = (void*)((size_t)bts + (vtss_bts_count / 4 * 3 * recsize));
+
+    if (IS_DSA_64ON32) {
+        dsa->v32.bts_base      = bts;
+        dsa->v32.bts_pad0      = NULL;
+        dsa->v32.bts_index     = bts;
+        dsa->v32.bts_pad1      = NULL;
+        dsa->v32.bts_absmax    = absmax;
+        dsa->v32.bts_pad2      = NULL;
+        dsa->v32.bts_threshold = threshold;
+        dsa->v32.bts_pad3      = NULL;
+    } else {
+        dsa->v64.bts_base      = bts;
+        dsa->v64.bts_index     = bts;
+        dsa->v64.bts_absmax    = absmax;
+        dsa->v64.bts_threshold = threshold;
+    }
+}
+
+static void vtss_bts_on_each_cpu_func(void* ctx)
+{
+    vtss_bts_disable();
+}
+
+int vtss_bts_init(int brcount)
+{
+    int cpu;
+    /* Fix count of branches */
+    brcount = (brcount < VTSS_BTS_MIN) ? VTSS_BTS_MIN : brcount;
+    brcount = (brcount > VTSS_BTS_MAX) ? VTSS_BTS_MAX : brcount;
+#ifdef BTS_BUFFER_SIZE
+    if (brcount > (BTS_BUFFER_SIZE/sizeof(vtss_bts_t)))
+        brcount = BTS_BUFFER_SIZE/sizeof(vtss_bts_t);
+#endif
+    vtss_bts_count = brcount;
+    for_each_possible_cpu(cpu) {
+        if (vtss_bts_alloc_buffer(cpu)) goto fail;
+    }
+    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_BRANCH)
+        INFO("BTS: count of branches: %d", vtss_bts_count);
+    return 0;
+
+fail:
+    for_each_possible_cpu(cpu) {
+        vtss_bts_release_buffer(cpu);
+    }
+    return VTSS_ERR_NOMEMORY;
+}
+
+void vtss_bts_fini(void)
+{
+    int cpu;
+
+    on_each_cpu(vtss_bts_on_each_cpu_func, NULL, SMP_CALL_FUNCTION_ARGS);
+    for_each_possible_cpu(cpu) {
+        vtss_bts_release_buffer(cpu);
+    }
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/collector.c b/drivers/misc/intel/sepdk/vtsspp/collector.c
new file mode 100644
index 000000000000..d5b401f4d713
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/collector.c
@@ -0,0 +1,3155 @@
+/*  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "collector.h"
+#include "globals.h"
+#include "transport.h"
+#include "procfs.h"
+#include "module.h"
+#include "record.h"
+#include "stack.h"
+#include "apic.h"
+#include "cpuevents.h"
+#include "dsa.h"
+#include "bts.h"
+#include "ipt.h"
+#include "lbr.h"
+#include "pebs.h"
+#include "time.h"
+#include "nmiwd.h"
+#include "memory_pool.h"
+
+#include <linux/nmi.h>
+#include <linux/spinlock.h>
+#include <linux/hardirq.h>
+#include <linux/sched.h>
+#include <linux/file.h>
+#include <linux/cred.h>         /* for current_uid_gid() */
+#include <linux/pid.h>
+#include <linux/dcache.h>
+#include <linux/module.h>
+#include <linux/workqueue.h>
+#include <linux/preempt.h>
+#include <linux/delay.h>        /* for msleep_interruptible() */
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/kallsyms.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+#include <linux/sched/task.h>
+#include <linux/sched/task_stack.h>
+#include <linux/sched/mm.h>
+#include <linux/sched/signal.h>
+#endif
+#include <asm/kexec.h>
+#include <asm/pgtable.h>
+#include <asm/fixmap.h>         /* VSYSCALL_START */
+#include <asm/page.h>
+#include <asm/elf.h>
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,32)
+#include <xen/xen.h>
+#endif
+
+#define DEBUG_COLLECTOR TRACE
+
+#ifndef KERNEL_IMAGE_SIZE
+#define KERNEL_IMAGE_SIZE (512 * 1024 * 1024)
+#endif
+
+#ifndef MODULES_VADDR
+#define MODULES_VADDR VMALLOC_START
+#endif
+
+#define SAFE       1
+#define NOT_SAFE   0
+#define IN_IRQ     1
+#define NOT_IN_IRQ 0
+
+#define VTSS_MIN_STACK_SPACE 1024
+
+#ifdef VTSS_AUTOCONF_DPATH_PATH
+#include <linux/path.h>
+#define D_PATH(vm_file, name, maxlen) d_path(&((vm_file)->f_path), (name), (maxlen))
+#else
+#define D_PATH(vm_file, name, maxlen) d_path((vm_file)->f_path.dentry, (vm_file)->f_vfsmnt, (name), (maxlen))
+#endif
+
+/* Only live tasks with mm and state == TASK_RUNNING | TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE */
+#define VTSS_IS_VALID_TASK(task) ((task->mm || (task->flags & PF_KTHREAD)) && (task)->state < 4 && (task)->exit_state == 0)
+
+static const char* state_str[4] = { "STOPPED", "INITING", "RUNNING", "PAUSED" };
+
+#define VTSS_EVENT_LOST_MODULE_ADDR 2
+static const char VTSS_EVENT_LOST_MODULE_NAME[] = "Events Lost On Trace Overflow";
+
+#define vtss_cpu_active(cpu) cpumask_test_cpu((cpu), &vtss_collector_cpumask)
+
+#define VTSS_RET_CANCEL 1 //Cancel scheduled work
+
+extern size_t vtss_pebs_record_size;
+
+atomic_t vtss_collector_state = ATOMIC_INIT(VTSS_COLLECTOR_STOPPED);
+
+static cpumask_t vtss_collector_cpumask = CPU_MASK_NONE;
+static atomic_t  vtss_target_count      = ATOMIC_INIT(0);
+static atomic_t  vtss_start_paused      = ATOMIC_INIT(0);
+static uid_t     vtss_session_uid       = 0;
+static gid_t     vtss_session_gid       = 0;
+
+atomic_t  vtss_mmap_reg_callcnt  = ATOMIC_INIT(0);
+
+// collector cannot work if transport uninitialized as  pointer "task->trnd" (transport)
+// is not set to NULL  during tranport "fini".
+
+static atomic_t  vtss_transport_state = ATOMIC_INIT(0);
+static atomic_t  vtss_transport_busy = ATOMIC_INIT(0); //check if we can remove this
+static atomic_t  vtss_events_enabling = ATOMIC_INIT(0);
+
+static atomic_t vtss_kernel_task_in_progress = ATOMIC_INIT(0);
+
+#if (!defined(VTSS_USE_UEC)) && (LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0))
+int vtss_need_switch_off_tracing = 0;
+#endif
+unsigned int vtss_client_major_ver = 0;
+unsigned int vtss_client_minor_ver = 0;
+
+struct vtss_target_list_item
+{
+    struct list_head list;
+    int pid;
+    int cnt;
+    int cnt_done;
+};
+
+#ifdef VTSS_CONFIG_REALTIME
+static DEFINE_RAW_SPINLOCK(vtss_target_temp_list_lock);
+#else
+static DEFINE_SPINLOCK(vtss_target_temp_list_lock);
+#endif
+static LIST_HEAD(vtss_target_temp_list);
+
+
+
+#define VTSS_ST_NEWTASK    (1<<0)
+#define VTSS_ST_SOFTCFG    (1<<1)
+#define VTSS_ST_SWAPIN     (1<<2)
+#define VTSS_ST_SWAPOUT    (1<<3)
+/*-----------------------------*/
+#define VTSS_ST_SAMPLE     (1<<4)
+#define VTSS_ST_STKDUMP    (1<<5)
+#define VTSS_ST_STKSAVE    (1<<6)
+#define VTSS_ST_PAUSE      (1<<7)
+/*-----------------------------*/
+#define VTSS_ST_IN_CONTEXT (1<<8)
+#define VTSS_ST_IN_SYSCALL (1<<9)
+#define VTSS_ST_CPUEVT     (1<<10)
+#define VTSS_ST_COMPLETE   (1<<11)
+/*-----------------------------*/
+#define VTSS_ST_NOTIFIER   (1<<12)
+#define VTSS_ST_PMU_SET    (1<<13)
+#define VTSS_ST_MMAP_INIT  (1<<14)  //modules currently writing to the trace first time in different thread.
+/*-----------------------------*/
+//#define VTSS_ST_REC_CTX    (1<<15)  //recorded context switch for calculatiing time, even if we do not need them in result.
+//#define VTSS_ST_CPU_CHANGE (1<<16)  //recorded context switch for calculatiing time, even if we do not need them in result.
+
+static const char* task_state_str[] = {
+    "-NEWTASK-",
+    "-SOFTCFG-",
+    "-SWAPIN-",
+    "-SWAPOUT-",
+    "-SAMPLE-",
+    "-STACK_DUMP-",
+    "-STACK_SAVE-",
+    "PAUSE",
+    "RUNNING",
+    "IN_SYSCALL",
+    "CPUEVT",
+    "(COMPLETE)",
+    "(NOTIFIER)",
+    "(PMU_SET)",
+    "(MMAP_INIT)",
+};
+
+#define VTSS_IN_CONTEXT(x)            ((x)->state & VTSS_ST_IN_CONTEXT)
+#define VTSS_IN_SYSCALL(x)            ((x)->state & VTSS_ST_IN_SYSCALL)
+#define VTSS_IN_NEWTASK(x)            (!((x)->state & (VTSS_ST_NEWTASK | VTSS_ST_SOFTCFG)))
+#define VTSS_IS_CPUEVT(x)             ((x)->state & VTSS_ST_CPUEVT)
+#define VTSS_IS_COMPLETE(x)           ((x)->state & VTSS_ST_COMPLETE)
+#define VTSS_IS_NOTIFIER(x)           ((x)->state & VTSS_ST_NOTIFIER)
+#define VTSS_IS_PMU_SET(x)            ((x)->state & VTSS_ST_PMU_SET)
+
+#define VTSS_IS_STATE_SET(x,st)      ((x)->state & st)
+
+
+#define VTSS_IS_MMAP_INIT(x)          ((x)->state & VTSS_ST_MMAP_INIT)
+#define VTSS_SET_MMAP_INIT(x)         (x)->state |= VTSS_ST_MMAP_INIT
+#define VTSS_CLEAR_MMAP_INIT(x)       (x)->state &= ~VTSS_ST_MMAP_INIT
+
+#define VTSS_NEED_STORE_NEWTASK(x)    ((x)->state & VTSS_ST_NEWTASK)
+#define VTSS_NEED_STORE_SOFTCFG(x)    (((x)->state & (VTSS_ST_NEWTASK | VTSS_ST_SOFTCFG)) == VTSS_ST_SOFTCFG)
+#define VTSS_NEED_STORE_PAUSE(x)      ((((x)->state & (VTSS_ST_NEWTASK | VTSS_ST_SOFTCFG | VTSS_ST_PAUSE)) == VTSS_ST_PAUSE) && (atomic_read(&vtss_collector_state)== VTSS_COLLECTOR_PAUSED) )
+#define VTSS_NEED_STACK_SAVE(x)       (((x)->state & (VTSS_ST_STKDUMP | VTSS_ST_STKSAVE)) == VTSS_ST_STKSAVE)
+
+#define VTSS_ERROR_STORE_SAMPLE(x)    ((x)->state & VTSS_ST_SAMPLE)
+#define VTSS_ERROR_STORE_SWAPIN(x)    ((x)->state & VTSS_ST_SWAPIN)
+#define VTSS_ERROR_STORE_SWAPOUT(x)   ((x)->state & VTSS_ST_SWAPOUT)
+#define VTSS_ERROR_STACK_DUMP(x)      ((x)->state & VTSS_ST_STKDUMP)
+#define VTSS_ERROR_STACK_SAVE(x)      ((x)->state & VTSS_ST_STKSAVE)
+
+#define VTSS_STORE_STATE(x,c,y)       ((x)->state = (c) ? (x)->state | (y) : (x)->state & ~(y))
+
+#define VTSS_STORE_NEWTASK(x,f)       VTSS_STORE_STATE((x), vtss_record_thread_create((x)->trnd, (x)->tid, (x)->pid, (x)->cpu, (f)), VTSS_ST_NEWTASK)
+#define VTSS_STORE_SOFTCFG(x,f)       VTSS_STORE_STATE((x), vtss_record_softcfg((x)->trnd, (x)->tid, (f)), VTSS_ST_SOFTCFG)
+#define VTSS_STORE_PAUSE(x,cpu,i,f)   VTSS_STORE_STATE((x), vtss_record_probe((x)->trnd, (cpu), (i), (f)), VTSS_ST_PAUSE)
+
+#define VTSS_STORE_SAMPLE(x,cpu,ip,f) VTSS_STORE_STATE((x), vtss_record_sample(VTSS_PT_FLUSH_MODE ? (x)->trnd_aux : (x)->trnd, (x)->tid, (cpu), (x)->cpuevent_chain, (ip), (f), &(x)->start_rec_id), VTSS_ST_SAMPLE)
+#define VTSS_STORE_SWAPIN(x,cpu,ip,f) VTSS_STORE_STATE((x), vtss_record_switch_to(VTSS_PT_FLUSH_MODE ? (x)->trnd_aux : (x)->trnd, (x)->tid, (cpu), (ip), (f), &(x)->start_rec_id), VTSS_ST_SWAPIN)
+#define VTSS_STORE_SWAPOUT(x,p,f)     VTSS_STORE_STATE((x), vtss_record_switch_from(VTSS_PT_FLUSH_MODE ? (x)->trnd_aux : (x)->trnd, (x)->cpu, (p), (f), &(x)->start_rec_id), VTSS_ST_SWAPOUT)
+
+#ifdef VTSS_SYSCALL_TRACE
+#define VTSS_STACK_DUMP(x,t,r,bp,f)   VTSS_STORE_STATE((x), vtss_stack_dump(VTSS_PT_FLUSH_MODE ? (x)->trnd_aux : (x)->trnd, &((x)->stk), (t), (r), (bp), (x)->syscall_sp, (f)), VTSS_ST_STKDUMP)
+#else
+#define VTSS_STACK_DUMP(x,t,r,bp,f)   VTSS_STORE_STATE((x), vtss_stack_dump(VTSS_PT_FLUSH_MODE ? (x)->trnd_aux : (x)->trnd, &((x)->stk), (t), (r), (bp), NULL, (f)), VTSS_ST_STKDUMP)
+#endif
+#define VTSS_STACK_SAVE(x,f)          VTSS_STORE_STATE((x), vtss_stack_record(VTSS_PT_FLUSH_MODE ? (x)->trnd_aux : (x)->trnd, &((x)->stk), (x)->tid, (x)->cpu, (f), &(x)->start_rec_id), VTSS_ST_STKSAVE)
+
+/* Replace definition above with following to turn off functionality: */
+//#define VTSS_STORE_SAMPLE(x,cpu,ip,f) VTSS_STORE_STATE((x), 0, VTSS_ST_SAMPLE)
+//#define VTSS_STORE_SWAPIN(x,cpu,ip,f) VTSS_STORE_STATE((x), 0, VTSS_ST_SWAPIN)
+//#define VTSS_STORE_SWAPOUT(x,p,f)     VTSS_STORE_STATE((x), 0, VTSS_ST_SWAPOUT)
+
+//#define VTSS_STACK_DUMP(x,t,r,bp,f)   VTSS_STORE_STATE((x), 0, VTSS_ST_STKDUMP)
+//#define VTSS_STACK_SAVE(x,f)          VTSS_STORE_STATE((x), 0, VTSS_ST_STKSAVE)
+
+struct vtss_task_data
+{
+    stack_control_t  stk;
+    lbr_control_t    lbr;
+    vtss_tcb_t       tcb;
+    struct vtss_transport_data* trnd;
+    struct vtss_transport_data* trnd_aux;
+#if defined(CONFIG_PREEMPT_NOTIFIERS) && defined(VTSS_USE_PREEMPT_NOTIFIERS)
+    struct preempt_notifier preempt_notifier;
+#endif
+    unsigned int     state;
+    int              m32;
+    pid_t            tid;
+    pid_t            pid;
+    pid_t            ppid;
+    unsigned int     cpu;
+    void*            ip;
+#ifdef VTSS_SYSCALL_TRACE
+    void*            syscall_sp;
+    unsigned long long syscall_enter;
+#endif
+#ifndef VTSS_NO_BTS
+    unsigned short   bts_size;
+    unsigned char    bts_buff[VTSS_BTS_MAX*sizeof(vtss_bts_t)];
+#endif
+    char             filename[VTSS_FILENAME_SIZE];
+    char             taskname[VTSS_TASKNAME_SIZE];
+    cpuevent_t       cpuevent_chain[VTSS_CFG_CHAIN_SIZE];
+    void*            from_ip;
+    unsigned long start_rec_id;
+};
+
+static int vtss_mmap_all(struct vtss_task_data*, struct task_struct*);
+static int vtss_kmap_all(struct vtss_task_data*);
+
+#ifndef VTSS_USE_NMI
+#define VTSS_RECOVERY_LOGIC
+#endif
+
+#ifdef VTSS_RECOVERY_LOGIC
+#ifdef VTSS_CONFIG_REALTIME
+static DEFINE_RAW_SPINLOCK(vtss_recovery_lock);
+#else
+static DEFINE_SPINLOCK(vtss_recovery_lock);
+#endif
+
+static DEFINE_PER_CPU_SHARED_ALIGNED(struct vtss_task_data*, vtss_recovery_tskd);
+#endif
+
+#if defined(CONFIG_PREEMPT_NOTIFIERS) && defined(VTSS_USE_PREEMPT_NOTIFIERS)
+static void vtss_notifier_sched_in (struct preempt_notifier *notifier, int cpu);
+static void vtss_notifier_sched_out(struct preempt_notifier *notifier, struct task_struct *next);
+
+static struct preempt_ops vtss_preempt_ops = {
+    .sched_in  = vtss_notifier_sched_in,
+    .sched_out = vtss_notifier_sched_out
+};
+#endif
+
+#ifdef VTSS_GET_TASK_STRUCT
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39)
+#include <linux/kallsyms.h>
+#include <linux/kprobes.h>
+typedef void (vtss__put_task_struct_t) (struct task_struct *tsk);
+static vtss__put_task_struct_t* vtss__put_task_struct = NULL;
+
+static struct kprobe _kp_dummy = {
+    .pre_handler = NULL,
+    .post_handler = NULL,
+    .fault_handler = NULL,
+#ifdef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+    .symbol_name = "__put_task_struct",
+#endif
+    .addr = (kprobe_opcode_t*)NULL
+};
+
+static inline void vtss_put_task_struct(struct task_struct *task)
+{
+    if (atomic_dec_and_test(&task->usage))
+        vtss__put_task_struct(task);
+}
+#else  /* LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39) */
+#define vtss_put_task_struct put_task_struct
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39) */
+
+#endif /* VTSS_GET_TASK_STRUCT */
+
+int  vtss_target_new(pid_t tid, pid_t pid, pid_t ppid, const char* filename, int fired_tid, int fired_order);
+int  vtss_target_del(vtss_task_map_item_t* item);
+
+static struct task_struct* vtss_find_task_by_tid(pid_t tid)
+{
+    struct task_struct *task = NULL;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,31)
+    struct pid *p_pid = NULL;
+    p_pid = find_get_pid(tid);
+    if (p_pid) {
+        rcu_read_lock();
+        task = pid_task(p_pid, PIDTYPE_PID);
+        rcu_read_unlock();
+        put_pid(p_pid);
+    }
+#else /* < 2.6.31 */
+    rcu_read_lock();
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+    task = find_task_by_vpid(tid);
+#else /* < 2.6.24 */
+    task = find_task_by_pid(tid);
+#endif /* 2.6.24 */
+    rcu_read_unlock();
+#endif /* 2.6.31 */
+    return task;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,0,0)
+static char *vtss_get_task_comm(char *taskname, struct task_struct *task)
+{
+    size_t size = 0;
+    task_lock(task);
+    size =  min((size_t)VTSS_TASKNAME_SIZE-1, (size_t)strlen(task->comm));
+    strncpy(taskname, task->comm, size);
+    taskname[size]='\0';
+    task_unlock(task);
+    return taskname;
+}
+#else  /* LINUX_VERSION_CODE < KERNEL_VERSION(3,0,0) */
+#define vtss_get_task_comm(name, task) get_task_comm(name, task)
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(3,0,0) */
+
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_cmd_stop_work(struct work_struct *work)
+#else
+static void vtss_cmd_stop_work(void *work)
+#endif
+{
+    vtss_cmd_stop();
+    vtss_kfree(work);
+}
+
+int vtss_queue_work(int cpu, vtss_work_func_t* func, void* data, size_t size)
+{
+    struct vtss_work* my_work = 0;
+
+    my_work = (struct vtss_work*)vtss_kmalloc(sizeof(struct vtss_work)+size, GFP_ATOMIC);
+
+    if (my_work != NULL) {
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+        INIT_WORK((struct work_struct*)my_work, func);
+#else
+        INIT_WORK((struct work_struct*)my_work, func, my_work);
+#endif
+        if (data != NULL && size > 0)
+            memcpy(&my_work->data, data, size);
+
+#ifdef VTSS_AUTOCONF_SYSTEM_UNBOUND_WQ
+
+#ifdef VTSS_CONFIG_REALTIME
+            queue_work(system_unbound_wq, (struct work_struct*)my_work);
+#else
+        if (cpu < 0) {
+            queue_work(system_unbound_wq, (struct work_struct*)my_work);
+        } else {
+            queue_work_on(cpu, system_unbound_wq, (struct work_struct*)my_work);
+        }
+#endif //VTSS_CONFIG_REALTIME
+
+#else  /* VTSS_AUTOCONF_SYSTEM_UNBOUND_WQ */
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+        if (cpu < 0) {
+            schedule_work((struct work_struct*)my_work);
+        } else {
+            schedule_work_on(cpu, (struct work_struct*)my_work);
+        }
+#else  /* VTSS_AUTOCONF_INIT_WORK_TWO_ARGS */
+        /* Don't support queue work on cpu */
+        schedule_work((struct work_struct*)my_work);
+#endif /* VTSS_AUTOCONF_INIT_WORK_TWO_ARGS */
+#endif /* VTSS_AUTOCONF_SYSTEM_UNBOUND_WQ */
+    } else {
+        ERROR("No memory for my_work");
+        return -ENOMEM;
+    }
+    return 0;
+}
+inline int is_aux_transport_allowed(void)
+{
+    return (vtss_client_major_ver > 1 || (vtss_client_major_ver ==1 && vtss_client_minor_ver >=1));
+}
+inline int is_aux_transport_ring_buffer(void)
+{
+    return (is_aux_transport_allowed() && VTSS_PT_FLUSH_MODE) ? VTSS_TR_MODE_RB : VTSS_TR_MODE_REG;
+}
+
+static void vtss_target_ctrl_wakeup(struct vtss_transport_data* trnd, struct vtss_transport_data* trnd_aux)
+{
+    if (trnd){
+        if (trnd_aux && trnd_aux != trnd) {
+            char *transport_path_aux = vtss_transport_get_filename(trnd_aux);
+            TRACE("wakeup for %s\n", transport_path_aux);
+            vtss_procfs_ctrl_wake_up(transport_path_aux, strlen(transport_path_aux) + 1);
+        } else {
+            char *transport_path = vtss_transport_get_filename(trnd);
+            TRACE("wakeup for %s", transport_path);
+            vtss_procfs_ctrl_wake_up(transport_path, strlen(transport_path) + 1);
+        }
+    }
+}
+
+static void vtss_target_transport_create(struct vtss_transport_data** trnd, struct vtss_transport_data** trnd_aux, pid_t ppid, pid_t pid, uid_t cuid, gid_t cgid)
+{
+    if (!trnd) return;
+    *trnd = vtss_transport_create(ppid, pid, cuid, cgid);
+    if (!(*trnd)) return;
+    if (trnd_aux){
+        if (is_aux_transport_allowed()){
+            *trnd_aux = vtss_transport_create_aux(*trnd, cuid, cgid, is_aux_transport_ring_buffer());
+        }
+        else{
+            *trnd_aux = NULL;
+        }
+    }
+    vtss_target_ctrl_wakeup(*trnd, *trnd_aux);
+}
+
+void vtss_target_del_empty_transport(struct vtss_transport_data* trnd, struct vtss_transport_data* trnd_aux)
+{
+    if (!trnd && !trnd_aux) return;
+
+    atomic_inc(&vtss_transport_busy);
+    if (atomic_read(&vtss_transport_state) == 1) {
+        if (trnd != NULL) {
+            if (trnd == trnd_aux) trnd_aux = NULL;
+            if (trnd_aux && vtss_transport_delref(trnd_aux) == 0) {
+                vtss_transport_complete(trnd_aux);
+                TRACE("COMPLETE empty aux transport");
+            }
+            if (trnd && vtss_transport_delref(trnd) == 0) {
+                vtss_transport_complete(trnd);
+                TRACE("COMPLETE empty transport");
+            }
+            /* NOTE: tskd->trnd will be destroyed in vtss_transport_fini() */
+        }
+    }
+    atomic_dec(&vtss_transport_busy);
+}
+
+struct vtss_target_list_item* vtss_target_find_in_temp_list_not_save(int pid)
+{
+    struct vtss_target_list_item* it = NULL;
+    struct vtss_target_list_item* ret = NULL;
+    struct list_head* p = NULL;
+    list_for_each(p, &vtss_target_temp_list) {
+        it = list_entry(p, struct vtss_target_list_item, list);
+        if (it->pid == pid){
+            ret = it;
+            break;
+        }
+    }
+    return ret;
+}
+
+void vtss_target_remove_from_temp_list(int pid, int failed)
+{
+    unsigned long flags;
+    struct vtss_target_list_item* it = NULL;
+    struct list_head* p = NULL;
+    struct list_head* tmp = NULL;
+    vtss_spin_lock_irqsave(&vtss_target_temp_list_lock, flags);
+    list_for_each_safe(p, tmp, &vtss_target_temp_list) {
+        it = list_entry(p, struct vtss_target_list_item, list);
+        if (it->pid == pid){
+            if (it->cnt == it->cnt_done){
+                DEBUG_COLLECTOR("deleted completely from list, pid = %d, cnt = %d", pid, it->cnt);
+                list_del(p);
+                vtss_kfree(it);
+            } else if (it->cnt < it->cnt_done){
+                ERROR("ERROR in list!!!");
+            } else {
+                if (failed) it->cnt--; //queue_work failed. do it as it was before.
+                else it->cnt_done++;
+                DEBUG_COLLECTOR("deleted from list, pid = %d, cnt_done = %d, cnt = %d", pid, it->cnt_done, it->cnt);
+            }
+            break;
+        }
+    }
+    vtss_spin_unlock_irqrestore(&vtss_target_temp_list_lock, flags);
+}
+
+int vtss_target_add_to_temp_list(int pid)
+{
+    unsigned long flags;
+    struct vtss_target_list_item* item = NULL;
+    
+    vtss_spin_lock_irqsave(&vtss_target_temp_list_lock, flags);
+    item = vtss_target_find_in_temp_list_not_save(pid);
+    if (item){
+        int cnt;
+        item->cnt++;
+        cnt = item->cnt;
+        vtss_spin_unlock_irqrestore(&vtss_target_temp_list_lock, flags);
+        DEBUG_COLLECTOR("added to list item %d, cnt %d", pid, cnt);
+        return cnt;
+    }
+    item = (struct vtss_target_list_item*)vtss_kmalloc(sizeof(struct vtss_target_list_item), GFP_KERNEL);
+    if (!item) {
+        vtss_spin_unlock_irqrestore(&vtss_target_temp_list_lock, flags);
+        ERROR("ERROR: No memory");
+        return -1;
+    }
+    item->pid = pid;
+    item->cnt = 0;
+    item->cnt_done = 0;
+    list_add_tail(&item->list, &vtss_target_temp_list);
+    vtss_spin_unlock_irqrestore(&vtss_target_temp_list_lock, flags);
+    DEBUG_COLLECTOR("added item pid %d, first time", pid);
+    return 0;
+}
+
+int vtss_target_find_in_temp_list(int pid)
+{
+    unsigned long flags;
+    struct vtss_target_list_item* it;
+    struct list_head* p;
+    int ret = -1;
+    vtss_spin_lock_irqsave(&vtss_target_temp_list_lock, flags);
+    list_for_each(p, &vtss_target_temp_list) {
+        it = list_entry(p, struct vtss_target_list_item, list);
+        if (it->pid == pid){
+            ret = it->cnt_done;
+            break;
+        }
+    }
+    vtss_spin_unlock_irqrestore(&vtss_target_temp_list_lock, flags);
+    return ret;
+}
+
+void vtss_target_clear_temp_list(void)
+{
+    unsigned long flags;
+    struct vtss_target_list_item* it;
+    struct list_head* p, *tmp;
+    vtss_spin_lock_irqsave(&vtss_target_temp_list_lock, flags);
+    list_for_each_safe(p, tmp, &vtss_target_temp_list) {
+        it = list_entry(p, struct vtss_target_list_item, list);
+        DEBUG_COLLECTOR("free pid = %d", it->pid);
+        list_del(p);
+        vtss_kfree(it);
+    }
+    INIT_LIST_HEAD(&vtss_target_temp_list);
+    vtss_spin_unlock_irqrestore(&vtss_target_temp_list_lock, flags);
+}
+
+int vtss_target_wait_work_time(int id, int order)
+{
+    int cnt = 100000;
+    int num = -1;
+    if (order == -1){
+        return 1;
+    }
+    while (cnt--){
+        num = vtss_target_find_in_temp_list(id);
+        if (num == -1){
+            //finished!
+            return 1;
+        }
+        if (num > order){
+             return 1;
+        }
+        if (!irqs_disabled())
+        {
+            msleep_interruptible(1);
+        }
+        else
+        {
+            touch_nmi_watchdog();
+        }
+    }
+    if(cnt <= 0) ERROR("Can not wait work %d with order %d, num = %d", id, order, num);
+    return 0;
+}
+
+#if 0
+struct vtss_target_exit_data
+{
+    pid_t fired_tid;
+    pid_t fired_order;
+};
+
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_target_exit_work(struct work_struct *work)
+#else
+static void vtss_target_exit_work(void *work)
+#endif
+{
+    struct vtss_work* my_work = NULL;
+    struct vtss_target_exit_data* data = NULL;
+    vtss_task_map_item_t* item = NULL;
+    struct vtss_task_data* tskd = NULL;
+    if (!work){
+        ERROR ("Internal error: exit have no any work to do");
+        return;
+    }
+
+    DEBUG_COLLECTOR("state = %d", atomic_read(&vtss_collector_state));
+
+    atomic_inc(&vtss_kernel_task_in_progress);
+    DEBUG_COLLECTOR("after inc vtss_kernel_task_in_progress = %d", atomic_read(&vtss_kernel_task_in_progress));
+
+    if (atomic_read(&vtss_collector_state) == VTSS_COLLECTOR_STOPPED){ // It's OK to call this function in (un)initialization state
+        ERROR("Internal error: exit work is called after collection was stopped");
+    }
+    my_work = (struct vtss_work*)work;
+    data = (struct vtss_target_exit_data*)(my_work->data);
+    if (data){
+        DEBUG_COLLECTOR("before wait work time, tid = %d, fired order = %d", (int)data->fired_tid, data->fired_order);
+        vtss_target_wait_work_time(data->fired_tid, data->fired_order);
+        DEBUG_COLLECTOR("after wait work time, tid = %d, fired order = %d", (int)data->fired_tid, data->fired_order);
+        item = vtss_task_map_get_item(data->fired_tid);
+        if (item){
+            tskd = (struct vtss_task_data*)&item->data;
+            DEBUG_COLLECTOR("(%d:%d): data=0x%p, u=%d, n=%d",
+                tskd->tid, tskd->pid, tskd, atomic_read(&item->usage), atomic_read(&vtss_target_count));
+            /* release data */
+            vtss_target_del(item);
+            vtss_task_map_put_item(item);
+        } else {
+            ERROR("Exit work failed find item");
+        }
+    }
+
+    atomic_dec(&vtss_kernel_task_in_progress);
+    DEBUG_COLLECTOR("after dec vtss_kernel_task_in_progress = %d", atomic_read(&vtss_kernel_task_in_progress));
+
+    vtss_target_remove_from_temp_list(data->fired_tid, 0);
+    vtss_kfree(work);
+}
+#endif
+
+#if 0
+struct vtss_target_exec_attach_data
+{
+    struct task_struct *task;
+    char filename[VTSS_FILENAME_SIZE];
+    char config[VTSS_FILENAME_SIZE];
+    struct vtss_transport_data* new_trnd;
+    struct vtss_transport_data* new_trnd_aux;
+};
+
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_target_exec_attach_work(struct work_struct *work)
+#else
+static void vtss_target_exec_attach_work(void *work)
+#endif
+{
+    int rc;
+    struct vtss_work* my_work = (struct vtss_work*)work;
+    struct vtss_target_exec_attach_data* data = (struct vtss_target_exec_attach_data*)(&my_work->data);
+    if (!my_work){
+        ERROR ("Internal error: exec have no any work to do");
+        return;
+    }
+    if (VTSS_COLLECTOR_IS_READY_OR_INITING){
+        data = (struct vtss_target_exec_attach_data*)(&my_work->data);
+        rc = vtss_target_new(TASK_TID(data->task), TASK_PID(data->task), TASK_PID(TASK_PARENT(data->task)), data->filename, -1, -1);
+        if (rc) {
+            TRACE("(%d:%d): Error in vtss_target_new()=%d", TASK_TID(data->task), TASK_PID(data->task), rc);
+            vtss_target_del_empty_transport(data->new_trnd, data->new_trnd_aux);
+        }
+    }
+    vtss_kfree(work);
+}
+#endif
+
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_target_add_mmap_work(struct work_struct *work)
+#else
+static void vtss_target_add_mmap_work(void *work)
+#endif
+{
+    //This function load module map in the case if smth was wrong during first time loading
+    //This is workaround on the problem:
+    //During attach the "transfer loop" is not activated till the collection started.
+    //The ring buffer is overflow on module loading as nobody reads it and for huge module maps we have unknowns.
+    //So, we have to schedule the new task and try again.
+    struct vtss_work* my_work = (struct vtss_work*)work;
+    vtss_task_map_item_t* item = NULL;
+    struct vtss_task_data* tskd = NULL;
+    struct task_struct* task = NULL;
+    struct vtss_transport_data* trnd = NULL;
+
+    if (my_work == NULL){
+        ERROR("Internal error: vtss_target_add_map_work: work == NULL");
+        atomic_dec(&vtss_kernel_task_in_progress);
+        return;
+    }
+    item = *((vtss_task_map_item_t**)(&my_work->data));
+    if (item == NULL){
+        ERROR("Internal error: vtss_target_add_map_work: item == NULL");
+        goto out1;
+    }
+    DEBUG_COLLECTOR("after inc vtss_kernel_task_in_progress = %d", atomic_read(&vtss_kernel_task_in_progress));
+    if (atomic_read(&vtss_collector_state) == VTSS_COLLECTOR_UNINITING){
+         //No needs to add module map
+         TRACE("adding mmap after stop, no need this");
+         goto out;
+    }
+    tskd = (struct vtss_task_data*)&item->data;
+    if (tskd == NULL){
+        ERROR("Internal error: vtss_target_add_map_work: tskd == NULL");
+        goto out;
+    }
+
+    trnd = VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux;
+    if (trnd == NULL){
+        ERROR("Internal error: vtss_target_add_map_work: tskd->trnd_aux == NULL");
+        goto out;
+    }
+    if (VTSS_IS_COMPLETE(tskd)){
+         //No needs to add module map
+         TRACE("task is complete, no needs to load modules anymore");
+         goto out;
+    }
+    task = vtss_find_task_by_tid(tskd->tid);
+    if (task == NULL){
+        ERROR("Internal error: vtss_target_add_map_work: tskd == NULL");
+        goto out;
+    }
+
+    if (vtss_mmap_all(tskd, task)){
+        msleep_interruptible(10); //wait again!
+        ERROR("Module map was not loaded completely to the trace!");
+        goto out;
+    }
+    if (atomic_read(&vtss_collector_state) == VTSS_COLLECTOR_UNINITING || vtss_kmap_all(tskd)){
+        ERROR("Kernel map was not loaded completely to the trace!");
+    }
+    TRACE("(%d:%d): data=0x%p, u=%d, n=%d",
+        tskd->tid, tskd->pid, tskd, atomic_read(&item->usage), atomic_read(&vtss_target_count));
+out:
+    /* release data */
+    vtss_task_map_put_item(item);
+out1:
+    atomic_dec(&vtss_kernel_task_in_progress);
+    DEBUG_COLLECTOR("after dec vtss_kernel_task_in_progress = %d", atomic_read(&vtss_kernel_task_in_progress));
+    vtss_kfree(work);
+}
+
+static inline int is_branch_overflow(struct vtss_task_data* tskd)
+{
+    int i;
+    int f = 0;
+    cpuevent_t* event;
+    event = (cpuevent_t*)tskd->cpuevent_chain;
+    if(!event) {
+        return 0;
+    }
+    for(i = 0; i < VTSS_CFG_CHAIN_SIZE; i++)
+    {
+        if(!event[i].valid){
+            break;
+        }
+        if((event[i].selmsk & 0xff) == 0xc4){
+            f = 1;
+            if(event[i].vft->overflowed(&event[i]))
+            {
+                return 1;
+            }
+        }
+    }
+    if(f){
+      return 0;
+    }
+    return 1;
+}
+
+static inline int is_bts_enable(struct vtss_task_data* tskd)
+{
+    return ((reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_BRANCH) && is_branch_overflow(tskd));
+}
+static inline int is_callcount_enable(struct vtss_task_data* tskd)
+{
+    return ((reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) || is_bts_enable(tskd));
+}
+static void vtss_callcount_disable(void)
+{
+    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) vtss_disable_ipt();
+    else vtss_bts_disable();
+}
+static void vtss_callcount_enable(void)
+{
+    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) vtss_enable_ipt(reqcfg.ipt_cfg.mode, 0);
+    else vtss_bts_enable();
+}
+static void vtss_profiling_pause(void)
+{
+    unsigned long flags;
+
+    local_irq_save(flags);
+    vtss_callcount_disable();
+    vtss_pebs_disable();
+    vtss_cpuevents_freeze();
+    vtss_lbr_disable();
+    local_irq_restore(flags);
+}
+
+
+static void vtss_profiling_resume(vtss_task_map_item_t* item, int bts_resume)
+{
+    int trace_flags = reqcfg.trace_cfg.trace_flags;
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+    int cpu;
+
+    cpu = raw_smp_processor_id();
+
+    tskd->state &= ~VTSS_ST_PMU_SET;
+
+    atomic_inc(&vtss_transport_busy);
+    if (atomic_read(&vtss_transport_state) != 1) {
+        vtss_profiling_pause();
+        atomic_dec(&vtss_transport_busy);
+        return;
+    }
+
+
+    if (unlikely(!vtss_cpu_active(cpu) || VTSS_IS_COMPLETE(tskd))) {
+        vtss_profiling_pause();
+        atomic_dec(&vtss_transport_busy);
+        return;
+    }
+    atomic_inc(&vtss_events_enabling);
+
+    switch (atomic_read(&vtss_collector_state)) {
+    case VTSS_COLLECTOR_RUNNING:
+        // all calls of "trnd" should be under vtss_transport_initialized_rwlock.
+        // this lock should be in caller function
+        if (vtss_transport_is_overflowing(tskd->trnd)) {
+#ifdef VTSS_OVERFLOW_PAUSE
+            vtss_cmd_pause();
+            vtss_profiling_pause();
+            atomic_dec(&vtss_events_enabling);
+            atomic_dec(&vtss_transport_busy);
+            return;
+#else
+//            vtss_queue_work(cpu, vtss_overflow_work, &(tskd->trnd), sizeof(void*));
+#endif
+        }
+        break;
+#ifdef VTSS_OVERFLOW_PAUSE
+    case VTSS_COLLECTOR_PAUSED:
+        // all calls of with "trnd" should be under vtss_transport_initialized_rwlock.
+        // this lock should be in caller function
+        if (!vtss_transport_is_overflowing(tskd->trnd))
+            vtss_cmd_resume();
+#endif
+    default:
+        vtss_profiling_pause();
+        atomic_dec(&vtss_events_enabling);
+        atomic_dec(&vtss_transport_busy);
+        return;
+    }
+    atomic_dec(&vtss_transport_busy);
+
+    /* clear BTS/PEBS buffers */
+    vtss_bts_init_dsa();
+    vtss_pebs_init_dsa();
+    vtss_dsa_init_cpu();
+    /* always enable PEBS */
+    vtss_pebs_enable();
+    if (likely(VTSS_IS_CPUEVT(tskd))) {
+        /* enable BTS (if requested) */
+        if (is_callcount_enable(tskd) && (bts_resume  || (reqcfg.ipt_cfg.mode & vtss_iptmode_full)))
+            vtss_callcount_enable();
+        /* enable LBR (if requested) */
+        if (trace_flags & VTSS_CFGTRACE_LASTBR)
+            vtss_lbr_enable(&tskd->lbr);
+        /* restart PMU events */
+        VTSS_PROFILE(pmu, vtss_cpuevents_restart(tskd->cpuevent_chain, 0));
+    } else {
+        /* enable LBR (if requested) */
+        if (trace_flags & VTSS_CFGTRACE_LASTBR)
+            vtss_lbr_enable(&tskd->lbr);
+        /* This need for Woodcrest and Clovertown */
+        vtss_cpuevents_enable();
+    }
+    atomic_dec(&vtss_events_enabling);
+    tskd->state |= VTSS_ST_PMU_SET;
+    tskd->state &= ~VTSS_ST_CPUEVT;
+}
+
+static void vtss_target_dtr(vtss_task_map_item_t* item, void* args)
+{
+#ifdef VTSS_RECOVERY_LOGIC
+    int cpu;
+    unsigned long flags;
+#endif
+    struct task_struct *task = NULL;
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+    DEBUG_COLLECTOR(" (%d:%d): fini='%s'", tskd->tid, tskd->pid, tskd->filename);
+    /* Set thread name in case of detach (exit has not been called) */
+    if (tskd->taskname[0] == '\0')  {
+        task = vtss_find_task_by_tid(tskd->tid);
+        if (task != NULL) { /* task exist */
+            vtss_get_task_comm(tskd->taskname, task);
+            tskd->taskname[VTSS_TASKNAME_SIZE-1] = '\0';
+        } else {
+            ERROR(" (%d:%d): u=%d, n=%d task don't exist",
+                    tskd->tid, tskd->pid, atomic_read(&item->usage), atomic_read(&vtss_target_count));
+        }
+    } else {
+        if ((strlen(tskd->taskname)==strlen("amplxe-runss")) && (!strcmp(tskd->taskname, "amplxe-runss"))){
+            tskd->taskname[0]='\0';
+        }
+    }
+#if defined(CONFIG_PREEMPT_NOTIFIERS) && defined(VTSS_USE_PREEMPT_NOTIFIERS)
+    if (VTSS_IS_NOTIFIER(tskd)) {
+        /* If forceful destruction from vtss_task_map_fini() */
+        if (vtss_find_task_by_tid(tskd->tid) != NULL) { /* task exist */
+            preempt_notifier_unregister(&tskd->preempt_notifier);
+            tskd->state &= ~VTSS_ST_NOTIFIER;
+        } else {
+            ERROR(" (%d:%d): u=%d, n=%d task don't exist",
+                    tskd->tid, tskd->pid, atomic_read(&item->usage), atomic_read(&vtss_target_count));
+        }
+    }
+#endif
+#ifdef VTSS_RECOVERY_LOGIC
+    /* Clear per_cpu recovery data for this tskd */
+    vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+    for_each_possible_cpu(cpu) {
+        if (per_cpu(vtss_recovery_tskd, cpu) == tskd)
+            per_cpu(vtss_recovery_tskd, cpu) = NULL;
+    }
+    vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+#endif
+
+    /* Finish trace transport */
+    atomic_inc(&vtss_transport_busy);
+    if (atomic_read(&vtss_transport_state) == 1) {
+        if (tskd->trnd != NULL) {
+            if (tskd->trnd == tskd->trnd_aux)tskd->trnd_aux = NULL;
+            if (vtss_record_thread_name(tskd->trnd, tskd->tid, (const char*)tskd->taskname, NOT_SAFE)) {
+                TRACE("vtss_record_thread_name() FAIL");
+            }
+            if (vtss_record_thread_stop(tskd->trnd, tskd->tid, tskd->pid, tskd->cpu, NOT_SAFE)) {
+                TRACE("vtss_record_thread_stop() FAIL");
+            }
+            if (tskd->trnd_aux && vtss_transport_delref(tskd->trnd_aux) == 0) {
+                if (vtss_record_magic(tskd->trnd_aux, NOT_SAFE)) {
+                    TRACE("vtss_record_magic() FAIL");
+                }
+                vtss_transport_complete(tskd->trnd_aux);
+                TRACE(" (%d:%d): COMPLETE", tskd->tid, tskd->pid);
+            }
+            if (vtss_transport_delref(tskd->trnd) == 0) {
+                if (vtss_record_process_exit(tskd->trnd, tskd->tid, tskd->pid, tskd->cpu, (const char*)tskd->filename, NOT_SAFE)) {
+                    TRACE("vtss_record_process_exit() FAIL");
+                }
+                if (vtss_record_magic(tskd->trnd, NOT_SAFE)) {
+                    TRACE("vtss_record_magic() FAIL");
+                }
+                vtss_transport_complete(tskd->trnd);
+                TRACE(" (%d:%d): COMPLETE", tskd->tid, tskd->pid);
+            }
+            tskd->trnd = NULL;
+            tskd->trnd_aux = NULL;
+            /* NOTE: tskd->trnd will be destroyed in vtss_transport_fini() */
+        }
+    }
+    atomic_dec(&vtss_transport_busy);
+
+    tskd->stk.destroy(&tskd->stk);
+}
+
+struct vtss_target_new_data
+{
+    pid_t tid;
+    pid_t pid;
+    pid_t ppid;
+    
+    int fired_tid;
+    int fired_order;
+
+    vtss_task_map_item_t *item;
+};
+
+#ifdef VTSS_RECOVERY_LOGIC
+static void vtss_clear_recovery(struct vtss_task_data *tskd)
+{
+    if (tskd && tskd->cpu >= 0){
+        unsigned long flags;
+        vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+        DEBUG_COLLECTOR("clear recovery, cpu = %d", tskd->cpu);
+        per_cpu(vtss_recovery_tskd, tskd->cpu) = NULL;
+        vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+    }
+}
+#endif
+
+int vtss_target_new_part1(pid_t tid, pid_t pid, pid_t ppid, vtss_task_map_item_t *item)
+{
+    struct task_struct *task;
+    struct vtss_task_data *tskd;
+    
+    struct vtss_transport_data* trnd = NULL;
+    struct vtss_transport_data* trnd_aux = NULL;
+    
+    if (!VTSS_COLLECTOR_IS_READY){ //If adding the task is not actual anymore
+        DEBUG_COLLECTOR("The task tid = %d will not be added as collection already stopped", tid);
+        return 0;
+    }
+    tskd = (struct vtss_task_data*)&item->data;
+    trnd = tskd->trnd;
+    trnd_aux = tskd->trnd_aux;
+
+    /* Transport initialization */
+    if (tskd->tid == tskd->pid) { /* New process */
+        if (!trnd){
+            vtss_target_transport_create(&trnd, &trnd_aux, tskd->ppid, tskd->pid, vtss_session_uid, vtss_session_gid);
+        }
+        if (trnd == NULL) {
+            ERROR(" (%d:%d): Unable to create transport", tid, pid);
+            return -ENOMEM;
+        }
+        tskd->trnd = trnd; 
+        if (tskd->trnd != NULL) {
+            //if aux transport was not created early, then use the same transport as for samples.
+            tskd->trnd_aux = trnd_aux ? trnd_aux : tskd->trnd;
+        }
+        if (tskd->trnd != NULL && tskd->trnd_aux != NULL) {
+            if (vtss_record_configs(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, tskd->m32, SAFE)) {
+                TRACE("vtss_record_configs() FAIL");
+            }
+            if (vtss_record_process_exec(tskd->trnd, tskd->tid, tskd->pid, tskd->cpu, (const char*)tskd->filename, SAFE)) {
+                TRACE("vtss_record_process_exec() FAIL");
+            }
+        }
+    } else { /* New thread */
+        struct vtss_task_data *tskd0;
+        vtss_task_map_item_t *item0 = vtss_task_map_get_item(tskd->pid);
+        if (item0 == NULL) {
+            ERROR(" (%d:%d): Unable to find main thread", tskd->tid, tskd->pid);
+            return -ENOENT;
+        }
+        tskd0 = (struct vtss_task_data*)&item0->data;
+        tskd->trnd = tskd0->trnd;
+        tskd->trnd_aux = tskd0->trnd_aux;
+        if (tskd->trnd != NULL) {
+            vtss_transport_addref(tskd->trnd);
+        }
+        if (tskd->trnd!=tskd->trnd_aux && tskd->trnd_aux != NULL) {
+            vtss_transport_addref(tskd->trnd_aux);
+        }
+        vtss_task_map_put_item(item0);
+    }
+    if (tskd->trnd == NULL) {
+        ERROR(" (%d:%d): Unable to create transport", tskd->tid, tskd->pid);
+        return -ENOMEM;
+    }
+    /* Create cpuevent chain */
+    memset(tskd->cpuevent_chain, 0, VTSS_CFG_CHAIN_SIZE*sizeof(cpuevent_t));
+    vtss_cpuevents_upload(tskd->cpuevent_chain, &reqcfg.cpuevent_cfg_v1[0], reqcfg.cpuevent_count_v1);
+    /* Store first records */
+    if (likely(VTSS_NEED_STORE_NEWTASK(tskd)))
+        VTSS_STORE_NEWTASK(tskd, SAFE);
+    if (likely(VTSS_NEED_STORE_SOFTCFG(tskd)))
+        VTSS_STORE_SOFTCFG(tskd, SAFE);
+    if (likely(VTSS_NEED_STORE_PAUSE(tskd)))
+        VTSS_STORE_PAUSE(tskd, tskd->cpu, 0x66 /* tpss_pi___itt_pause from TPSS ini-file */, SAFE);
+    /* ========================================================= */
+    /* Add new item in task map. Tracing starts after this call. */
+    /* ========================================================= */
+    if (!vtss_task_map_add_item(item))
+        atomic_inc(&vtss_target_count);
+    DEBUG_COLLECTOR("(%d:%d): u=%d, n=%d, init='%s'",
+        tskd->tid, tskd->pid, atomic_read(&item->usage), atomic_read(&vtss_target_count), tskd->filename);
+    task = vtss_find_task_by_tid(tskd->tid);
+    DEBUG_COLLECTOR("Found task = %p", task);
+    if (task != NULL && !(task->state & TASK_DEAD)) { /* task exist */
+#ifdef VTSS_GET_TASK_STRUCT
+        get_task_struct(task);
+#endif
+        /* Setting up correct arch (32-bit/64-bit) of user application */
+        tskd->m32 = test_tsk_thread_flag(task, TIF_IA32) ? 1 : 0;
+        tskd->stk.lock(&tskd->stk);
+        tskd->stk.wow64 = tskd->m32;
+        tskd->stk.clear(&tskd->stk);
+        tskd->stk.unlock(&tskd->stk);
+#ifdef VTSS_SYSCALL_TRACE
+        /* NOTE: Need this for BP save and FIXUP_TOP_OF_STACK into pt_regs
+         * when is called from the SYSCALL. Actual only for 64-bit kernel! */
+        set_tsk_thread_flag(task, TIF_SYSCALL_TRACE);
+#endif
+        if (tskd->tid == tskd->pid) { /* New process */
+            unsigned long addr = VTSS_EVENT_LOST_MODULE_ADDR;
+            unsigned long size = 1;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,35)
+            unsigned long dyn_addr = 0;
+#endif
+
+            long long cputsc, realtsc;
+            vtss_time_get_sync(&cputsc, &realtsc);
+            if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, tskd->m32, addr, size, VTSS_EVENT_LOST_MODULE_NAME, 0, cputsc, realtsc, SAFE)) {
+                TRACE("vtss_record_module() FAIL");
+            }
+            addr = VTSS_KSTART;
+            /* TODO: reduce the size to real instead of maximum */
+            size = VTSS_KSIZE;
+#ifdef CONFIG_RANDOMIZE_BASE
+#ifdef CONFIG_KALLSYMS
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,35)
+            /* fixup kernel start address for newest kernels */
+            dyn_addr = kallsyms_lookup_name("_text") & ~(PAGE_SIZE - 1);
+            if(dyn_addr > addr) {
+                TRACE("vmlinux addr=0x%lx, dyn_addr=0x%lx", addr, dyn_addr);
+                size -= (dyn_addr - addr);
+                addr = dyn_addr;
+            }
+            else if(!dyn_addr) {
+                dyn_addr = kallsyms_lookup_name("_stext") & ~(PAGE_SIZE - 1);
+                if(dyn_addr > addr) {
+                    TRACE("vmlinux addr=0x%lx, stext dyn_addr=0x%lx", addr, dyn_addr);
+                    size -= (dyn_addr - addr);
+                    addr = dyn_addr;
+                }
+            }
+#endif
+#endif
+#endif
+            if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, 0, addr, size, "vmlinux", 0, cputsc, realtsc, SAFE)) {
+                TRACE("vtss_record_module() FAIL");
+            }
+#ifdef CONFIG_X86_64
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,16,0)
+            addr = (unsigned long)VSYSCALL_START;
+            size = (unsigned long)(VSYSCALL_MAPPED_PAGES * PAGE_SIZE);
+#else
+            addr = (unsigned long)VSYSCALL_ADDR;
+            size = (unsigned long)PAGE_SIZE;
+#endif
+            if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, 0, addr, size, "[vsyscall]", 0, cputsc, realtsc, SAFE)) {
+                TRACE("vtss_record_module() FAIL");
+            }
+#endif
+            if (irqs_disabled() || vtss_mmap_all(tskd, task) || vtss_kmap_all(tskd)){
+                 // we have to try load map again!
+                 vtss_task_map_item_t* item_temp = vtss_task_map_get_item(tskd->tid);
+                 if (item == item_temp) {
+                    DEBUG_COLLECTOR("Map file was not loaded completely. Arranged the task to finish this");
+                    atomic_inc(&vtss_kernel_task_in_progress);
+                    if (vtss_queue_work(-1, vtss_target_add_mmap_work, &item, sizeof(item))) {
+                        ERROR("Internal error: add mmap task was not arranged");
+                        atomic_dec(&vtss_kernel_task_in_progress);
+                        vtss_task_map_put_item(item_temp);
+                    } else {
+                        set_tsk_need_resched(current);
+                    }
+                } else {
+                    vtss_task_map_put_item(item_temp);
+                    ERROR("Internal error: task map error");
+                }
+            } else {
+                TRACE("Modules are added");
+            }
+        }
+#if defined(CONFIG_PREEMPT_NOTIFIERS) && defined(VTSS_USE_PREEMPT_NOTIFIERS)
+        /**
+         * TODO: add to task, not to current !!!
+         * This API will be added in future version of kernel:
+         * preempt_notifier_register_task(&tskd->preempt_notifier, task);
+         * So far I should use following:
+         */
+        hlist_add_head(&tskd->preempt_notifier.link, &task->preempt_notifiers);
+        tskd->state |= VTSS_ST_NOTIFIER;
+#endif
+#ifdef VTSS_GET_TASK_STRUCT
+        vtss_put_task_struct(task);
+#endif
+    } else {
+        char dbgmsg[128];
+        int rc = snprintf(dbgmsg, sizeof(dbgmsg)-1, "vtss_target_new(%d,%d,%d,'%s'): u=%d, n=%d task(%ld) don't exist or not valid.",
+                tskd->tid, tskd->pid, tskd->ppid, tskd->filename, atomic_read(&item->usage), atomic_read(&vtss_target_count), task ? task->state : 0);
+        if (rc > 0 && rc < sizeof(dbgmsg)-1) {
+            dbgmsg[rc] = '\0';
+            vtss_record_debug_info(tskd->trnd, dbgmsg, 0);
+        }
+        DEBUG_COLLECTOR("The new task is not valid. The task is not added to profile (%d:%d): u=%d, n=%d, done='%s'",
+                tskd->tid, tskd->pid, atomic_read(&item->usage), atomic_read(&vtss_target_count), tskd->filename);
+        vtss_target_del(item);
+        return 0;
+    }
+
+    DEBUG_COLLECTOR("end: (%d:%d): u=%d, n=%d, done='%s'",
+            tskd->tid, tskd->pid, atomic_read(&item->usage), atomic_read(&vtss_target_count), tskd->filename);
+    return 0;
+}
+
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_target_new_part2(struct work_struct *work)
+#else
+static void vtss_target_new_part2(void *work)
+#endif
+{
+    struct vtss_work* my_work = (struct vtss_work*)work;
+    struct vtss_target_new_data* data = NULL;
+
+    DEBUG_COLLECTOR("vtss_target_new");
+
+    if (!my_work){
+        ERROR ("Internal error: target_new have no any work to do");
+        atomic_dec(&vtss_kernel_task_in_progress);
+        return;
+    }
+
+    DEBUG_COLLECTOR("after inc vtss_kernel_task_in_progress = %d", atomic_read(&vtss_kernel_task_in_progress));
+    if (VTSS_COLLECTOR_IS_READY){
+        data = (struct vtss_target_new_data*)(&my_work->data);
+        if (!data){
+            ERROR ("Internal error: no data in work");
+            goto out;
+        }
+        if (data->fired_tid != -1){
+          DEBUG_COLLECTOR("awaiting fired order = %d for tid = %d", data->fired_order, data->fired_tid);
+          if (!vtss_target_wait_work_time(data->fired_tid, data->fired_order))
+              DEBUG_COLLECTOR("wait task failed");
+          DEBUG_COLLECTOR("stop awaiting");
+        }
+        if (VTSS_COLLECTOR_IS_READY) //If adding the task still actual after wait
+        /**data->rc =*/ vtss_target_new_part1(data->tid, data->pid, data->ppid, data->item);
+
+    } else {
+        DEBUG_COLLECTOR("newask after collection is done");
+    }
+
+    vtss_target_remove_from_temp_list(data->tid, 0);
+
+    DEBUG_COLLECTOR("after removing from the list");
+out:
+    vtss_task_map_put_item(data->item);
+    vtss_kfree(work);
+    DEBUG_COLLECTOR("after free work");
+    atomic_dec(&vtss_kernel_task_in_progress);
+    DEBUG_COLLECTOR("after dec vtss_kernel_task_in_progress = %d", atomic_read(&vtss_kernel_task_in_progress));
+
+    return;
+}
+
+static void vtss_sched_switch_to(vtss_task_map_item_t* item, struct task_struct* task, void* ip);
+int vtss_target_new(pid_t tid, pid_t pid, pid_t ppid, const char* filename, int fired_tid, int fired_order)
+{
+    int rc = 0;
+    size_t size = 0;
+
+    vtss_task_map_item_t *item = NULL;
+    struct vtss_task_data *tskd = NULL;
+    struct task_struct *task = NULL;
+
+    DEBUG_COLLECTOR("vtss_target_new(%d,%d,%d) \n",tid, pid, ppid);
+
+    if (atomic_read(&vtss_transport_state) != 1){
+        ERROR(" (%d:%d): Transport not initialized", tid, pid);
+        return VTSS_ERR_INTERNAL;
+    }
+
+    atomic_inc(&vtss_kernel_task_in_progress);
+
+    if (!VTSS_COLLECTOR_IS_READY){ //If adding the task is not actual anymore
+        DEBUG_COLLECTOR("The task tid = %d will not be added as collection already stopped", tid);
+        atomic_dec(&vtss_kernel_task_in_progress);
+        return 0;
+    }
+
+    item = vtss_task_map_alloc(tid, sizeof(struct vtss_task_data), vtss_target_dtr, GFP_KERNEL);
+
+    if (item == NULL) {
+        ERROR(" (%d:%d): Unable to allocate, size = %d", tid, pid, (int)sizeof(struct vtss_task_data));
+        atomic_dec(&vtss_kernel_task_in_progress);
+        return -ENOMEM;
+    }
+
+    tskd = (struct vtss_task_data*)&item->data;
+    tskd->tid        = tid;
+    tskd->pid        = pid;
+    tskd->trnd       = NULL;
+    tskd->trnd_aux   = NULL;
+    tskd->ppid       = ppid;
+
+    tskd->m32        = 0; /* unknown so far, assume native */
+    tskd->ip         = NULL;
+    tskd->cpu = raw_smp_processor_id();
+    tskd->from_ip    = NULL;
+#ifndef VTSS_NO_BTS
+    tskd->bts_size   = 0;
+#endif
+    tskd->state      = (VTSS_ST_NEWTASK | VTSS_ST_SOFTCFG | VTSS_ST_STKDUMP);
+    if (atomic_read(&vtss_collector_state) == VTSS_COLLECTOR_PAUSED)
+        tskd->state |= VTSS_ST_PAUSE;
+#ifdef VTSS_SYSCALL_TRACE
+    tskd->syscall_sp = NULL;
+    tskd->syscall_enter = 0ULL;
+#endif
+#if defined(CONFIG_PREEMPT_NOTIFIERS) && defined(VTSS_USE_PREEMPT_NOTIFIERS)
+    preempt_notifier_init(&tskd->preempt_notifier, &vtss_preempt_ops);
+#endif
+    rc = vtss_init_stack(&tskd->stk);
+    if (rc) {
+        ERROR(" (%d:%d): Unable to init STK: %d", tid, pid, rc);
+        atomic_dec(&vtss_kernel_task_in_progress);
+        return rc;
+    }
+    task = vtss_find_task_by_tid(tskd->tid);
+    if (task != NULL && !(task->state & TASK_DEAD)&&task->comm != NULL) { /* task exist */
+        size = min((size_t)VTSS_FILENAME_SIZE-1, (size_t)strnlen(task->comm, TASK_COMM_LEN));
+        memcpy(tskd->filename, task->comm, size);
+    } else if (filename != NULL) {
+        size = min((size_t)VTSS_FILENAME_SIZE-1, (size_t)strlen(filename));
+        memcpy(tskd->filename, filename, size);
+    }
+    tskd->filename[size] = '\0';
+    tskd->taskname[0] = '\0';
+
+    if (tid == pid){
+        vtss_target_transport_create(&tskd->trnd, &tskd->trnd_aux, ppid, pid, vtss_session_uid, vtss_session_gid);
+    }
+    if((!irqs_disabled() || (tskd->trnd && tskd->trnd_aux))){
+        if (fired_tid!=-1){
+            DEBUG_COLLECTOR("awaiting fired order = %d for tid = %d", fired_order, fired_tid);
+            vtss_target_wait_work_time(fired_tid, fired_order);
+            DEBUG_COLLECTOR("stop awaiting");
+        }
+
+        rc = vtss_target_new_part1(tid, pid, ppid, item);
+
+        if (task == current /*|| tskd->tid == TASK_TID(current)*/)
+        {
+            DEBUG_COLLECTOR("mark swap in");
+            tskd->from_ip = (void*)_THIS_IP_;
+            vtss_sched_switch_to(item, task, NULL);
+        }
+        vtss_task_map_put_item(item);
+
+        atomic_dec(&vtss_kernel_task_in_progress);
+    } else {
+        struct vtss_target_new_data data;
+
+        data.tid = tid;
+        data.pid = pid;
+        data.ppid = ppid;
+
+        data.fired_tid = fired_tid;
+        data.fired_order = fired_order;
+        data.item = item;
+
+        vtss_target_add_to_temp_list(data.tid);
+
+        if ((rc = vtss_queue_work(-1, vtss_target_new_part2, &data, sizeof(data)))) {
+              vtss_task_map_put_item(item);
+              atomic_dec(&vtss_kernel_task_in_progress);
+              vtss_target_remove_from_temp_list(data.tid, 1);
+        } else {
+            set_tsk_need_resched(current);
+        }
+    }
+    DEBUG_COLLECTOR("Target added: rc = %d", rc);
+    return rc;
+}
+
+int vtss_target_del(vtss_task_map_item_t* item)
+{
+    vtss_task_map_del_item(item);
+    if (atomic_dec_and_test(&vtss_target_count)) {
+        //This function should be called after transport deinitialization
+        //vtss_procfs_ctrl_wake_up(NULL, 0);
+    }
+    return 0;
+}
+
+void vtss_target_fork(struct task_struct *task, struct task_struct *child)
+{
+    if(!VTSS_COLLECTOR_IS_READY){
+         //vtss collector is unitialized
+         return;
+    }
+     if (task != NULL && child != NULL) {
+        int fired_order = -1;
+        vtss_task_map_item_t* item = NULL;
+
+        fired_order = vtss_target_find_in_temp_list(TASK_TID(task));
+        if (fired_order !=-1){
+            DEBUG_COLLECTOR("fired_order = %d", fired_order);
+            vtss_target_new(TASK_TID(child), TASK_PID(child), TASK_TID(task), task->comm, TASK_TID(task), fired_order);
+        } else {
+            item = vtss_task_map_get_item(TASK_TID(task));
+        }
+        if (item) {
+            struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+
+            DEBUG_COLLECTOR("(%d:%d)=>(%d:%d): u=%d, n=%d, file='%s', irqs=%d",
+                  TASK_TID(task), TASK_PID(task), TASK_TID(child), TASK_PID(child),
+                  atomic_read(&item->usage), atomic_read(&vtss_target_count), tskd->filename, !irqs_disabled());
+            preempt_disable();
+            tskd->cpu = smp_processor_id();
+            preempt_enable_no_resched();
+            {
+                int rc = 0;
+                rc = vtss_target_new(TASK_TID(child), TASK_PID(child), TASK_PID(task), tskd->filename, -1, -1);
+                if (unlikely(rc)) ERROR("(%d:%d): Error in vtss_target_new()=%d. Fork failed!", TASK_TID(task), TASK_PID(task), rc);
+            } 
+            vtss_task_map_put_item(item);
+        }
+    }
+}
+
+void vtss_target_exec_enter(struct task_struct *task, const char *filename, const char *config)
+{
+    vtss_task_map_item_t* item;
+    int fired_order = -1;
+
+    vtss_profiling_pause();
+    if(!VTSS_COLLECTOR_IS_READY){
+         //vtss collector is unitialized
+         TRACE("Not ready");
+         return;
+    }
+    if (atomic_read(&vtss_transport_state)!=1) return;
+    fired_order = vtss_target_find_in_temp_list(TASK_TID(task));
+    if (fired_order !=-1)
+        vtss_target_wait_work_time(TASK_TID(task), fired_order);
+    item = vtss_task_map_get_item(TASK_TID(task));
+    if (item != NULL) {
+        struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+
+        DEBUG_COLLECTOR("(%d:%d): u=%d, n=%d, file='%s'",
+                tskd->tid, tskd->pid, atomic_read(&item->usage), atomic_read(&vtss_target_count), filename);
+#if defined(CONFIG_PREEMPT_NOTIFIERS) && defined(VTSS_USE_PREEMPT_NOTIFIERS)
+        if (VTSS_IS_NOTIFIER(tskd)) {
+            preempt_notifier_unregister(&tskd->preempt_notifier);
+            tskd->state &= ~VTSS_ST_NOTIFIER;
+        }
+#endif
+        if (task != NULL){
+            //tskd->taskname[0]='\0';
+            //the lines below leads a bug when thread name is shown as amplxe_runss. We need fix it in GUI part before.
+            size_t size =  min((size_t)VTSS_TASKNAME_SIZE-1, (size_t)strlen(task->comm));
+            strncpy(tskd->taskname, task->comm, size);
+            tskd->taskname[size]='\0';
+        }
+        tskd->taskname[VTSS_TASKNAME_SIZE-1] = '\0';
+
+        tskd->state |= VTSS_ST_COMPLETE;
+        vtss_task_map_put_item(item);
+    }
+}
+           
+void vtss_target_exec_leave(struct task_struct *task, const char *filename, const char *config, int rc, int fired_tid)
+{
+    vtss_task_map_item_t* item = NULL;
+    int fired_order = -1;
+    
+    if(!VTSS_COLLECTOR_IS_READY) {
+        //vtss collector is unitialized
+        TRACE("collector is not initialized");
+        return;
+    }
+    fired_order = vtss_target_find_in_temp_list(fired_tid);
+    if (fired_order !=-1){
+        if (rc == 0) vtss_target_new(TASK_TID(task), TASK_PID(task), TASK_PID(TASK_PARENT(task)), filename, fired_tid, fired_order);
+    } else {
+        item = vtss_task_map_get_item(fired_tid);
+    }
+    if (item) {
+        struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+
+        DEBUG_COLLECTOR("(%d:%d); file='%s', rc=%d, item = %p, u:%d",TASK_PID(TASK_PARENT(task)), TASK_PID(task), filename, rc, item, atomic_read(&item->usage));
+        if (rc == 0) { /* Execution success, so start tracing new process */
+
+            preempt_disable();
+            tskd->cpu = smp_processor_id();
+            preempt_enable_no_resched();
+            vtss_target_new(TASK_TID(task), TASK_PID(task), TASK_PID(TASK_PARENT(task)), filename, -1, -1);
+
+        } else { /* Execution failed, so restore tracing current process */
+#if defined(CONFIG_PREEMPT_NOTIFIERS) && defined(VTSS_USE_PREEMPT_NOTIFIERS)
+            /**
+             * TODO: add to task, not to current !!!
+             * This API will be added in future version of kernel:
+             * preempt_notifier_register_task(&tskd->preempt_notifier, task);
+             * So far I should use following:
+             */
+            hlist_add_head(&tskd->preempt_notifier.link, &task->preempt_notifiers);
+            tskd->state |= VTSS_ST_NOTIFIER;
+#endif
+            tskd->state &= ~VTSS_ST_COMPLETE;
+            tskd->state |= VTSS_ST_PMU_SET;
+        }
+        DEBUG_COLLECTOR("old item usage = %d", atomic_read(&item->usage));
+
+        vtss_task_map_put_item(item);
+        
+    } else{
+       if (*config != '\0' && rc == 0) { /* attach to current process */
+
+
+         vtss_target_new(TASK_TID(task), TASK_PID(task), TASK_PID(TASK_PARENT(task)), filename, -1, -1);
+      }
+    }
+}
+
+static void vtss_dump_save_events(struct vtss_task_data* tskd)
+{
+unsigned long flags;
+local_irq_save(flags);
+preempt_disable();
+if (VTSS_IN_CONTEXT(tskd)){
+    if (unlikely((reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) && (reqcfg.ipt_cfg.mode & vtss_iptmode_full))){
+        VTSS_PROFILE(bts, vtss_dump_ipt(VTSS_PT_FLUSH_MODE ? tskd->trnd_aux : tskd->trnd, tskd->tid, tskd->cpu, 0));
+    }
+}
+preempt_enable_no_resched();
+local_irq_restore(flags);
+
+}
+
+void vtss_target_exit(struct task_struct *task)
+{
+    vtss_task_map_item_t* item = NULL;
+    int fired_order = -1;
+
+    vtss_profiling_pause();
+
+    fired_order = vtss_target_find_in_temp_list(TASK_TID(task));
+    if (fired_order !=-1)
+        vtss_target_wait_work_time(TASK_TID(task), fired_order);
+    item = vtss_task_map_get_item(TASK_TID(task));
+    
+    if (item != NULL) {
+        struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+#ifdef VTSS_RECOVERY_LOGIC
+        int cpu = -1;
+        unsigned long flags;
+#endif
+        if (!tskd){
+            ERROR("!tskd");
+        }else DEBUG_COLLECTOR("tskd=%p", tskd);
+        DEBUG_COLLECTOR("(%d:%d): u=%d, n=%d, file='%s', irqs=%d",
+              tskd->tid, tskd->pid, atomic_read(&item->usage), atomic_read(&vtss_target_count), tskd->filename, !irqs_disabled());
+#if defined(CONFIG_PREEMPT_NOTIFIERS) && defined(VTSS_USE_PREEMPT_NOTIFIERS)
+        if (VTSS_IS_NOTIFIER(tskd)) {
+            preempt_notifier_unregister(&tskd->preempt_notifier);
+            tskd->state &= ~VTSS_ST_NOTIFIER;
+        }
+#endif
+        vtss_dump_save_events(tskd);
+#ifdef VTSS_RECOVERY_LOGIC
+        vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+        for_each_possible_cpu(cpu) {
+            if (per_cpu(vtss_recovery_tskd, cpu) == tskd)
+                per_cpu(vtss_recovery_tskd, cpu) = NULL;
+        }
+        vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+#endif
+        
+        tskd->cpu = raw_smp_processor_id();
+
+        tskd->state |= VTSS_ST_COMPLETE;
+        vtss_dump_save_events(tskd);
+        if (task != NULL){
+            size_t size =  min((size_t)VTSS_TASKNAME_SIZE-1, (size_t)strlen(task->comm));
+            strncpy(tskd->taskname, task->comm, size);
+            tskd->taskname[size]='\0';
+        }
+        tskd->taskname[VTSS_TASKNAME_SIZE-1] = '\0';
+        DEBUG_COLLECTOR("before del");
+        vtss_target_del(item);
+        DEBUG_COLLECTOR("after del");
+        vtss_task_map_put_item(item);
+        DEBUG_COLLECTOR("after put");
+    }
+}
+
+#ifdef VTSS_SYSCALL_TRACE
+
+static struct vtss_task_data* vtss_wait_for_completion(vtss_task_map_item_t** pitem)
+{
+    unsigned long i;
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&((*pitem)->data);
+
+    /* It's task after exec(), so waiting for re-initialization */
+    DEBUG_COLLECTOR("Waiting task: 0x%p ....", current);
+    for (i = 0; i < 1000000UL && *pitem != NULL && atomic_read(&vtss_collector_state) == VTSS_COLLECTOR_RUNNING; i++) {
+        vtss_task_map_put_item(*pitem);
+        /* TODO: waiting... */
+        *pitem = vtss_task_map_get_item(TASK_TID(current));
+        if (*pitem != NULL) {
+            tskd = (struct vtss_task_data*)&((*pitem)->data);
+            if (!VTSS_IS_COMPLETE(tskd))
+                break;
+        }
+    }
+    DEBUG_COLLECTOR("Waiting task: 0x%p done(%lu)", current, i);
+    if (*pitem == NULL) {
+        ERROR("Tracing task 0x%p error", current);
+        return NULL;
+    } else if (VTSS_IS_COMPLETE(tskd)) {
+        DEBUG_COLLECTOR("Task 0x%p wait timeout", current);
+        vtss_task_map_put_item(*pitem);
+        return NULL;
+    }
+    return tskd;
+}
+
+void vtss_syscall_enter(struct pt_regs* regs)
+{
+    vtss_task_map_item_t* item = vtss_task_map_get_item(TASK_TID(current));
+
+    if (item != NULL) {
+        struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+
+        TRACE("task=0x%p, syscall=%.3ld, ip=0x%lx, sp=0x%lx, bp=0x%lx",
+            current, REG(orig_ax, regs), REG(ip, regs), REG(sp, regs), REG(bp, regs));
+        if (unlikely(VTSS_IS_COMPLETE(tskd)))
+            tskd = vtss_wait_for_completion(&item);
+        if (tskd != NULL) {
+            /* Just store BP register for following unwinding */
+            tskd->syscall_sp = (void*)REG(sp, regs);
+            tskd->syscall_enter = (atomic_read(&vtss_collector_state) == VTSS_COLLECTOR_RUNNING) ? vtss_time_cpu() : 0ULL;
+            tskd->state |= VTSS_ST_IN_SYSCALL;
+            vtss_task_map_put_item(item);
+        }
+    }
+}
+
+void vtss_syscall_leave(struct pt_regs* regs)
+{
+    vtss_task_map_item_t* item = vtss_task_map_get_item(TASK_TID(current));
+
+    if (item != NULL) {
+        struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+
+        TRACE("task=0x%p, syscall=%.3ld, ip=0x%lx, sp=0x%lx, bp=0x%lx, ax=0x%lx",
+            current, REG(orig_ax, regs), REG(ip, regs), REG(sp, regs), REG(bp, regs), REG(ax, regs));
+        if (VTSS_IN_SYSCALL(tskd) && tskd->syscall_enter && atomic_read(&vtss_collector_state) == VTSS_COLLECTOR_RUNNING) {
+            tskd->tcb.syscall_count++;
+            tskd->tcb.syscall_duration += vtss_time_cpu() - tskd->syscall_enter;
+        }
+        tskd->state &= ~VTSS_ST_IN_SYSCALL;
+        tskd->syscall_sp = NULL;
+        vtss_task_map_put_item(item);
+    }
+}
+
+#endif /* VTSS_SYSCALL_TRACE */
+
+static int vtss_kmap_all(struct vtss_task_data* tskd)
+{
+    struct module* mod;
+    struct list_head* modules;
+    long long cputsc, realtsc;
+    int repeat = 0;
+
+    if (VTSS_IS_MMAP_INIT(tskd)){
+        ERROR("Kernel map was not loaded because currently the map is in initialized state");
+        return 1;
+    }
+#ifdef VTSS_AUTOCONF_MODULE_MUTEX
+    mutex_lock(&module_mutex);
+#endif
+    VTSS_SET_MMAP_INIT(tskd);
+    vtss_time_get_sync(&cputsc, &realtsc);
+    for(modules = THIS_MODULE->list.prev; (unsigned long)modules > MODULES_VADDR; modules = modules->prev);
+    list_for_each_entry(mod, modules, list){
+        const char *name   = mod->name;
+#ifdef VTSS_AUTOCONF_MODULE_CORE_LAYOUT
+        unsigned long addr = (unsigned long)mod->core_layout.base;
+        unsigned long size = mod->core_layout.size;
+#else
+        unsigned long addr = (unsigned long)mod->module_core;
+        unsigned long size = mod->core_size;
+#endif
+        if (module_is_live(mod)) {
+            TRACE("module: addr=0x%lx, size=%lu, name='%s'", addr, size, name);
+            if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, 0, addr, size, name, 0, cputsc, realtsc, SAFE)) {
+                TRACE("vtss_record_module() FAIL");
+                repeat = 1;
+            }
+        }
+    }
+    VTSS_CLEAR_MMAP_INIT(tskd);
+#ifdef VTSS_AUTOCONF_MODULE_MUTEX
+    mutex_unlock(&module_mutex);
+#endif
+    return repeat;
+}
+
+void vtss_kmap(struct task_struct* task, const char* name, unsigned long addr, unsigned long pgoff, unsigned long size)
+{
+    vtss_task_map_item_t* item = NULL;
+
+    if (!task) return;
+
+    item = vtss_task_map_get_item(TASK_TID(task));
+    atomic_inc(&vtss_transport_busy);
+    if (atomic_read(&vtss_transport_state) == 1) {
+        if (item != NULL) {
+            struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+            long long cputsc, realtsc;
+
+            if (!VTSS_IS_MMAP_INIT(tskd)){
+                vtss_time_get_sync(&cputsc, &realtsc);
+                TRACE("addr=0x%lx, size=%lu, name='%s', pgoff=%lu", addr, size, name, pgoff);
+                if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, 0, addr, size, name, pgoff, cputsc, realtsc, SAFE)) {
+                     ERROR("vtss_record_module() FAIL");
+                }
+            }
+        }
+    }
+    atomic_dec(&vtss_transport_busy);
+    if (item != NULL) vtss_task_map_put_item(item);
+}
+
+static int vtss_mmap_all(struct vtss_task_data* tskd, struct task_struct* task)
+{
+    struct mm_struct *mm;
+    char *pname, *tmp = (char*)vtss_get_free_page(GFP_KERNEL);
+    int repeat = 0;
+
+    if (!tmp){
+        ERROR("No memory");
+        return 1;
+    }
+    if (VTSS_IS_MMAP_INIT(tskd)){
+        ERROR("Module map was not loaded because currently the map is in initialized state");
+        return 1;
+    }
+    if ((mm = get_task_mm(task)) != NULL) {
+        int is_vdso_found = 0;
+        struct vm_area_struct* vma;
+        long long cputsc, realtsc;
+        VTSS_SET_MMAP_INIT(tskd);
+        vtss_time_get_sync(&cputsc, &realtsc);
+        down_read(&mm->mmap_sem);
+        for (vma = mm->mmap; vma != NULL; vma = vma->vm_next) {
+            TRACE("vma=[0x%lx - 0x%lx], flags=0x%lx", vma->vm_start, vma->vm_end, vma->vm_flags);
+            if (((vma->vm_flags & VM_EXEC) || (vma->vm_flags & VM_MAYEXEC)) && !(vma->vm_flags & VM_WRITE) &&
+                vma->vm_file && vma->vm_file->f_path.dentry)
+            {
+                pname = D_PATH(vma->vm_file, tmp, PAGE_SIZE);
+                if (!IS_ERR(pname)) {
+                    TRACE("addr=0x%lx, size=%lu, file='%s', pgoff=%lu", vma->vm_start, (vma->vm_end - vma->vm_start), pname, vma->vm_pgoff);
+                    if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, tskd->m32, vma->vm_start, (vma->vm_end - vma->vm_start), pname, vma->vm_pgoff, cputsc, realtsc, SAFE)) {
+                        TRACE("vtss_record_module() FAIL");
+                        repeat = 1;
+                    }
+                }
+#ifdef VM_HUGEPAGE
+            /**
+             * Try to recover the mappings of some hugepages
+             * by looking at segments immediately precede and
+             * succeed them 
+             */
+            } else if ((vma->vm_flags & VM_HUGEPAGE) &&
+                ((vma->vm_flags & VM_EXEC) || (vma->vm_flags & VM_MAYEXEC)) && !(vma->vm_flags & VM_WRITE) &&
+                !vma->vm_file)
+            {
+                struct vm_area_struct *vma_pred = find_vma(mm, vma->vm_start - 1);
+                struct vm_area_struct *vma_succ = find_vma(mm, vma->vm_end);
+                if (vma_pred && vma_succ) {
+                    if (((vma_pred->vm_flags & VM_EXEC) || (vma_pred->vm_flags & VM_MAYEXEC)) && !(vma_pred->vm_flags & VM_WRITE) &&
+                        vma_pred->vm_file && vma_pred->vm_file->f_path.dentry) {
+                        char *pname_pred = D_PATH(vma_pred->vm_file, tmp, PAGE_SIZE);
+                        if (!IS_ERR(pname_pred)) {
+                            if (vma_succ->vm_file && vma_succ->vm_file->f_path.dentry) {
+                                char *pname_succ = D_PATH(vma_succ->vm_file, tmp, PAGE_SIZE);
+                                if (!IS_ERR(pname_succ)) {
+                                    if (strcmp(pname_pred, pname_succ) == 0) {
+                                        TRACE("recover vma=[0x%lx - 0x%lx] flags=0x%lx, pgoff=0x%lx, file='%s'", vma->vm_start, vma->vm_end, vma->vm_flags, vma_pred->vm_pgoff + ((vma_pred->vm_end - vma_pred->vm_start) >> PAGE_SHIFT), pname_pred);
+                                        if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, tskd->m32, vma->vm_start, (vma->vm_end - vma->vm_start), pname_pred, vma_pred->vm_pgoff + ((vma_pred->vm_end - vma_pred->vm_start) >> PAGE_SHIFT), cputsc, realtsc, SAFE)) {
+                                            TRACE("vtss_record_module() FAIL");
+                                            repeat = 1;
+                                        }
+                                    }
+                                }
+                            }
+                        }
+                    }
+                }
+#endif
+            } else if (vma->vm_mm && vma->vm_start == (long)vma->vm_mm->context.vdso) {
+                is_vdso_found = 1;
+                TRACE("addr=0x%lx, size=%lu, name='%s', pgoff=%lu", vma->vm_start, (vma->vm_end - vma->vm_start), "[vdso]", 0UL);
+                if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, tskd->m32, vma->vm_start, (vma->vm_end - vma->vm_start), "[vdso]", 0, cputsc, realtsc, SAFE)) {
+                    TRACE("vtss_record_module() FAIL");
+                    repeat = 1;
+                }
+            }
+        }
+        if (!is_vdso_found && mm->context.vdso) {
+            TRACE("addr=0x%p, size=%lu, name='%s', pgoff=%lu", mm->context.vdso, PAGE_SIZE, "[vdso]", 0UL);
+            if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, tskd->m32, (unsigned long)((size_t)mm->context.vdso), PAGE_SIZE, "[vdso]", 0, cputsc, realtsc, SAFE)) {
+                TRACE("vtss_record_module() FAIL");
+                repeat = 1;
+            }
+        }
+        up_read(&mm->mmap_sem);
+        VTSS_CLEAR_MMAP_INIT(tskd);
+        mmput(mm);
+    }
+    if (tmp)
+        vtss_free_page((unsigned long)tmp);
+    return repeat;
+}
+
+void vtss_mmap(struct file *file, unsigned long addr, unsigned long pgoff, unsigned long size)
+{
+    vtss_task_map_item_t* item = NULL;
+
+    if (!VTSS_COLLECTOR_IS_READY_OR_INITING){
+        return;
+    }
+    atomic_inc(&vtss_kernel_task_in_progress);
+    if ((atomic_read(&vtss_transport_state) == 1) && VTSS_COLLECTOR_IS_READY_OR_INITING) {
+        item = vtss_task_map_get_item(TASK_TID(current));
+        if (item != NULL) {
+            struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+            atomic_inc(&vtss_mmap_reg_callcnt);
+            if ((!VTSS_IS_COMPLETE(tskd)) && (!VTSS_IS_MMAP_INIT(tskd))) {
+#ifdef VTSS_USE_NMI
+                // we cannot allocate memory in irqs_disabled mode
+                char tmp[960];
+#else
+                char *tmp = (char*)vtss_get_free_page(GFP_NOWAIT);
+#endif
+                long long cputsc, realtsc;
+                vtss_time_get_sync(&cputsc, &realtsc);
+
+                if (tmp != NULL) {
+#ifdef VTSS_USE_NMI
+                    char* pname = D_PATH(file, tmp, 960);
+#else
+                    char* pname = D_PATH(file, tmp, PAGE_SIZE);
+#endif
+                    if (!IS_ERR(pname)) {
+                        TRACE("vma=[0x%lx - 0x%lx], file='%s', pgoff=%lu", addr, addr+size, pname, pgoff);
+                        if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, tskd->m32, addr, size, pname, pgoff, cputsc, realtsc, SAFE)) {
+                            TRACE("vtss_record_module() FAIL");
+                        }
+                    }
+#ifndef VTSS_USE_NMI
+                    vtss_free_page((unsigned long)tmp);
+#endif
+                }
+            }
+            vtss_task_map_put_item(item);
+        }
+    }
+    atomic_dec(&vtss_kernel_task_in_progress);
+}
+
+#if 0
+void vtss_mmap_reload(struct file *file, unsigned long addr)
+{
+    vtss_task_map_item_t* item = vtss_task_map_get_item(TASK_TID(current));
+
+    atomic_inc(&vtss_transport_busy);
+    if (atomic_read(&vtss_transport_initialized) == 1)  {
+        if (item != NULL) {
+            struct mm_struct *mm =  current->mm;
+            if (mm != NULL) {
+                struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+                if (unlikely(VTSS_IS_COMPLETE(tskd)))
+                    tskd = vtss_wait_for_completion(&item);
+                if (tskd != NULL) {
+                    char *pname, *tmp = (char*)vtss_get_free_page(GFP_NOWAIT);
+                    if (tmp != NULL)
+                    {
+                        long long cputsc, realtsc;
+                        struct vm_area_struct* vma = NULL;
+                        down_read(&mm->mmap_sem);
+                        vma = find_vma(mm, addr);
+                        vtss_time_get_sync(&cputsc, &realtsc);
+                        if (vma!=0 && tmp!=0 && (vma->vm_flags & VM_EXEC) && !(vma->vm_flags & VM_WRITE) &&
+                                                                vma->vm_file && vma->vm_file->f_path.dentry)
+                        {
+                            pname = D_PATH(vma->vm_file, tmp, PAGE_SIZE);
+                            if (!IS_ERR(pname)) {
+                               TRACE("addr=0x%lx, size=%lu, file='%s', pgoff=%lu", vma->vm_start, (vma->vm_end - vma->vm_start), pname, vma->vm_pgoff);
+                               if (vtss_record_module(VTSS_PT_FLUSH_MODE ? tskd->trnd : tskd->trnd_aux, tskd->m32, vma->vm_start, (vma->vm_end - vma->vm_start), pname, vma->vm_pgoff, cputsc, realtsc, SAFE)) {
+                                   TRACE("vtss_record_module() FAIL");
+                               }
+                            }
+                        }
+                        up_read(&mm->mmap_sem);
+                        vtss_free_page((unsigned long)tmp);
+                    }
+                }
+            }
+            vtss_task_map_put_item(item);
+        }
+    }
+    atomic_dec(&vtss_transport_busy);
+}
+#endif
+
+static void vtss_dump_stack(struct vtss_task_data* tskd, struct task_struct* task, struct pt_regs* regs, void* bp, int clear)
+{
+    if (task != current) {
+        return;
+    }
+    if (likely(!VTSS_IS_COMPLETE(tskd) &&
+        (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_STACKS) &&
+        VTSS_IS_VALID_TASK(task) &&
+        tskd->stk.trylock(&tskd->stk)))
+    {
+        void* reg_bp;
+        reg_bp = regs ? (void*)REG(bp, regs) : bp;
+        if (!reg_bp && task == current){
+            unsigned long bp_val;
+            vtss_get_current_bp(bp_val);
+            reg_bp = (void*)bp_val;
+        }
+        /* Clear stack history if stack was not stored in trace */
+        if (unlikely(VTSS_NEED_STACK_SAVE(tskd)) || clear)
+            tskd->stk.clear(&tskd->stk);
+        VTSS_PROFILE(stk, VTSS_STACK_DUMP(tskd, task, regs, reg_bp, IN_IRQ));
+
+        if (likely(!VTSS_ERROR_STACK_DUMP(tskd))) {
+            VTSS_STORE_STATE(tskd, 1, VTSS_ST_STKSAVE);
+        } else {
+            tskd->stk.clear(&tskd->stk);
+        }
+        tskd->stk.unlock(&tskd->stk);
+    }
+}
+
+static void vtss_sched_switch_from(vtss_task_map_item_t* item, struct task_struct* task, void* bp, void* ip)
+{
+    unsigned long flags;
+    int state = atomic_read(&vtss_collector_state);
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+    int is_preempt = (task->state == TASK_RUNNING) ? 1 : 0;
+    int cpu;
+    unsigned long start_rec_id = tskd->start_rec_id;
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+
+#ifdef VTSS_DEBUG_STACOAK
+    unsigned long stack_size = ((unsigned long)(&stack_size)) & (THREAD_SIZE-1);
+    if (unlikely(stack_size < (VTSS_MIN_STACK_SPACE + sizeof(struct thread_info)))) {
+        ERROR("(%d:%d): LOW STACK %lu", TASK_PID(current), TASK_TID(current), stack_size);
+        vtss_profiling_pause();
+        tskd->state &= ~VTSS_ST_PMU_SET;
+        return;
+    }
+#endif
+    if (unlikely(!vtss_cpu_active(cpu) || VTSS_IS_COMPLETE(tskd))) {
+        vtss_profiling_pause();
+        tskd->state &= ~VTSS_ST_PMU_SET;
+        return;
+    }
+
+    local_irq_save(flags);
+    preempt_disable();
+    vtss_lbr_disable_save(&tskd->lbr);
+    /* read and freeze cpu counters if ... */
+    if (likely((state == VTSS_COLLECTOR_RUNNING || VTSS_IN_CONTEXT(tskd)) &&
+                VTSS_IS_PMU_SET(tskd))){
+        VTSS_PROFILE(pmu, vtss_cpuevents_sample(tskd->cpuevent_chain));
+        if ((reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) && (reqcfg.ipt_cfg.mode & vtss_iptmode_full)){
+            VTSS_PROFILE(bts, vtss_dump_ipt(VTSS_PT_FLUSH_MODE ? tskd->trnd_aux : tskd->trnd, tskd->tid, tskd->cpu, 0));
+        }
+    }
+    tskd->state &= ~VTSS_ST_PMU_SET;
+    { /* update and restart system counters always but with proper flag */
+        int flag = (state == VTSS_COLLECTOR_RUNNING || VTSS_IN_CONTEXT(tskd)) ?
+                (is_preempt ? 2 : 3) : (is_preempt ? -2 : -3);
+        /* set correct TCB for following vtss_cpuevents_quantum_border() */
+        pcb_cpu.tcb_ptr = &tskd->tcb;
+        /* update system counters */
+        VTSS_PROFILE(sys, vtss_cpuevents_quantum_border(tskd->cpuevent_chain, flag));
+        pcb_cpu.tcb_ptr = NULL;
+    }
+    /* store swap-out record */
+    if (ip) tskd->from_ip = ip;
+    if (likely(VTSS_IN_CONTEXT(tskd))) {
+        if (1/*(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX) || (VTSS_IS_STATE_SET(tskd, VTSS_ST_REC_CTX))*/){
+            VTSS_STORE_SWAPOUT(tskd, is_preempt, NOT_SAFE);}
+        else
+            VTSS_STORE_STATE(tskd, 0, VTSS_ST_SWAPOUT);
+#ifdef VTSS_RECOVERY_LOGIC
+        if (likely(!VTSS_ERROR_STORE_SWAPOUT(tskd))) {
+            vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+            per_cpu(vtss_recovery_tskd, tskd->cpu) = NULL;
+            vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+            tskd->state &= ~VTSS_ST_IN_CONTEXT;
+        }
+#else
+        tskd->state &= ~VTSS_ST_IN_CONTEXT;
+#endif
+    }
+    if (likely(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX)) {
+        vtss_dump_stack(tskd, task, NULL, bp, start_rec_id < tskd->start_rec_id);
+    }
+    preempt_enable_no_resched();
+    local_irq_restore(flags);
+}
+
+static void vtss_sched_switch_to(vtss_task_map_item_t* item, struct task_struct* task, void* ip)
+{
+    int cpu;
+    unsigned long flags;
+    int state = atomic_read(&vtss_collector_state);
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+
+#ifdef VTSS_DEBUG_STACK
+    unsigned long stack_size = ((unsigned long)(&stack_size)) & (THREAD_SIZE-1);
+    if (unlikely(stack_size < (VTSS_MIN_STACK_SPACE + sizeof(struct thread_info)))) {
+        ERROR("(%d:%d): LOW STACK %lu", TASK_PID(current), TASK_TID(current), stack_size);
+        vtss_profiling_pause();
+        tskd->state &= ~VTSS_ST_PMU_SET;
+        return;
+    }
+#endif
+
+    if (unlikely(!vtss_cpu_active(cpu) || VTSS_IS_COMPLETE(tskd))) {
+        vtss_profiling_pause();
+        tskd->state &= ~VTSS_ST_PMU_SET;
+#ifdef VTSS_RECOVERY_LOGIC
+        vtss_clear_recovery(tskd);
+#endif
+        return;
+    }
+    local_irq_save(flags);
+    preempt_disable();
+    { /* update and restart system counters always but with proper flag */
+        int flag = (state == VTSS_COLLECTOR_RUNNING) ? 1 : -1;
+        /* set correct TCB for following vtss_cpuevents_quantum_border() */
+        pcb_cpu.tcb_ptr = &tskd->tcb;
+        VTSS_PROFILE(sys, vtss_cpuevents_quantum_border(tskd->cpuevent_chain, flag));
+        pcb_cpu.tcb_ptr = NULL;
+    }
+    /* recover logic */
+    if (unlikely(VTSS_NEED_STORE_NEWTASK(tskd)))
+        VTSS_STORE_NEWTASK(tskd, NOT_SAFE);
+    if (unlikely(VTSS_NEED_STORE_SOFTCFG(tskd)))
+        VTSS_STORE_SOFTCFG(tskd, NOT_SAFE);
+    if (unlikely(VTSS_NEED_STORE_PAUSE(tskd)))
+        VTSS_STORE_PAUSE(tskd, tskd->cpu, 0x66 /* tpss_pi___itt_pause from TPSS ini-file */, SAFE);
+    if (likely(VTSS_IN_NEWTASK(tskd))) {
+        /* Exit from context on CPU if error was */
+        struct vtss_task_data* cpu_tskd = NULL;
+#ifdef VTSS_RECOVERY_LOGIC
+        unsigned long flags;
+        vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+        cpu_tskd = per_cpu(vtss_recovery_tskd, cpu);
+        if (unlikely(1/*(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX)*/ &&
+            cpu_tskd != NULL &&
+            VTSS_IN_CONTEXT(cpu_tskd) &&
+            VTSS_ERROR_STORE_SWAPOUT(cpu_tskd)))
+        {
+            unsigned long start_rec_id = cpu_tskd->start_rec_id;
+            VTSS_STORE_SWAPOUT(cpu_tskd, 1, NOT_SAFE);
+            if (start_rec_id < cpu_tskd->start_rec_id){
+              //Put clear stack one more time after switch to
+              VTSS_STORE_STATE(cpu_tskd, 1, VTSS_ST_STKSAVE);
+            }
+            if (likely(!VTSS_ERROR_STORE_SWAPOUT(cpu_tskd))) {
+                per_cpu(vtss_recovery_tskd, cpu) = NULL;
+                cpu_tskd->state &= ~VTSS_ST_IN_CONTEXT;
+                cpu_tskd = NULL;
+            }
+        }
+        vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+        /* Exit from context for the task if error was */
+        if (unlikely(VTSS_IN_CONTEXT(tskd) && VTSS_ERROR_STORE_SWAPOUT(tskd))) {
+            if (1/*reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX*/){
+                unsigned long start_rec_id = tskd->start_rec_id;
+                VTSS_STORE_SWAPOUT(tskd, 1, NOT_SAFE);
+                if (start_rec_id < tskd->start_rec_id){
+                  //Put clear stack one more time after switch to
+                  VTSS_STORE_STATE(tskd, 1, VTSS_ST_STKSAVE);
+                }
+            }else VTSS_STORE_STATE(tskd, 0, VTSS_ST_SWAPOUT);
+            if (likely(!VTSS_ERROR_STORE_SWAPOUT(tskd))) {
+                vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+                per_cpu(vtss_recovery_tskd, tskd->cpu) = NULL;
+                vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+                tskd->state &= ~VTSS_ST_IN_CONTEXT;
+                if (unlikely(cpu == tskd->cpu))
+                    cpu_tskd = NULL;
+            }
+        }
+#endif
+        /* Enter in context for the task if: */
+        if (likely(cpu_tskd == NULL && /* CPU is free      */
+            !VTSS_IN_CONTEXT(tskd)  && /* in correct state */
+            state == VTSS_COLLECTOR_RUNNING))
+        {
+            unsigned long start_rec_id = tskd->start_rec_id;
+#ifdef VTSS_KERNEL_CONTEXT_SWITCH
+            if ( !ip )
+            {
+                ip = tskd->from_ip;
+            }
+            /* Use user IP if no stacks on context switches */
+            if (!ip || !(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX))
+#endif
+                ip = (void*)KSTK_EIP(task);
+            if (1/*reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX || (cpu != tskd->cpu) || (tskd->state&VTSS_ST_CPU_CHANGE)*/) {
+                if (reqcfg.cpuevent_count_v1 !=0) VTSS_STORE_SAMPLE(tskd, tskd->cpu, NULL, NOT_SAFE);
+                VTSS_STORE_SWAPIN(tskd, cpu, ip, NOT_SAFE/*, (cpu == tskd->cpu ? 0 : VTSS_ST_REC_CTX)*/);
+            } else {
+                if (reqcfg.cpuevent_count_v1 !=0) VTSS_STORE_SAMPLE(tskd, tskd->cpu, ip, NOT_SAFE);
+                VTSS_STORE_STATE(tskd, 0, VTSS_ST_SWAPIN);
+            }
+
+#ifdef VTSS_RECOVERY_LOGIC
+            if (likely(!VTSS_ERROR_STORE_SWAPIN(tskd))) {
+                vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+                per_cpu(vtss_recovery_tskd, cpu) = tskd;
+                vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+#else
+            {
+#endif
+                tskd->state |= VTSS_ST_IN_CONTEXT;
+                tskd->cpu = cpu;
+                if (unlikely(!VTSS_NEED_STACK_SAVE(tskd))) {
+                    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX) {
+                        vtss_dump_stack(tskd, task, NULL, NULL, start_rec_id < tskd->start_rec_id);
+                    }
+                }
+                if (likely(VTSS_NEED_STACK_SAVE(tskd) &&
+                    (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_STACKS) &&
+                    !vtss_transport_is_overflowing(tskd->trnd) &&
+                    tskd->stk.trylock(&tskd->stk)))
+                {
+                    VTSS_STACK_SAVE(tskd, NOT_SAFE);
+                    tskd->stk.unlock(&tskd->stk);
+                }
+            }
+        }
+    } else {
+        tskd->state |= VTSS_ST_SWAPIN;
+    }
+    VTSS_STORE_STATE(tskd, 1, VTSS_ST_CPUEVT);
+    vtss_profiling_resume(item, 0);
+    preempt_enable_no_resched();
+    local_irq_restore(flags);
+}
+
+#if defined(CONFIG_PREEMPT_NOTIFIERS) && defined(VTSS_USE_PREEMPT_NOTIFIERS)
+
+static void vtss_notifier_sched_out(struct preempt_notifier *notifier, struct task_struct *next)
+{
+    vtss_task_map_item_t* item;
+
+    vtss_profiling_pause();
+    item = vtss_task_map_get_item(TASK_TID(current));
+    if (item != NULL) {
+        unsigned long bp;
+        vtss_get_current_bp(bp);
+        VTSS_PROFILE(ctx, vtss_sched_switch_from(item, current, (void*)bp, 0));
+        vtss_task_map_put_item(item);
+    }
+}
+
+static void vtss_notifier_sched_in(struct preempt_notifier *notifier, int cpu)
+{
+    vtss_task_map_item_t* item = vtss_task_map_get_item(TASK_TID(current));
+
+    if (item != NULL) {
+        void* bp;
+        vtss_get_current_bp(bp);
+        VTSS_PROFILE(ctx, vtss_sched_switch_to(item, current, (void*)_THIS_IP_));
+        vtss_task_map_put_item(item);
+    } else {
+        vtss_profiling_pause();
+    }
+}
+
+#endif
+
+
+void vtss_sched_switch(struct task_struct* prev, struct task_struct* next, void* prev_bp, void* prev_ip)
+{
+    vtss_task_map_item_t *item;
+
+    vtss_profiling_pause();
+    item = vtss_task_map_get_item(TASK_TID(prev));
+    if (item != NULL) {
+        VTSS_PROFILE(ctx, vtss_sched_switch_from(item, prev, prev_bp, prev_ip));
+        vtss_task_map_put_item(item);
+    }
+    item = vtss_task_map_get_item(TASK_TID(next));
+    if (item != NULL) {
+        VTSS_PROFILE(ctx, vtss_sched_switch_to(item, next, 0));
+        vtss_task_map_put_item(item);
+    }
+}
+
+static void vtss_pmi_dump(struct pt_regs* regs, vtss_task_map_item_t* item, int is_bts_overflowed)
+{
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+    int cpu;
+
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+
+    VTSS_STORE_STATE(tskd, !is_bts_overflowed, VTSS_ST_CPUEVT);
+    if (likely(VTSS_IS_CPUEVT(tskd))) {
+        /* fetch PEBS.IP, if available, or continue as usual */
+        vtss_pebs_t* pebs = vtss_pebs_get(cpu);
+        if (pebs != NULL) {
+            TRACE("ip = %p, pebs_ip = %llx, eventing_ip = %llx", tskd->ip, pebs->v1.ip, pebs->v3.eventing_ip );
+            if (0) { //vtss_pebs_is_trap()){
+                /* correct the trap-IP - disabled to be consistent with SEP   */
+                /* tskd->ip = vtss_lbr_correct_ip((void*)((size_t)pebs->v1.ip)); */
+                if (vtss_pebs_record_size == sizeof(pebs->v3)){
+                    tskd->ip = (void*)((size_t)pebs->v3.eventing_ip);
+                } else {
+                    tskd->ip = (void*)((size_t)pebs->v1.ip);
+                }
+                TRACE("in trap");
+            }
+            else {
+                /* fault-IP is already correct */
+                if (vtss_pebs_record_size == sizeof(pebs->v3)){
+                    tskd->ip = (void*)((size_t)pebs->v3.eventing_ip);
+                } else {
+                    tskd->ip = (void*)((size_t)pebs->v1.ip);
+                }
+                TRACE("fault-IP is already correct");
+            }
+        } else
+            tskd->ip = vtss_lbr_correct_ip((void*)instruction_pointer(regs));
+        if (likely(VTSS_IS_PMU_SET(tskd))) {
+            VTSS_PROFILE(pmu, vtss_cpuevents_sample(tskd->cpuevent_chain));
+            tskd->state &= ~VTSS_ST_PMU_SET;
+        }
+    }
+#ifndef VTSS_NO_BTS
+    /* dump trailing BTS buffers */
+    if (unlikely(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_BRANCH)) {
+        VTSS_PROFILE(bts, tskd->bts_size = vtss_bts_dump(tskd->bts_buff));
+        vtss_bts_disable();
+    }else if (unlikely(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT)) {
+        vtss_disable_ipt();
+    }
+#endif
+}
+
+static void vtss_pmi_record(struct pt_regs* regs, vtss_task_map_item_t* item, int is_bts_overflowed)
+{
+    int cpu;
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+   
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+#ifdef VTSS_DEBUG_STACK
+    unsigned long stack_size = ((unsigned long)(&stack_size)) & (THREAD_SIZE-1);
+    if (unlikely(stack_size < (VTSS_MIN_STACK_SPACE + sizeof(struct thread_info)))) {
+        ERROR("(%d:%d): LOW STACK %lu", TASK_PID(current), TASK_TID(current), stack_size);
+        return;
+    }
+#endif
+    if (unlikely(VTSS_NEED_STORE_NEWTASK(tskd)))
+        VTSS_STORE_NEWTASK(tskd, NOT_SAFE);
+    if (unlikely(VTSS_NEED_STORE_SOFTCFG(tskd)))
+        VTSS_STORE_SOFTCFG(tskd, NOT_SAFE);
+    if (unlikely(VTSS_NEED_STORE_PAUSE(tskd)))
+        VTSS_STORE_PAUSE(tskd, tskd->cpu, 0x66 /* tpss_pi___itt_pause from TPSS ini-file */, SAFE);
+    if (likely(VTSS_IN_NEWTASK(tskd))) {
+#ifdef VTSS_RECOVERY_LOGIC
+        /* Exit from context on CPU if error was */
+        unsigned long flags;
+        struct vtss_task_data* cpu_tskd = NULL;
+
+        vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+        cpu_tskd = per_cpu(vtss_recovery_tskd, cpu);
+        if (unlikely(1/*(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX)*/ &&
+            cpu_tskd != NULL && cpu_tskd != tskd &&
+            VTSS_IN_CONTEXT(cpu_tskd) &&
+            VTSS_ERROR_STORE_SWAPOUT(cpu_tskd)))
+        {
+            unsigned long start_rec_id = cpu_tskd->start_rec_id;
+            VTSS_STORE_SWAPOUT(cpu_tskd, 1, NOT_SAFE);
+            if ( start_rec_id < cpu_tskd->start_rec_id){
+                VTSS_STORE_STATE(cpu_tskd, 1, VTSS_ST_STKSAVE);
+            }
+            if (likely(!VTSS_ERROR_STORE_SWAPOUT(cpu_tskd))) {
+                per_cpu(vtss_recovery_tskd, cpu) = NULL;
+                cpu_tskd->state &= ~VTSS_ST_IN_CONTEXT;
+                cpu_tskd = NULL;
+            }
+        }
+        vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+        /* Enter in context for the task if CPU is free and no error */
+        if (unlikely(cpu_tskd == NULL &&
+            !VTSS_IN_CONTEXT(tskd) &&
+            VTSS_ERROR_STORE_SWAPIN(tskd) &&
+            atomic_read(&vtss_collector_state) == VTSS_COLLECTOR_RUNNING))
+        {
+            unsigned long  start_rec_id = tskd->start_rec_id;
+            if (1/*(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX) || (cpu != tskd->cpu)||(tskd->state&VTSS_ST_CPU_CHANGE)*/){
+                VTSS_STORE_SWAPIN(tskd, cpu, (void*)instruction_pointer(regs), NOT_SAFE/*, (cpu == tskd->cpu ? 0 : VTSS_ST_REC_CTX)*/);
+            }
+            else
+                VTSS_STORE_STATE(tskd, 0, VTSS_ST_SWAPIN);
+            if (start_rec_id < tskd->start_rec_id){
+                VTSS_STORE_STATE(tskd, 1, VTSS_ST_STKSAVE);
+            }
+
+            if (likely(!VTSS_ERROR_STORE_SWAPIN(tskd))) {
+                vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+                per_cpu(vtss_recovery_tskd, cpu) = tskd;
+                vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+                tskd->state |= VTSS_ST_IN_CONTEXT;
+                tskd->cpu = cpu;
+                if (unlikely(VTSS_NEED_STACK_SAVE(tskd) && 
+                    (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CTX) &&
+                    (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_STACKS) &&
+                    !vtss_transport_is_overflowing(tskd->trnd) &&
+                    tskd->stk.trylock(&tskd->stk)))
+                {
+                    VTSS_STACK_SAVE(tskd, NOT_SAFE);
+                    tskd->stk.unlock(&tskd->stk);
+                }
+            }
+            if (start_rec_id < tskd->start_rec_id){
+                VTSS_STORE_STATE(tskd, 1, VTSS_ST_STKSAVE);
+            }
+        }
+#endif
+    } else {
+        tskd->state |= VTSS_ST_SWAPIN;
+    }
+    if (likely(VTSS_IN_CONTEXT(tskd))) {
+#ifndef VTSS_NO_BTS
+        if (!is_bts_overflowed) 
+        {
+            if (unlikely (tskd->bts_size)){
+                VTSS_PROFILE(bts, vtss_record_bts(tskd->trnd, tskd->tid, tskd->cpu, tskd->bts_buff, tskd->bts_size, 0));
+                tskd->bts_size = 0;
+            }
+        }
+#endif
+        
+        if (unlikely((reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) &&
+            !VTSS_ERROR_STORE_SAMPLE(tskd) &&
+            !(VTSS_ERROR_STACK_DUMP(tskd)&&(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_STACKS)) &&
+            !VTSS_ERROR_STACK_SAVE(tskd)))
+        {
+            VTSS_PROFILE(bts, vtss_dump_ipt(VTSS_PT_FLUSH_MODE ? tskd->trnd_aux : tskd->trnd, tskd->tid, tskd->cpu, 0));
+        }
+
+        if (likely(VTSS_IS_CPUEVT(tskd))) {
+            void* ip = VTSS_ERROR_STORE_SAMPLE(tskd) ? (void*)VTSS_EVENT_LOST_MODULE_ADDR : tskd->ip;
+            unsigned long  start_rec_id = tskd->start_rec_id;
+            VTSS_STORE_SAMPLE(tskd, tskd->cpu, ip, NOT_SAFE);
+            
+            if (likely(!VTSS_ERROR_STORE_SAMPLE(tskd)))
+            {
+                vtss_dump_stack(tskd, current, regs, NULL, start_rec_id < tskd->start_rec_id);
+                if (likely(VTSS_NEED_STACK_SAVE(tskd) &&
+                    (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_STACKS) &&
+                    !vtss_transport_is_overflowing(tskd->trnd) &&
+                    tskd->stk.trylock(&tskd->stk)))
+                {
+                    VTSS_STACK_SAVE(tskd, NOT_SAFE);
+                    tskd->stk.unlock(&tskd->stk);
+                }
+            }
+        }
+        if (unlikely((reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) &&
+            !VTSS_ERROR_STORE_SAMPLE(tskd) &&
+            !(VTSS_ERROR_STACK_DUMP(tskd)&&(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_STACKS)) &&
+            !VTSS_ERROR_STACK_SAVE(tskd)))
+        {
+            ;
+            //INFO("dump PT");
+            //VTSS_PROFILE(bts, vtss_dump_ipt(VTSS_PT_FLUSH_MODE ? tskd->trnd_aux : tskd->trnd, tskd->tid, tskd->cpu, 0));
+        }
+#ifndef VTSS_NO_BTS
+        else if (unlikely(is_bts_overflowed && tskd->bts_size &&
+            (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_BRANCH) &&
+            !VTSS_ERROR_STORE_SAMPLE(tskd) &&
+            !(VTSS_ERROR_STACK_DUMP(tskd)&&(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_STACKS)) &&
+            !VTSS_ERROR_STACK_SAVE(tskd)))
+        {
+            VTSS_PROFILE(bts, vtss_record_bts(tskd->trnd, tskd->tid, tskd->cpu, tskd->bts_buff, tskd->bts_size, 0));
+            tskd->bts_size = 0;
+        }
+#endif
+    }
+
+}
+
+static int is_callcount_overflowed(void)
+{
+    int cpu;
+
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+
+    if (reqcfg.trace_cfg.trace_flags & (VTSS_CFGTRACE_IPT)) return vtss_has_ipt_overflowed();
+    return vtss_bts_overflowed(cpu);
+}
+/**
+ * CPU event counter overflow handler and BTS/PEBS buffer overflow handler
+ * sample counter values, form the trace record
+ * select a new mux group (if applicable)
+ * program event counters
+ * NOTE: LBR/BTS/PEBS is already disabled in vtss_perfvec_handler()
+ */
+asmlinkage void vtss_pmi_handler(struct pt_regs *regs)
+{
+    unsigned long flags = 0;
+    int is_bts_overflowed = 0;
+    int bts_enable = 0;
+    vtss_task_map_item_t* item = NULL;
+
+#ifndef VTSS_USE_NMI
+    if (unlikely(!vtss_apic_read_priority())) {
+        ERROR("INT 0xFE was called");
+        return;
+    }
+#endif
+    local_irq_save(flags);
+    preempt_disable();
+#ifndef VTSS_NO_BTS
+    is_bts_overflowed = is_callcount_overflowed();
+#endif
+    if (likely(!is_bts_overflowed))
+        vtss_cpuevents_freeze();
+    if (likely(VTSS_IS_VALID_TASK(current)))
+        item = vtss_task_map_get_item(TASK_TID(current));
+    if (likely(item != NULL)) {
+        struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+        bts_enable = is_callcount_enable(tskd);
+        VTSS_PROFILE(pmi, vtss_pmi_dump(regs, item, is_bts_overflowed));
+    } else {
+        vtss_profiling_pause();
+    }
+#ifndef VTSS_USE_NMI
+    vtss_apic_ack_eoi();
+#endif
+    vtss_pmi_enable();
+    if (likely(item != NULL)) {
+        VTSS_PROFILE(pmi, vtss_pmi_record(regs, item, is_bts_overflowed));
+        vtss_profiling_resume(item, bts_enable);
+        vtss_task_map_put_item(item);
+    }
+    preempt_enable_no_resched();
+    local_irq_restore(flags);
+}
+
+/* ------------------------------------------------------------------------- */
+
+#ifdef VTSS_DEBUG_PROFILE
+cycles_t vtss_profile_cnt_stk  = 0;
+cycles_t vtss_profile_clk_stk  = 0;
+cycles_t vtss_profile_cnt_ctx  = 0;
+cycles_t vtss_profile_clk_ctx  = 0;
+cycles_t vtss_profile_cnt_pmi  = 0;
+cycles_t vtss_profile_clk_pmi  = 0;
+cycles_t vtss_profile_cnt_pmu  = 0;
+cycles_t vtss_profile_clk_pmu  = 0;
+cycles_t vtss_profile_cnt_sys  = 0;
+cycles_t vtss_profile_clk_sys  = 0;
+cycles_t vtss_profile_cnt_bts  = 0;
+cycles_t vtss_profile_clk_bts  = 0;
+cycles_t vtss_profile_cnt_vma  = 0;
+cycles_t vtss_profile_clk_vma  = 0;
+cycles_t vtss_profile_cnt_pgp  = 0;
+cycles_t vtss_profile_clk_pgp  = 0;
+cycles_t vtss_profile_cnt_cpy  = 0;
+cycles_t vtss_profile_clk_cpy  = 0;
+cycles_t vtss_profile_cnt_vld  = 0;
+cycles_t vtss_profile_clk_vld  = 0;
+cycles_t vtss_profile_cnt_unw  = 0;
+cycles_t vtss_profile_clk_unw  = 0;
+#endif
+
+int vtss_cmd_open(void)
+{
+    return 0;
+}
+
+int vtss_cmd_close(void)
+{
+    return 0;
+}
+
+
+static int vtss_cmd_set_target_task(struct task_struct *task);
+
+static int vtss_cmd_set_target_task(struct task_struct *task)
+{
+    int rc = -EINVAL;
+    int state = atomic_read(&vtss_collector_state);
+
+    if (state == VTSS_COLLECTOR_RUNNING || state == VTSS_COLLECTOR_PAUSED) {
+        struct task_struct *p;
+        if (task != NULL) {
+            char *tmp = NULL;
+            char *pathname = NULL;
+            struct mm_struct *mm;
+            struct pid *pgrp;
+
+            pgrp = get_pid(task->pids[PIDTYPE_PID].pid);
+            if ((mm = get_task_mm(task)) != NULL) {
+                struct file *exe_file = mm->exe_file;
+                mmput(mm);
+                if (exe_file) {
+                    get_file(exe_file);
+                    tmp = (char*)vtss_get_free_page(GFP_KERNEL);
+                    if (tmp) {
+                        pathname = d_path(&exe_file->f_path, tmp, PAGE_SIZE);
+                        if (!IS_ERR(pathname)) {
+                            char *p = strrchr(pathname, '/');
+                            pathname = p ? p+1 : pathname;
+                        } else {
+                            pathname = NULL;
+                        }
+                    }
+                    fput(exe_file);
+                }
+            }
+            rc = -ENOENT;
+            do_each_pid_thread(pgrp, PIDTYPE_PID, p) {
+                DEBUG_COLLECTOR("profile the process <%d>: tid=%d, pid=%d, ppid = %d, pathname='%s'", TASK_PID(task), TASK_TID(task), TASK_PID(task), TASK_PID(TASK_PARENT(p)),pathname);
+                if (!vtss_target_new(TASK_TID(p), TASK_PID(p), TASK_PID(TASK_PARENT(p)), pathname, -1, -1)) {
+                    rc = 0;
+                }
+            } while_each_pid_thread(pgrp, /*PIDTYPE_MAX*/PIDTYPE_PID, p);
+
+            if (rc != 0) {
+                ERROR("Error: cannot profile the process <%d>: tid=%d, pid=%d,  pathname='%s'", TASK_PID(task), TASK_TID(task), TASK_PID(task), pathname);
+            }
+            put_pid(pgrp);
+            if (tmp)
+                vtss_free_page((unsigned long)tmp);
+        } else
+            rc = -ENOENT;
+    }
+    return rc;
+}
+
+int vtss_cmd_set_target(pid_t pid)
+{
+    int rc = -EINVAL;
+    struct task_struct *task = vtss_find_task_by_tid(pid);
+    rc = vtss_cmd_set_target_task(task);
+    return rc;
+}
+static void vtss_collector_pmi_disable_on_cpu(void *ctx)
+{
+    vtss_pmi_disable();
+}
+
+
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_transport_fini_work(struct work_struct *work)
+#else
+static void vtss_transport_fini_work(void *work)
+#endif
+{
+    DEBUG_COLLECTOR("start");
+    vtss_transport_fini();
+    atomic_set(&vtss_transport_state, 0);
+    vtss_procfs_ctrl_wake_up(NULL, 0);
+    if (work) vtss_kfree(work);
+    DEBUG_COLLECTOR("end");
+}
+static void wait_transport_fini(void)
+{
+    int cnt = 0;
+    DEBUG_COLLECTOR("1, transport state = %d", atomic_read(&vtss_transport_state));
+    while (atomic_read(&vtss_transport_state) != 0){
+        cnt++;
+    }
+    DEBUG_COLLECTOR("2, cnt = %d", cnt);
+    return;
+}
+
+
+static int vtss_callcount_init(void)
+{
+    int rc = 0;
+    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) rc = vtss_ipt_init();
+    else  rc = vtss_bts_init(reqcfg.bts_cfg.brcount);
+    return rc;
+}
+
+static void vtss_callcount_fini(void)
+{
+    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) vtss_ipt_fini();
+    else vtss_bts_fini();
+}
+
+static void vtss_target_complete_item(vtss_task_map_item_t* item, void* args)
+{
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+    if (tskd) tskd->state |= VTSS_ST_COMPLETE;
+}
+
+#ifndef VTSS_USE_NMI
+
+#define VTSS_CLR_PEBS_OVF 0x4000000000000000ULL
+#define VTSS_CLR_STATUS_PEBS_OVF 0x4000000000000000ULL
+#define MSR_PERF_GLOBAL_OVF_CTRL 0x390
+#define IA32_PERF_GLOBAL_STATUS 0x38e
+
+static void vtss_collector_check_status(void *ctx)
+{
+// workaround for RHEL 6 update 5 for HSW/BDW EP machines
+// kernel panic happens without calling this function
+    unsigned long long val = 0;
+
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f){
+        rdmsrl(IA32_PERF_GLOBAL_STATUS, val);
+        if (val & VTSS_CLR_STATUS_PEBS_OVF){
+            wrmsrl(MSR_PERF_GLOBAL_OVF_CTRL,VTSS_CLR_PEBS_OVF);
+            if (ctx)(*((int*)ctx))++;
+            vtss_pmi_enable();
+        }
+    }
+}
+
+static void vtss_collector_freeze_events(void *ctx)
+{
+    unsigned long flags, flags_rec;
+    int cpu;
+    struct vtss_task_data* tskd;
+    local_irq_save(flags);
+    preempt_disable();
+    
+//    vtss_bts_init_dsa();
+    vtss_profiling_pause();
+    
+    vtss_collector_check_status(NULL);
+
+#ifdef VTSS_RECOVERY_LOGIC
+    cpu = smp_processor_id();
+    vtss_spin_lock_irqsave(&vtss_recovery_lock, flags_rec);
+    if ((tskd = per_cpu(vtss_recovery_tskd, cpu)) != NULL){
+        VTSS_PROFILE(pmu, vtss_cpuevents_sample(tskd->cpuevent_chain));
+        if ((reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) && (reqcfg.ipt_cfg.mode & vtss_iptmode_full)) {
+            VTSS_PROFILE(bts, vtss_dump_ipt(VTSS_PT_FLUSH_MODE ? tskd->trnd_aux : tskd->trnd, tskd->tid, tskd->cpu, 0));
+        }
+    }
+    vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags_rec);
+//        INFO("end recovering...");
+#endif
+    vtss_pmi_disable();
+    vtss_bts_init_dsa();
+
+    preempt_enable_no_resched();
+    local_irq_restore(flags);
+}
+#endif /* VTSS_USE_NMI */
+
+int vtss_collection_fini(void)
+{
+    int i = 0;
+
+    while (atomic_read(&vtss_kernel_task_in_progress) != 0){
+        i++;
+        if (i==1000) ERROR("Kernel task is not finished! vtss_kernel_task_in_progress = %d", atomic_read(&vtss_kernel_task_in_progress));
+    }
+    DEBUG_COLLECTOR("All kernel tasks finished i = %d, no new items can be added from now", i);
+
+    if (atomic_read(&vtss_target_count)!=0){
+        //detach case
+        vtss_task_map_foreach(vtss_target_complete_item, NULL);
+#ifndef VTSS_USE_NMI
+        on_each_cpu(vtss_collector_freeze_events, NULL, SMP_CALL_FUNCTION_ARGS);
+#endif
+    }
+
+#ifndef VTSS_USE_NMI
+     {
+        int br_ovl_status = 0;
+        for (i = 0; i < 20; i++){
+            br_ovl_status=0;
+            on_each_cpu(vtss_collector_check_status, &br_ovl_status, SMP_CALL_FUNCTION_ARGS);
+            if (br_ovl_status == 0) break;
+        }
+#if (LINUX_VERSION_CODE == KERNEL_VERSION(2,6,32))
+        if (br_ovl_status){
+            ERROR("Kernel panic may happen in several minutes if your OS is RHEL 6.5. Please, upgrade you system till RHEL 6.7");
+        }
+#endif
+    }
+#endif
+
+    while (atomic_read(&vtss_events_enabling) != 0) i++;
+    DEBUG_COLLECTOR("No enabled events i = %d", i);
+    
+    //workaround on the problem when pmi enabling while the collection is stopping, but some threads is still collecting data
+    //on_each_cpu(vtss_collector_pmi_disable_on_cpu, NULL, SMP_CALL_FUNCTION_ARGS);
+ 
+    vtss_probe_fini();
+    vtss_cpuevents_fini_pmu();
+    vtss_pebs_fini();
+    vtss_callcount_fini();
+    vtss_lbr_fini();
+    vtss_dsa_fini();
+#ifndef VTSS_USE_NMI
+    vtss_apic_pmi_fini();
+#endif
+    /* NOTE: !!! vtss_transport_fini() should be after vtss_task_map_fini() !!! */
+    vtss_task_map_fini();
+    atomic_set(&vtss_transport_state, 2);
+    while (atomic_read(&vtss_transport_busy) != 0) i++;
+    DEBUG_COLLECTOR("No transport usage i = %d", i);
+    if (vtss_queue_work(-1, vtss_transport_fini_work, NULL, 0)){
+        vtss_transport_fini();
+        atomic_set(&vtss_transport_state, 0);
+        vtss_procfs_ctrl_wake_up(NULL, 0);
+    } else {
+       set_tsk_need_resched(current);
+    }
+    vtss_session_uid = 0;
+    vtss_session_gid = 0;
+    vtss_time_limit  = 0ULL; /* set default value */
+#if 0
+    on_each_cpu(errata_fix, NULL, SMP_CALL_FUNCTION_ARGS);
+#endif
+    DEBUG_COLLECTOR("repare nmi watchdog");
+    vtss_nmi_watchdog_enable(0);
+    atomic_set(&vtss_start_paused, 0);
+    atomic_set(&vtss_collector_state, VTSS_COLLECTOR_STOPPED);
+    vtss_target_clear_temp_list();
+#if (!defined(VTSS_USE_UEC)) && (LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0))
+    if (vtss_need_switch_off_tracing){
+        tracing_off();
+        vtss_need_switch_off_tracing = 0;
+    }
+#endif
+    INFO("vtss++ collection stopped");
+    VTSS_PROFILE_PRINT(printk);
+    return 0;
+}
+
+void vtss_collection_cfg_init(void)
+{
+    int i = 0;
+    memset(&reqcfg, 0, sizeof(process_cfg_t));
+    for (i = 0; i < vtss_stk_last; i++){
+        reqcfg.stk_sz[i] = (unsigned long)-1;
+        reqcfg.stk_pg_sz[i] = 0;
+    }
+}
+
+static int vtss_verify_settings(void)
+{
+    if (reqcfg.cpuevent_count_v1==0 && (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_BRANCH))
+        return -1;
+    return 0;
+}
+
+int vtss_cmd_start(void)
+{
+    int rc = 0;
+    unsigned long flags;
+    int old_state = atomic_cmpxchg(&vtss_collector_state, VTSS_COLLECTOR_STOPPED, VTSS_COLLECTOR_INITING);
+    if (old_state != VTSS_COLLECTOR_STOPPED) {
+        TRACE("Already running");
+        return VTSS_ERR_START_IN_RUN;
+    }
+
+    if (vtss_verify_settings())
+    {
+        TRACE("Incoming settings is incorrect\n");
+        return VTSS_ERR_BADARG;
+    }
+    //workaround on the problem when pmi enabling while the collection is stopping, but some threads is still collecting data
+    on_each_cpu(vtss_collector_pmi_disable_on_cpu, NULL, SMP_CALL_FUNCTION_ARGS);
+    vtss_nmi_watchdog_disable(0);
+    wait_transport_fini();
+#ifdef VTSS_CONFIG_INTERNAL_MEMORY_POOL
+    vtss_memory_pool_clear();
+#endif
+
+#ifdef VTSS_DEBUG_PROFILE
+    vtss_profile_cnt_stk  = 0;
+    vtss_profile_clk_stk  = 0;
+    vtss_profile_cnt_ctx  = 0;
+    vtss_profile_clk_ctx  = 0;
+    vtss_profile_cnt_pmi  = 0;
+    vtss_profile_clk_pmi  = 0;
+    vtss_profile_cnt_pmu  = 0;
+    vtss_profile_clk_pmu  = 0;
+    vtss_profile_cnt_sys  = 0;
+    vtss_profile_clk_sys  = 0;
+    vtss_profile_cnt_bts  = 0;
+    vtss_profile_clk_bts  = 0;
+    vtss_profile_cnt_vma  = 0;
+    vtss_profile_clk_vma  = 0;
+    vtss_profile_cnt_pgp  = 0;
+    vtss_profile_clk_pgp  = 0;
+    vtss_profile_cnt_cpy  = 0;
+    vtss_profile_clk_cpy  = 0;
+    vtss_profile_cnt_vld  = 0;
+    vtss_profile_clk_vld  = 0;
+    vtss_profile_cnt_unw  = 0;
+    vtss_profile_clk_unw  = 0;
+#endif
+#if (!defined(VTSS_USE_UEC)) && (LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0))
+   // We need switch on tracing as for 2.x.x version of kernel it's necessary for ring_buffer functionality
+   // that is used in transport.c
+   DEBUG_COLLECTOR("in define");
+   vtss_need_switch_off_tracing = 0;
+   if (!tracing_is_on()){
+       DEBUG_COLLECTOR("in tracing off");
+       vtss_need_switch_off_tracing = 1;
+       tracing_on();
+   }
+   if (!tracing_is_on()){
+       ERROR("tracing is off, please, build VTSS in \"uec\" mode");
+       return VTSS_ERR_RING_BUFFER_DENIED;
+   }
+#endif
+    INFO("Starting vtss++ collection");
+    INFO("HARDCFG: family: 0x%02x, model: 0x%02x", hardcfg.family, hardcfg.model);
+    INFO("SYSCFG: kernel: %d.%d.%d", (LINUX_VERSION_CODE>>16) & 0xff, (LINUX_VERSION_CODE>>8) & 0xff, (LINUX_VERSION_CODE) & 0xff);
+
+    atomic_set(&vtss_target_count, 0);
+    atomic_set(&vtss_mmap_reg_callcnt, 1);
+    cpumask_copy(&vtss_collector_cpumask, vtss_procfs_cpumask());
+
+    vtss_spin_lock_irqsave(&vtss_target_temp_list_lock, flags);
+    INIT_LIST_HEAD(&vtss_target_temp_list);
+    vtss_spin_unlock_irqrestore(&vtss_target_temp_list_lock, flags);
+
+#if defined CONFIG_UIDGID_STRICT_TYPE_CHECKS || (LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0))
+    {
+    kuid_t uid;
+    kgid_t gid;
+    current_uid_gid(&uid, &gid);
+    vtss_session_uid = uid.val;
+    vtss_session_gid = gid.val;
+    }
+#else
+    current_uid_gid(&vtss_session_uid, &vtss_session_gid);
+#endif
+    vtss_procfs_ctrl_flush();
+    rc |= vtss_transport_init(is_aux_transport_ring_buffer());
+    atomic_set(&vtss_transport_state, 1);
+    rc |= vtss_task_map_init();
+#ifdef VTSS_CONFIG_KPTI
+    rc |= vtss_cea_init();
+#elif defined(VTSS_CONFIG_KAISER)
+    rc |= vtss_kaiser_init();
+#endif
+    rc |= vtss_dsa_init();
+    rc |= vtss_lbr_init();
+    rc |= vtss_callcount_init();
+    rc |= vtss_pebs_init();
+    rc |= vtss_cpuevents_init_pmu(vtss_procfs_defsav());
+    rc |= vtss_probe_init();
+#ifndef VTSS_USE_NMI
+    vtss_apic_pmi_init();
+#endif
+    if (!rc) {
+        atomic_set(&vtss_collector_state, VTSS_COLLECTOR_RUNNING);
+        TRACE("state: %s => RUNNING", state_str[old_state]);
+        if (atomic_read(&vtss_start_paused)) {
+            atomic_set(&vtss_start_paused, 0);
+            vtss_cmd_pause();
+        }
+    } else {
+        ERROR("Collection was not started because of initialization error.");
+        vtss_collection_fini();
+    }
+    return rc;
+}
+
+int vtss_cmd_stop(void)
+{
+    int old_state = atomic_cmpxchg(&vtss_collector_state, VTSS_COLLECTOR_RUNNING, VTSS_COLLECTOR_UNINITING);
+
+    if (old_state == VTSS_COLLECTOR_STOPPED) {
+        DEBUG_COLLECTOR("Already stopped");
+        return 0;
+    }
+    if (old_state == VTSS_COLLECTOR_INITING) {
+        DEBUG_COLLECTOR("STOP in INITING state");
+        return 0;
+    }
+    if (old_state == VTSS_COLLECTOR_UNINITING) {
+        DEBUG_COLLECTOR("STOP in UNINITING state");
+        return 0;
+    }
+    if (old_state == VTSS_COLLECTOR_PAUSED) {
+        old_state = atomic_cmpxchg(&vtss_collector_state, VTSS_COLLECTOR_PAUSED, VTSS_COLLECTOR_UNINITING);
+    }
+    DEBUG_COLLECTOR("state: %s => STOPPING", state_str[old_state]);
+    return vtss_collection_fini();
+}
+
+int vtss_cmd_stop_async(void)
+{
+    int rc = 0;
+    if (atomic_read(&vtss_collector_state) != VTSS_COLLECTOR_STOPPED){
+        rc = vtss_queue_work(-1, vtss_cmd_stop_work, NULL, 0);
+        DEBUG_COLLECTOR("Async STOP (%d)", rc);
+    }
+    return rc;
+}
+
+int vtss_cmd_stop_ring_buffer(void)
+{
+    int rc = 0;
+    vtss_transport_stop_ring_bufer();
+    return rc;
+}
+
+int vtss_cmd_pause(void)
+{
+    int rc = -EINVAL;
+    int cpu;
+    int old_state = atomic_cmpxchg(&vtss_collector_state, VTSS_COLLECTOR_RUNNING, VTSS_COLLECTOR_PAUSED);
+
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+
+    if (old_state == VTSS_COLLECTOR_RUNNING) {
+        if (!vtss_record_probe_all(cpu, 0x66 /* tpss_pi___itt_pause from TPSS ini-file */, SAFE)) {
+            rc = 0;
+        } else {
+            TRACE("vtss_record_probe_all() FAIL");
+        }
+    } else if (old_state == VTSS_COLLECTOR_PAUSED) {
+        TRACE("Already paused");
+        rc = 0;
+    } else if (old_state == VTSS_COLLECTOR_STOPPED) {
+        atomic_inc(&vtss_start_paused);
+        TRACE("It's STOPPED. Start paused = %d", atomic_read(&vtss_start_paused));
+        rc = 0;
+    } else {
+        /* Pause can be done in RUNNING state only */
+        TRACE("PAUSE in wrong state %d", old_state);
+    }
+    TRACE("state: %s => PAUSED (%d)", state_str[old_state], rc);
+    return rc;
+}
+
+int vtss_cmd_resume(void)
+{
+    int rc = -EINVAL;
+    int cpu;
+    int old_state = atomic_cmpxchg(&vtss_collector_state, VTSS_COLLECTOR_PAUSED, VTSS_COLLECTOR_RUNNING);
+
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+
+    if (old_state == VTSS_COLLECTOR_PAUSED) {
+        if (!vtss_record_probe_all(cpu, 0x67 /* tpss_pi___itt_resume from TPSS ini-file */, SAFE)) {
+            rc = 0;
+        } else {
+            TRACE("vtss_record_probe_all() FAIL");
+        }
+    } else if (old_state == VTSS_COLLECTOR_RUNNING) {
+        TRACE("Already resumed");
+        rc = 0;
+    } else if (old_state == VTSS_COLLECTOR_STOPPED) {
+        atomic_dec(&vtss_start_paused);
+        TRACE("It's STOPPED. Start paused = %d", atomic_read(&vtss_start_paused));
+        rc = 0;
+    } else {
+        /* Resume can be done in PAUSED state only */
+        TRACE("RESUME in wrong state %d", old_state);
+    }
+    TRACE("state: %s => RUNNING (%d)", state_str[old_state], rc);
+    return rc;
+}
+
+static void vtss_debug_info_target(vtss_task_map_item_t* item, void* args)
+{
+    int i;
+    struct seq_file *s = (struct seq_file*)args;
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+
+    seq_printf(s, "\n[task %d:%d]\nname='%s'\nstate=0x%04x (",
+                tskd->tid, tskd->pid, tskd->filename, tskd->state);
+    for (i = 0; i < sizeof(task_state_str)/sizeof(char*); i++) {
+        if (tskd->state & (1<<i))
+            seq_printf(s, " %s", task_state_str[i]);
+    }
+    seq_printf(s, " )\n");
+}
+
+int vtss_debug_info(struct seq_file *s)
+{
+    int rc = 0;
+
+    seq_printf(s, "[collector]\nstate=%s\ntargets=%d\ncpu_mask=",
+                state_str[atomic_read(&vtss_collector_state)],
+                atomic_read(&vtss_target_count));
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,0,0)
+    seq_cpumask_list(s, &vtss_collector_cpumask);
+#else
+    seq_printf(s, "%*pbl", cpumask_pr_args(&vtss_collector_cpumask));
+#endif
+    seq_putc(s, '\n');
+
+#ifdef VTSS_DEBUG_PROFILE
+    seq_puts(s, "\n[profile]\n");
+    VTSS_PROFILE_PRINT(seq_printf, s,);
+#endif
+    rc |= vtss_transport_debug_info(s);
+    rc |= vtss_task_map_foreach(vtss_debug_info_target, s);
+    return rc;
+}
+
+static void vtss_target_pids_item(vtss_task_map_item_t* item, void* args)
+{
+    struct seq_file *s = (struct seq_file*)args;
+    struct vtss_task_data* tskd = (struct vtss_task_data*)&item->data;
+
+    if (tskd->tid == tskd->pid) /* Show only processes */
+        seq_printf(s, "%d\n", tskd->pid);
+}
+
+int vtss_target_pids(struct seq_file *s)
+{
+    return vtss_task_map_foreach(vtss_target_pids_item, s);
+}
+
+void vtss_fini(void)
+{
+    DEBUG_COLLECTOR("Unloading vtss...");
+    vtss_cmd_stop();
+    wait_transport_fini();
+    vtss_procfs_fini();
+    vtss_user_vm_fini();
+    vtss_cpuevents_fini();
+    vtss_globals_fini();
+#ifdef VTSS_CONFIG_INTERNAL_MEMORY_POOL
+    vtss_memory_pool_fini();
+#endif
+    DEBUG_COLLECTOR("vtss stopped.");
+}
+
+int vtss_init(void)
+{
+    int rc = 0;
+#ifdef VTSS_RECOVERY_LOGIC
+    int cpu;
+    unsigned long flags;
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,32)
+    if (xen_initial_domain()) {
+        ERROR("XEN dom0 is not supported by VTSS++");
+        return -1;
+    }
+#endif
+
+#ifdef VTSS_GET_TASK_STRUCT
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39)
+    if (vtss__put_task_struct == NULL) {
+#ifndef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+        vtss__put_task_struct = (vtss__put_task_struct_t*)kallsyms_lookup_name("__put_task_struct");
+#else  /* VTSS_AUTOCONF_KPROBE_SYMBOL_NAME */
+        if (!register_kprobe(&_kp_dummy)) {
+            vtss__put_task_struct = (vtss__put_task_struct_t*)_kp_dummy.addr;
+            TRACE("__put_task_struct=0x%p", vtss__put_task_struct);
+            unregister_kprobe(&_kp_dummy);
+        }
+#endif /* VTSS_AUTOCONF_KPROBE_SYMBOL_NAME */
+        if (vtss__put_task_struct == NULL) {
+            ERROR("Cannot find '__put_task_struct' symbol");
+            return -1;
+        }
+    }
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39) */
+#endif /* VTSS_GET_TASK_STRUCT */
+
+#ifdef VTSS_RECOVERY_LOGIC
+    vtss_spin_lock_irqsave(&vtss_recovery_lock, flags);
+    for_each_possible_cpu(cpu) {
+        per_cpu(vtss_recovery_tskd, cpu) = NULL;
+    }
+    vtss_spin_unlock_irqrestore(&vtss_recovery_lock, flags);
+#endif
+    cpumask_copy(&vtss_collector_cpumask, cpu_present_mask);
+
+#ifdef VTSS_CONFIG_INTERNAL_MEMORY_POOL
+    rc |= vtss_memory_pool_init();
+#endif
+    rc |= vtss_globals_init();
+    rc |= vtss_cpuevents_init();
+    rc |= vtss_user_vm_init();
+    rc |= vtss_procfs_init();
+    return rc;
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/cpuevents.c b/drivers/misc/intel/sepdk/vtsspp/cpuevents.c
new file mode 100644
index 000000000000..b9f06b7ac7e6
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/cpuevents.c
@@ -0,0 +1,1071 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "cpuevents.h"
+#include "globals.h"
+#include "collector.h"
+#include "apic.h"               /* for CPU_PERF_VECTOR */
+#include "time.h"
+
+#include <linux/linkage.h>      /* for asmlinkage */
+#include <linux/interrupt.h>
+#include <asm/desc.h>           /* for gate_desc  */
+#include <asm/uaccess.h>
+
+/*
+ * PMU macro definitions
+ */
+#define CPU_EVENTS_SUPPORTED 3
+#define CPU_CLKEVT_THRESHOLD 5000LL
+#define CPU_EVTCNT_THRESHOLD 0x80000000LL
+
+#define DEBUGCTL_MSR             0x1d9
+#define MSR_PERF_GLOBAL_OVF_CTRL 0x390
+
+/// P6 MSRs
+#define MSR_PERF_GLOBAL_CTRL 0x38f
+#define IA32_FIXED_CTR_CTRL  0x38d
+
+#define IA32_FIXED_CTR0      0x309 //instruction ret counter
+      //IA32_FIXED_CTR0+1)           core counter
+      //IA32_FIXED_CTR0+2            ref counter
+
+#define IA32_PERFEVTSEL0     0x186
+#define IA32_PMC0            0x0c1
+
+#define IA32_PERF_GLOBAL_STATUS     0x38e
+
+/// SNB power MSRs
+#define VTSS_MSR_PKG_ENERGY_STATUS  0x611
+#define VTSS_MSR_PP0_ENERGY_STATUS  0x639
+#define VTSS_MSR_PP1_ENERGY_STATUS  0x641    /// 06_2a
+#define VTSS_MSR_DRAM_ENERGY_STATUS 0x619    /// 06_2d
+
+/// NHM/SNB C-state residency MSRs
+#define VTSS_MSR_CORE_C3_RESIDENCY 0x3fc
+#define VTSS_MSR_CORE_C6_RESIDENCY 0x3fd
+#define VTSS_MSR_CORE_C7_RESIDENCY 0x3fe ///SNB
+
+/* Knight family, KNC */
+#define KNX_CORE_PMC0                   0x20
+#define KNX_CORE_PMC1                   0x21
+#define KNX_CORE_PERFEVTSEL0            0x28
+#define KNX_CORE_PERFEVTSEL1            0x29
+#define KNC_PERF_SPFLT_CTRL             0x2C         // KNC only
+#define KNX_CORE_PERF_GLOBAL_STATUS     0x2D
+#define KNX_CORE_PERF_GLOBAL_OVF_CTRL   0x2E
+#define KNX_CORE_PERF_GLOBAL_CTRL       0x2F
+
+#define DEBUG_CPUEVT TRACE
+/**
+ * Globals for CPU Monitoring Functionality
+ */
+static int pmu_counter_no    = 0;
+static int pmu_counter_width = 0;
+static unsigned long long pmu_counter_width_mask = 0x000000ffffffffffULL;
+
+static int pmu_fixed_counter_no    = 0;
+static int pmu_fixed_counter_width = 0;
+static unsigned long long pmu_fixed_counter_width_mask = 0x000000ffffffffffULL;
+static atomic_t  vtss_cpuevents_active = ATOMIC_INIT(0);
+
+/// event descriptors
+cpuevent_desc_t cpuevent_desc[CPU_EVENTS_SUPPORTED];
+sysevent_desc_t sysevent_desc[] = {
+    {"Synchronization Context Switches", "Thread being swapped out due to contention on a synchronization object"},
+    {"Preemption Context Switches", "Thread being preempted by the OS scheduler"},
+    {"Wait Time", "Time while the thread is waiting on a synchronization object"},
+    {"Inactive Time", "Time while the thread is preempted and resides in the ready queue"},
+    {"Idle Time", "Time while no other thread was active before activating the current thread"},
+    {"Idle Wakeup", "Thread waking up the system from idleness"},
+    {"C3 Residency", "Time in low power sleep mode with all but the shared cache flushed"},
+    {"C6 Residency", "Time in low power sleep mode with all caches flushed"},
+    {"C7 Residency", "Time in low power sleep mode with all caches flushed and powered off"},
+    {"Energy Core", "Energy (uJoules) consumed by the processor core"},
+    {"Energy GFX", "Energy (uJoules) consumed by the uncore graphics"},
+    {"Energy Pack", "Energy (uJoules) consumed by the processor package"},
+    {"Energy DRAM", "Energy (uJoules) consumed by the memory"},
+    {"Charge SoC", "Charge (Coulombs) consumed by the system"},
+#ifdef VTSS_SYSCALL_TRACE
+    {"Syscalls", "The number of calls to OS functions"},
+    {"Syscalls Time", "Time spent in system calls"},
+#endif
+    {NULL, NULL}
+};
+
+extern void vtss_perfvec_handler(void);
+
+void vtss_cpuevents_enable(void)
+{
+    if (!atomic_read(&vtss_cpuevents_active)) return;
+    vtss_pmi_enable();
+    /* enable counters globally (required for some Core2 & Core i7 systems) */
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+        unsigned long long mask = (((1ULL << pmu_fixed_counter_no) - 1) << 32) | ((1ULL << pmu_counter_no) - 1);
+
+        TRACE("MSR(0x%x)<=0x%llx", MSR_PERF_GLOBAL_CTRL, mask);
+        wrmsrl(MSR_PERF_GLOBAL_CTRL, mask);
+        mask |= 3ULL << 62;
+        TRACE("MSR(0x%x)<=0x%llx", MSR_PERF_GLOBAL_OVF_CTRL, mask);
+        wrmsrl(MSR_PERF_GLOBAL_OVF_CTRL, mask);
+    } else if (hardcfg.family == 0x0b) { // KNX_CORE_FAMILY
+        unsigned long long mask = (1ULL << pmu_counter_no) - 1;
+
+        TRACE("MSR(0x%x)<=0x%llx", KNX_CORE_PERF_GLOBAL_CTRL, mask);
+        wrmsrl(KNX_CORE_PERF_GLOBAL_CTRL, mask);
+        if (hardcfg.model == 0x01) { // KNC Only
+            TRACE("MSR(0x%x)<=0x%llx", KNX_CORE_PERF_GLOBAL_OVF_CTRL, mask);
+            wrmsrl(KNX_CORE_PERF_GLOBAL_OVF_CTRL, mask);
+            mask |= 1ULL << 63;
+            TRACE("MSR(0x%x)<=0x%llx", KNC_PERF_SPFLT_CTRL, mask);
+            wrmsrl(KNC_PERF_SPFLT_CTRL, mask);
+        }
+    }
+}
+
+static cpuevent_t cpuevent; /*< NOTE: Need  for offset calculation in _stop()/_freeze() */
+
+void vtss_cpuevents_stop(void)
+{
+    int offset = (int)((char*)&cpuevent.opaque - (char*)&cpuevent);
+
+    if (cpuevent_desc[0].vft) {
+        cpuevent_desc[0].vft->stop((cpuevent_t*)((char*)&(cpuevent_desc[0].opaque) - offset));
+    }
+}
+
+void vtss_cpuevents_freeze(void)
+{
+    int offset = (int)((char*)&cpuevent.opaque - (char*)&cpuevent);
+
+    if (cpuevent_desc[0].vft) {
+        cpuevent_desc[0].vft->freeze((cpuevent_t*)((char*)&cpuevent_desc[0].opaque - offset));
+    }
+}
+
+#include "cpuevents_p6.c"
+#include "cpuevents_knx.c"
+#include "cpuevents_sys.c"
+
+void vtss_cpuevents_reqcfg_default(int need_clear, int defsav)
+{
+    int i, len0, len1;
+    int mux_cnt = 0;
+    int namespace_size = 0;
+
+    DEBUG_CPUEVT("reqcfg.cpuevent_count_v1 = %d", (int)reqcfg.cpuevent_count_v1);
+    if (need_clear) {
+        memset(&reqcfg, 0, sizeof(process_cfg_t));
+        reqcfg.trace_cfg.trace_flags =  VTSS_CFGTRACE_CTX    | VTSS_CFGTRACE_CPUEV   |
+                                        VTSS_CFGTRACE_SWCFG  | VTSS_CFGTRACE_HWCFG   |
+                                        VTSS_CFGTRACE_SAMPLE | VTSS_CFGTRACE_OSEV    |
+                                        VTSS_CFGTRACE_MODULE | VTSS_CFGTRACE_PROCTHR |
+                                        VTSS_CFGTRACE_STACKS | VTSS_CFGTRACE_TREE;
+        TRACE("trace_flags=0x%0X (%u)", reqcfg.trace_cfg.trace_flags, reqcfg.trace_cfg.trace_flags);
+    }
+    for (i = 0; i < CPU_EVENTS_SUPPORTED; i++) {
+        if (i > 1 && hardcfg.family != 0x06)
+            break;
+
+        len0 = (int)strlen(cpuevent_desc[i].name)+1;
+        len1 = (int)strlen(cpuevent_desc[i].desc)+1;
+
+        if (namespace_size + len0 + len1 >= VTSS_CFG_SPACE_SIZE * 16)
+            break;
+
+        TRACE("Add cpuevent[%02d]: '%s' into mux_grp=%d", reqcfg.cpuevent_count_v1, cpuevent_desc[i].name, mux_cnt);
+        /// copy event name
+        memcpy(&reqcfg.cpuevent_namespace_v1[namespace_size], cpuevent_desc[i].name, len0);
+        /// adjust event record
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].name_off = (int)((size_t)&reqcfg.cpuevent_namespace_v1[namespace_size] - (size_t)&reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1]);
+        /// adjust namespace size
+        namespace_size += len0;
+        /// copy event description
+        memcpy(&reqcfg.cpuevent_namespace_v1[namespace_size], cpuevent_desc[i].desc, len1);
+        /// adjust event record
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].desc_off = (int)((size_t)&reqcfg.cpuevent_namespace_v1[namespace_size] - (size_t)&reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1]);
+        /// adjust namespace size
+        namespace_size += len1;
+
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].name_len = len0;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].desc_len = len1;
+
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].event_id = i;
+        if (defsav) {
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].interval = defsav;
+        } else if (hardcfg.family == 0x06) { // P6
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].interval = 2000000;
+        } else if (hardcfg.family == 0x0b) { // KNX_CORE_FAMILY
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].interval = 20000000;
+        } else {
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].interval = 10000000;
+        }
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].mux_grp  = mux_cnt;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].mux_alg  = VTSS_CFGMUX_SEQ;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].mux_arg  = 1;
+
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].selmsr.idx = cpuevent_desc[i].selmsr;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].selmsr.val = cpuevent_desc[i].selmsk;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].selmsr.msk = 0;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].cntmsr.idx = cpuevent_desc[i].cntmsr;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].cntmsr.val = 0;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].cntmsr.msk = 0;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].extmsr.idx = cpuevent_desc[i].extmsr;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].extmsr.val = cpuevent_desc[i].extmsk;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].extmsr.msk = 0;
+
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].reqtype = VTSS_CFGREQ_CPUEVENT_V1;
+        reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].reqsize = sizeof(cpuevent_cfg_v1_t) + len0 + len1;
+        reqcfg.cpuevent_count_v1++;
+    }
+}
+
+void vtss_sysevents_reqcfg_append(void)
+{
+    int i, j;
+    int mux_grp = 0;
+    int namespace_size = 0;
+
+    const int idle_idx = 2;
+    const int idle_last = 8;
+    const int active_idx = 9;
+    const int active_last = 13;
+    int sys_event_idx = 2;
+
+    DEBUG_CPUEVT("reqcfg.cpuevent_count_v1 = %d", (int)reqcfg.cpuevent_count_v1);
+    /* Find out the count of mux groups and namespace size */
+    for (i = 0; i < reqcfg.cpuevent_count_v1; i++) {
+        mux_grp = (mux_grp < reqcfg.cpuevent_cfg_v1[i].mux_grp) ? reqcfg.cpuevent_cfg_v1[i].mux_grp : mux_grp;
+        namespace_size += reqcfg.cpuevent_cfg_v1[i].name_len + reqcfg.cpuevent_cfg_v1[i].desc_len;
+    }
+    /* insert system event records (w/names) into each mux_grp */
+    for (i = sys_event_idx; i < vtss_sysevent_end && reqcfg.cpuevent_count_v1 < VTSS_CFG_CHAIN_SIZE; i++) {
+        if (sysevent_type[i] == vtss_sysevent_end) {
+            /* skip events that are not supported on this architecture */
+            continue;
+        }
+        if (i >= idle_idx && i <= idle_last && (!( reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_PWRIDLE)))
+        {
+            // idle power not required
+            continue;
+        }
+        if (i >= active_idx && i <= active_last && (!( reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_PWRACT)))
+        {
+            // active power not required
+            continue;
+        }
+        for (j = 0; j <= mux_grp && reqcfg.cpuevent_count_v1 < VTSS_CFG_CHAIN_SIZE; j++) {
+            int len0 = (int)strlen(sysevent_desc[i].name)+1;
+            int len1 = (int)strlen(sysevent_desc[i].desc)+1;
+
+            if (namespace_size + len0 + len1 >= VTSS_CFG_SPACE_SIZE * 16) {
+                i = vtss_sysevent_end;
+                break;
+            }
+
+            TRACE("Add sysevent[%02d]: '%s' into mux_grp=%d of %d", reqcfg.cpuevent_count_v1, sysevent_desc[i].name, j, mux_grp);
+            /// copy event name
+            memcpy(&reqcfg.cpuevent_namespace_v1[namespace_size], sysevent_desc[i].name, len0);
+            /// adjust event record
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].name_off = (int)((size_t)&reqcfg.cpuevent_namespace_v1[namespace_size] - (size_t)&reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1]);
+            /// adjust namespace size
+            namespace_size += len0;
+            /// copy event description
+            memcpy(&reqcfg.cpuevent_namespace_v1[namespace_size], sysevent_desc[i].desc, len1);
+            /// adjust event record
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].desc_off = (int)((size_t)&reqcfg.cpuevent_namespace_v1[namespace_size] - (size_t)&reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1]);
+            /// adjust namespace size
+            namespace_size += len1;
+
+            /// copy event record
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].name_len = len0;
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].desc_len = len1;
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].mux_grp  = j;
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].event_id = i + VTSS_CFG_CHAIN_SIZE;
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].interval = 0;
+
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].reqtype = VTSS_CFGREQ_CPUEVENT_V1;
+            reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].reqsize = sizeof(cpuevent_cfg_v1_t) + len0 + len1;
+            reqcfg.cpuevent_count_v1++;
+        }
+    }
+}
+
+// called from process_init() to form a common event chain from the configuration records
+void vtss_cpuevents_upload(cpuevent_t* cpuevent_chain, cpuevent_cfg_v1_t* cpuevent_cfg, int count)
+{
+    int i = 0;
+    int j = 0;
+    int mux_cnt = 0;
+    int fixed_cnt[3];
+    int fixed_cnt_slave = 0;
+
+    fixed_cnt[0]=fixed_cnt[1]=fixed_cnt[2]=-1;
+    DEBUG_CPUEVT("reqcfg.cpuevent_count_v1 = %d", (int)reqcfg.cpuevent_count_v1);
+
+    for (i = 0; i < count; i++) {
+        cpuevent_chain[i].valid = 1;
+        if(reqcfg.ipt_cfg.mode & vtss_iptmode_full)
+        {
+            cpuevent_cfg[i].interval = 0;
+            cpuevent_cfg[i].cntmsr.val = 0;
+        }
+        if (cpuevent_cfg[i].event_id >= VTSS_CFG_CHAIN_SIZE) {
+            /// fake sysevents
+            cpuevent_chain[i].vft      = &vft_sys;
+            cpuevent_chain[i].interval = sysevent_type[cpuevent_cfg[i].event_id - VTSS_CFG_CHAIN_SIZE];
+            cpuevent_chain[i].modifier = 0;
+        } else {
+            cpuevent_chain[i].vft      = cpuevent_desc[0].vft;
+            cpuevent_chain[i].interval = cpuevent_cfg[i].interval;
+            cpuevent_chain[i].slave_interval = 0;
+
+            /// copy MSRs
+            cpuevent_chain[i].selmsr = cpuevent_cfg[i].selmsr.idx;
+            cpuevent_chain[i].selmsk = cpuevent_cfg[i].selmsr.val;
+            cpuevent_chain[i].cntmsr = cpuevent_cfg[i].cntmsr.idx;
+            cpuevent_chain[i].extmsr = cpuevent_cfg[i].extmsr.idx;
+            cpuevent_chain[i].extmsk = cpuevent_cfg[i].extmsr.val;
+
+            if (cpuevent_cfg[i].name_len) {
+                /* replace BR_INST_RETIRED.NEAR_CALL_R3_PS event */
+                int name_len = cpuevent_cfg[i].name_len > 32 ? 32 : cpuevent_cfg[i].name_len;
+                if (!memcmp(((char*)&cpuevent_cfg[i] + cpuevent_cfg[i].name_off),
+                             "BR_INST_RETIRED.NEAR_CALL_R3_PS", name_len))
+                {
+                    cpuevent_cfg[i].name_len = 11; /* strlen("Call Count") + '\0' */
+                    memcpy(((char*)&cpuevent_cfg[i] + cpuevent_cfg[i].name_off),
+                           "Call Count", cpuevent_cfg[i].name_len);
+                }
+            }
+            /// correct the sampling interval if not setup explicitly
+            if (!cpuevent_chain[i].interval && cpuevent_cfg[i].cntmsr.val) {
+                if ((cpuevent_chain[i].interval = -(int)(cpuevent_cfg[i].cntmsr.val | 0xffffffff00000000ULL)) < CPU_CLKEVT_THRESHOLD) {
+                    cpuevent_chain[i].interval = CPU_CLKEVT_THRESHOLD * 400;
+                }
+                cpuevent_cfg[i].interval = cpuevent_chain[i].interval;
+            }
+            /// set up counter offset for fixed events
+            if (hardcfg.family == 0x06 && cpuevent_cfg[i].selmsr.idx == IA32_FIXED_CTR_CTRL) {
+                if (cpuevent_cfg[i].cntmsr.idx - IA32_FIXED_CTR0 < 3 && cpuevent_cfg[i].cntmsr.idx - IA32_FIXED_CTR0 >= 0)
+                {
+                    fixed_cnt[cpuevent_cfg[i].cntmsr.idx - IA32_FIXED_CTR0]=i;
+                }
+                /// form the modifier to enable correct masking of control MSR in vft->restart()
+                cpuevent_chain[i].modifier = (int)((cpuevent_cfg[i].selmsr.val >>
+                                                   (4 * (cpuevent_cfg[i].cntmsr.idx - IA32_FIXED_CTR0))) << 16);
+                ((event_modifier_t*)&cpuevent_chain[i].modifier)->cnto = cpuevent_cfg[i].cntmsr.idx - IA32_FIXED_CTR0;
+            } else {
+                cpuevent_chain[i].modifier = (int)(cpuevent_cfg[i].selmsr.val & VTSS_EVMOD_ALL);
+            }
+            if (hardcfg.family == 0x0b) { // KNX_CORE_FAMILY
+                /// set up counter events as slaves (that follow leading events)
+                if (cpuevent_cfg[i].cntmsr.idx > KNX_CORE_PMC0) {
+                    cpuevent_chain[i].slave_interval = cpuevent_cfg[i].interval;
+                    cpuevent_chain[i].interval = 0;
+                }
+            }
+        }
+        cpuevent_chain[i].mux_grp = cpuevent_cfg[i].mux_grp;
+        cpuevent_chain[i].mux_alg = cpuevent_cfg[i].mux_alg;
+        cpuevent_chain[i].mux_arg = cpuevent_cfg[i].mux_arg;
+
+        mux_cnt = (mux_cnt < cpuevent_chain[i].mux_grp) ? cpuevent_chain[i].mux_grp : mux_cnt;
+
+        TRACE("Upload event[%02d]: '%s' .modifier=%x .selmsr=%x .cntmsr=%x .selmsk=%x",
+              i, ((char*)&cpuevent_cfg[i] + cpuevent_cfg[i].name_off),
+              cpuevent_chain[i].modifier, cpuevent_chain[i].selmsr,
+              cpuevent_chain[i].cntmsr,   cpuevent_chain[i].selmsk
+        );
+    }
+
+    for (j = 2; j>=0; j--)
+    {
+        if (fixed_cnt[j] == -1) continue;
+        if (fixed_cnt_slave == 0)
+        {
+            fixed_cnt_slave = 1;
+            continue;
+        }
+        /// set up fixed counter events as slaves (that follow leading events)
+        cpuevent_chain[fixed_cnt[j]].slave_interval = cpuevent_cfg[fixed_cnt[j]].interval;
+        cpuevent_chain[fixed_cnt[j]].interval = 0;
+    }
+    if (i) {
+        cpuevent_chain[0].mux_cnt = mux_cnt;
+        if (hardcfg.family == 0x06) { // P6
+            /* force LBR collection to correct sampled IPs */
+            reqcfg.trace_cfg.trace_flags |= VTSS_CFGTRACE_LASTBR;
+        }
+    }
+}
+
+int vtss_cpuevents_get_sampling_interval(void)
+{
+    int i;
+
+    for (i = 0; i < reqcfg.cpuevent_count_v1; i++) {
+        if (hardcfg.family == 0x06 && reqcfg.cpuevent_cfg_v1[i].selmsr.idx == IA32_FIXED_CTR_CTRL) {
+            if((reqcfg.cpuevent_cfg_v1[i].cntmsr.idx - IA32_FIXED_CTR0) == 2) {
+                unsigned int sav = *(unsigned int*)&reqcfg.cpuevent_cfg_v1[i].interval;
+                if (sav > 0 && hardcfg.cpu_freq > 0 && hardcfg.cpu_freq > sav) {
+                    if (hardcfg.cpu_freq/sav < 1000)
+                        return 1000/(hardcfg.cpu_freq/sav);
+                }
+            }
+        }
+    }
+    return 1;
+}
+
+/// TODO: generate correct records for system-wide sampling/counting
+/// called from swap_in(), pmi_handler(), and vtssreq_trigger()
+/// to read event values and form a sample record
+void vtss_cpuevents_sample(cpuevent_t* cpuevent_chain)
+{
+    int i;
+
+    if (unlikely(!cpuevent_chain)){
+        ERROR("CPU event chain is empty!!!");
+        return;
+    }
+
+    /// select between thread-specific and per-processor chains (system-wide)
+    for (i = 0; i < VTSS_CFG_CHAIN_SIZE && cpuevent_chain[i].valid; i++) {
+        TRACE("[%02d]: mux_idx=%d, mux_grp=%d of %d %s", i,
+              cpuevent_chain[i].mux_idx, cpuevent_chain[i].mux_grp, cpuevent_chain[0].mux_cnt,
+              (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx) ? "skip" : ".vft->freeze_read()");
+        if (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx)
+            continue;
+        cpuevent_chain[i].vft->freeze_read((cpuevent_t*)&cpuevent_chain[i]);
+    }
+}
+
+/* this->tmp: positive - update and restart, negative - just restart
+ *  0 - reset counter,
+ *  1 - switch_to,
+ *  2 - preempt,
+ *  3 - sync,
+ * -1 - switch_to no update,
+ * -2 - preempt no update,
+ * -3 - sync no update
+ */
+void vtss_cpuevents_quantum_border(cpuevent_t* cpuevent_chain, int flag)
+{
+    int i;
+    if (unlikely(!cpuevent_chain)){
+        ERROR("CPU event chain is empty!!!");
+        return;
+    }
+#if 0
+    /// compute idle characteristics
+    if (0 /*tidx == pcb_cpu.idle_tidx*/) {
+        long long tmp;
+        if (flag == 1) { /* switch_to */
+            pcb_cpu.idle_c1_residency = vtss_time_cpu();
+
+            if (sysevent_type[vtss_sysevent_idle_c3] != vtss_sysevent_end) {
+                rdmsrl(VTSS_MSR_CORE_C3_RESIDENCY, pcb_cpu.idle_c3_residency);
+                rdmsrl(VTSS_MSR_CORE_C6_RESIDENCY, pcb_cpu.idle_c6_residency);
+            }
+            if (sysevent_type[vtss_sysevent_idle_c7] != vtss_sysevent_end) {
+                rdmsrl(VTSS_MSR_CORE_C7_RESIDENCY, pcb_cpu.idle_c7_residency);
+            }
+        } else if (pcb_cpu.idle_c1_residency) {
+            pcb_cpu.idle_duration = vtss_time_cpu() - pcb_cpu.idle_c1_residency;
+            pcb_cpu.idle_c1_residency = 0;
+
+            if (sysevent_type[vtss_sysevent_idle_c3] != vtss_sysevent_end) {
+                rdmsrl(VTSS_MSR_CORE_C3_RESIDENCY, tmp);
+                pcb_cpu.idle_c3_residency = tmp - pcb_cpu.idle_c3_residency;
+                rdmsrl(VTSS_MSR_CORE_C6_RESIDENCY, tmp);
+                pcb_cpu.idle_c6_residency = tmp - pcb_cpu.idle_c6_residency;
+            }
+            if (sysevent_type[vtss_sysevent_idle_c7] != vtss_sysevent_end) {
+                rdmsrl(VTSS_MSR_CORE_C7_RESIDENCY, tmp);
+                pcb_cpu.idle_c7_residency = tmp - pcb_cpu.idle_c7_residency;
+            }
+        }
+        if (pcb_cpu.idle_duration < 0 || pcb_cpu.idle_c3_residency < 0 || pcb_cpu.idle_c6_residency < 0 || pcb_cpu.idle_c7_residency < 0) {
+            pcb_cpu.idle_duration = pcb_cpu.idle_c3_residency = pcb_cpu.idle_c6_residency = pcb_cpu.idle_c7_residency = 0;
+        }
+    }
+#endif
+    for (i = 0; i < VTSS_CFG_CHAIN_SIZE && cpuevent_chain[i].valid; i++) {
+        TRACE("[%02d]: mux_idx=%d, mux_grp=%d of %d %s flag=%d", i,
+              cpuevent_chain[i].mux_idx, cpuevent_chain[i].mux_grp, cpuevent_chain[i].mux_cnt,
+              (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx) ? "skip" : ".vft->update_restart()", flag);
+        if (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx)
+            continue;
+        cpuevent_chain[i].tmp = flag;
+        cpuevent_chain[i].vft->update_restart((cpuevent_t*)&cpuevent_chain[i]);
+    }
+#if 0
+    /// flush idleness data
+    if (0 /*tidx != pcb_cpu.idle_tidx*/) {
+        pcb_cpu.idle_duration     = 0;
+        pcb_cpu.idle_c1_residency = 0;
+        pcb_cpu.idle_c3_residency = 0;
+        pcb_cpu.idle_c6_residency = 0;
+        pcb_cpu.idle_c7_residency = 0;
+    }
+#endif
+}
+
+// called from swap_in() and pmi_handler()
+// to re-select multiplexion groups and restart counting
+void vtss_cpuevents_restart(cpuevent_t* cpuevent_chain, int flag)
+{
+    int i, j;
+    long long muxchange_time = 0;
+    int muxchange_alt = 0;
+    int mux_idx = 0;
+    int mux_cnt;
+    int mux_alg;
+    int mux_arg;
+    int mux_flag;
+
+    if (!atomic_read(&vtss_cpuevents_active)) return;
+    vtss_cpuevents_enable();
+    for (i = 0; i < VTSS_CFG_CHAIN_SIZE && cpuevent_chain[i].valid; i++) {
+        /// update current MUX group in accordance with MUX algorithm
+        /// and parameter restart counting for the active MUX group
+        if (i == 0) {
+            /// load MUX context
+            muxchange_time = cpuevent_chain[0].muxchange_time;
+            muxchange_alt  = cpuevent_chain[0].muxchange_alt;
+            mux_idx = cpuevent_chain[0].mux_idx;
+            mux_cnt = cpuevent_chain[0].mux_cnt;
+            mux_alg = cpuevent_chain[0].mux_alg;
+            mux_arg = cpuevent_chain[0].mux_arg;
+
+            /// update current MUX index
+            switch (mux_alg) {
+            case VTSS_CFGMUX_NONE:
+                /// no update to MUX index
+                break;
+
+            case VTSS_CFGMUX_TIME:
+                if (!muxchange_time) {
+                    /// setup new time interval
+                    muxchange_time = vtss_time_cpu() + (mux_arg * hardcfg.cpu_freq);
+                } else if (vtss_time_cpu() >= muxchange_time) {
+                    mux_idx = (mux_idx + 1 > mux_cnt) ? 0 : mux_idx + 1;
+                    muxchange_time = 0;
+                }
+                break;
+
+            case VTSS_CFGMUX_MST:
+            case VTSS_CFGMUX_SLV:
+                for (j = 0, mux_flag = 0; j < VTSS_CFG_CHAIN_SIZE && cpuevent_chain[j].valid; j++) {
+                    if (cpuevent_chain[j].mux_grp == mux_idx && cpuevent_chain[j].mux_alg == VTSS_CFGMUX_MST) {
+                        if (cpuevent_chain[j].vft->overflowed((cpuevent_t*)&cpuevent_chain[j])) {
+                            mux_flag = 1;
+                            break;
+                        }
+                    }
+                }
+                if (!mux_flag) {
+                    break;
+                }
+                /// else fall through
+
+            case VTSS_CFGMUX_SEQ:
+                if (!muxchange_alt) {
+                    muxchange_alt = mux_arg;
+                }
+                if (!--muxchange_alt) {
+                    mux_idx = (mux_idx + 1 > mux_cnt) ? 0 : mux_idx + 1;
+                }
+                break;
+
+            default:
+                /// erroneously configured, ignore
+                break;
+            }
+        }
+
+        /// save MUX context
+        cpuevent_chain[i].muxchange_time = muxchange_time;
+        cpuevent_chain[i].muxchange_alt  = muxchange_alt;
+        cpuevent_chain[i].mux_idx        = mux_idx;
+
+        TRACE("[%02d]: mux_idx=%d, mux_grp=%d of %d %s", i,
+              cpuevent_chain[i].mux_idx, cpuevent_chain[i].mux_grp, cpuevent_chain[0].mux_cnt,
+              (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx) ? "skip" : ".vft->restart()");
+        /* restart counting */
+        if (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx)
+            continue;
+        cpuevent_chain[i].vft->restart((cpuevent_t*)&cpuevent_chain[i]);
+    }
+}
+
+static void vtss_cpuevents_save(void *ctx)
+{
+    unsigned long flags;
+#ifndef VTSS_USE_NMI
+    gate_desc *idt_base;
+    struct desc_ptr idt_ptr;
+#endif
+
+    local_irq_save(flags);
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+        rdmsrl(MSR_PERF_GLOBAL_OVF_CTRL, pcb_cpu.saved_msr_ovf);
+        wrmsrl(MSR_PERF_GLOBAL_OVF_CTRL, 0ULL);
+        rdmsrl(MSR_PERF_GLOBAL_CTRL,     pcb_cpu.saved_msr_perf);
+        wrmsrl(MSR_PERF_GLOBAL_CTRL,     0ULL);
+        rdmsrl(DEBUGCTL_MSR,             pcb_cpu.saved_msr_debug);
+        wrmsrl(DEBUGCTL_MSR,             0ULL);
+    } else if (hardcfg.family == 0x0b) { // KNX_CORE_FAMILY
+        rdmsrl(KNX_CORE_PERF_GLOBAL_OVF_CTRL, pcb_cpu.saved_msr_ovf);
+        wrmsrl(KNX_CORE_PERF_GLOBAL_OVF_CTRL, 0ULL);
+        rdmsrl(KNX_CORE_PERF_GLOBAL_CTRL,     pcb_cpu.saved_msr_perf);
+        wrmsrl(KNX_CORE_PERF_GLOBAL_CTRL,     0ULL);
+    }
+#ifdef VTSS_USE_NMI
+    pcb_cpu.saved_apic_lvtpc = apic_read(APIC_LVTPC);
+    apic_write(APIC_LVTPC, APIC_DM_NMI);
+#endif
+#ifndef VTSS_USE_NMI
+    store_idt(&idt_ptr);
+    idt_base = (gate_desc*)idt_ptr.address;
+    pcb_cpu.idt_base = idt_base;
+    memcpy(&pcb_cpu.saved_perfvector, &idt_base[CPU_PERF_VECTOR], sizeof(gate_desc));
+#endif
+    local_irq_restore(flags);
+}
+
+static void vtss_cpuevents_stop_all(void *ctx)
+{
+    unsigned long flags;
+
+    local_irq_save(flags);
+    vtss_pmi_disable();
+    vtss_cpuevents_stop();
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+        wrmsrl(MSR_PERF_GLOBAL_OVF_CTRL, 0ULL);
+        wrmsrl(MSR_PERF_GLOBAL_CTRL,     0ULL);
+        wrmsrl(DEBUGCTL_MSR,             0ULL);
+    } else if (hardcfg.family == 0x0b) { // KNX_CORE_FAMILY
+        wrmsrl(KNX_CORE_PERF_GLOBAL_OVF_CTRL, 0ULL);
+        wrmsrl(KNX_CORE_PERF_GLOBAL_CTRL,     0ULL);
+    }
+    local_irq_restore(flags);
+}
+
+static void vtss_cpuevents_restore(void *ctx)
+{
+    unsigned long flags;
+#ifndef VTSS_USE_NMI
+    gate_desc *idt_base;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0)
+    unsigned long cr0;
+#endif
+#endif
+
+    local_irq_save(flags);
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+        wrmsrl(MSR_PERF_GLOBAL_OVF_CTRL, pcb_cpu.saved_msr_ovf);
+        wrmsrl(MSR_PERF_GLOBAL_CTRL,     pcb_cpu.saved_msr_perf);
+        wrmsrl(DEBUGCTL_MSR,             pcb_cpu.saved_msr_debug);
+    } else if (hardcfg.family == 0x0b) { // KNX_CORE_FAMILY
+        wrmsrl(KNX_CORE_PERF_GLOBAL_OVF_CTRL, pcb_cpu.saved_msr_ovf);
+        wrmsrl(KNX_CORE_PERF_GLOBAL_CTRL,     pcb_cpu.saved_msr_perf);
+    }
+#ifdef VTSS_USE_NMI
+    apic_write(APIC_LVTPC, pcb_cpu.saved_apic_lvtpc);
+#endif
+
+#ifndef VTSS_USE_NMI
+    idt_base = pcb_cpu.idt_base;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0)
+    cr0 = read_cr0();
+    write_cr0(cr0 & ~X86_CR0_WP);
+#endif
+    write_idt_entry(idt_base, CPU_PERF_VECTOR, &pcb_cpu.saved_perfvector);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0)
+    write_cr0(cr0);
+#endif
+#endif
+        
+    local_irq_restore(flags);
+}
+
+#ifndef VTSS_USE_NMI
+static void vtss_cpuevents_setup(void *ctx)
+{
+    unsigned long flags;
+    gate_desc *idt_base, g;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0)
+    unsigned long cr0;
+#endif
+    local_irq_save(flags);
+    idt_base = pcb_cpu.idt_base;
+    pack_gate(&g, GATE_INTERRUPT, (unsigned long)vtss_perfvec_handler, 3, 0, __KERNEL_CS);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0)
+    cr0 = read_cr0();
+    write_cr0(cr0 & ~X86_CR0_WP);
+#endif
+    write_idt_entry(idt_base, CPU_PERF_VECTOR, &g);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0)
+    write_cr0(cr0);
+#endif
+
+    local_irq_restore(flags);
+}
+#endif
+
+#ifdef VTSS_USE_NMI
+#include <asm/nmi.h>
+#include <asm/apic.h>
+
+void vtss_pmi_handler(struct pt_regs *regs);
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,2,0))
+static int vtss_nmi_handler(unsigned int cmd, struct pt_regs *regs)
+{
+    int state = VTSS_COLLECTOR_STATE();
+
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+        wrmsrl(DEBUGCTL_MSR, 0ULL);
+    }
+    if (state >= VTSS_COLLECTOR_RUNNING) {
+        vtss_pmi_handler(regs);
+        return NMI_HANDLED;
+    }
+    else if (state == VTSS_COLLECTOR_UNINITING) {
+        return NMI_HANDLED;
+    }
+    return NMI_DONE;
+}
+#else
+#include <linux/kdebug.h>
+static int vtss_nmi_handler(struct notifier_block *self, unsigned long val, void *data)
+{
+    struct die_args *args = (struct die_args *)data;
+    int state = VTSS_COLLECTOR_STATE();
+
+    if (args) {
+        switch (val) {
+            case DIE_NMI:
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38))
+            case DIE_NMI_IPI:
+#endif
+                if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+                    wrmsrl(DEBUGCTL_MSR, 0ULL);
+                }
+                if (state >= VTSS_COLLECTOR_RUNNING) {
+                    vtss_pmi_handler(args->regs);
+                    return NOTIFY_STOP;
+                }
+                else if (state == VTSS_COLLECTOR_UNINITING) {
+                    return NOTIFY_STOP;
+                }
+        }
+    }
+    return NOTIFY_DONE;
+}
+
+static struct notifier_block vtss_notifier = {
+    .notifier_call = vtss_nmi_handler,
+    .next = NULL,
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38))
+    .priority = 2
+#else
+    .priority = NMI_LOCAL_LOW_PRIOR,
+#endif
+};
+#endif
+#endif
+
+int vtss_cpuevents_init_pmu(int defsav)
+{
+    INFO("PMU: counters: %d", (int)reqcfg.cpuevent_count_v1);
+    atomic_set(&vtss_cpuevents_active, 1);
+    if ((reqcfg.cpuevent_count_v1 == 0 && !(reqcfg.trace_cfg.trace_flags & (VTSS_CFGTRACE_CTX|VTSS_CFGTRACE_PWRACT|VTSS_CFGTRACE_PWRIDLE)))||
+        (reqcfg.cpuevent_count_v1 == 0 && hardcfg.family == 0x0b))
+    {
+        /* There is no configuration was get from runtool, so init defaults */
+        DEBUG_CPUEVT("There is no configuration was get from runtool, so init defaults");
+        vtss_cpuevents_reqcfg_default(1, defsav);
+        vtss_sysevents_reqcfg_append();
+    }
+#ifdef VTSS_USE_NMI
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,2,0))
+    register_nmi_handler(NMI_LOCAL, vtss_nmi_handler, 0, "vtss_pmi");
+#else
+    register_die_notifier(&vtss_notifier);
+#endif
+    INFO("PMI: registered NMI handler");
+#endif
+    on_each_cpu(vtss_cpuevents_save,  NULL, SMP_CALL_FUNCTION_ARGS);
+#ifndef VTSS_USE_NMI
+    on_each_cpu(vtss_cpuevents_setup, NULL, SMP_CALL_FUNCTION_ARGS);
+    INFO("PMI: installed IDT vector 0x%x", CPU_PERF_VECTOR);
+#endif
+    return 0;
+}
+
+void vtss_cpuevents_fini_pmu(void)
+{
+    atomic_set(&vtss_cpuevents_active, 0);
+    on_each_cpu(vtss_cpuevents_stop_all, NULL, SMP_CALL_FUNCTION_ARGS);
+    on_each_cpu(vtss_cpuevents_restore,  NULL, SMP_CALL_FUNCTION_ARGS);
+#ifdef VTSS_USE_NMI
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,2,0))
+    unregister_nmi_handler(NMI_LOCAL, "vtss_pmi");
+#else
+    unregister_die_notifier(&vtss_notifier);
+#endif
+#endif
+}
+
+union cpuid_0AH_eax
+{
+    struct
+    {
+        unsigned int version_id:8;
+        unsigned int counters_no:8;
+        unsigned int counters_width:8;
+        unsigned int reserved:8;
+    } split;
+    unsigned int full;
+};
+
+union cpuid_0AH_edx
+{
+    struct
+    {
+        unsigned int fixed_counters_no:5;
+        unsigned int fixed_counters_width:8;
+        unsigned int reserved:29;
+    } split;
+    unsigned int full;
+};
+
+int vtss_cpuevents_init(void)
+{
+    int counter_offset = 0; /* .modifier = VTSS_EVMOD_ALL | counter_offset; */
+
+    if (hardcfg.family == 0x0b) {   // KNX_CORE_FAMILY
+        pmu_counter_no    = 2;
+        pmu_counter_width = 40;
+        pmu_counter_width_mask = (((unsigned long long)1) << pmu_counter_width) - 1;
+
+        pmu_fixed_counter_no    = 0;
+        pmu_fixed_counter_width = 0;
+        pmu_fixed_counter_width_mask = 0;
+    } else if (hardcfg.family == 0x0f || hardcfg.family == 0x06) { // P4 or P6
+        union cpuid_0AH_eax eax;
+        union cpuid_0AH_edx edx;
+        unsigned int ebx, ecx;
+
+        cpuid(0x0a, &eax.full, &ebx, &ecx, &edx.full);
+
+        pmu_counter_no    = eax.split.counters_no;
+        pmu_counter_width = eax.split.counters_width;
+        pmu_counter_width_mask = (1ULL << pmu_counter_width) - 1;
+
+        pmu_fixed_counter_no    = edx.split.fixed_counters_no;
+        pmu_fixed_counter_width = edx.split.fixed_counters_width;
+        pmu_fixed_counter_width_mask = (1ULL << pmu_fixed_counter_width) - 1;
+
+        counter_offset = (pmu_counter_no >= 4) ? VTSS_EVMOD_CNT3 : VTSS_EVMOD_CNT1;
+    }
+    TRACE("PMU: counters=%d, width=%d (0x%llX)", pmu_counter_no, pmu_counter_width, pmu_counter_width_mask);
+    TRACE("PMU:    fixed=%d, width=%d (0x%llX)", pmu_fixed_counter_no, pmu_fixed_counter_width, pmu_fixed_counter_width_mask);
+
+    if (!pmu_counter_no) {
+        ERROR("PMU counters are not detected");
+        hardcfg.family = VTSS_UNKNOWN_ARCH;
+    }
+
+    memset(cpuevent_desc, 0, sizeof(cpuevent_desc));
+    if (hardcfg.family == 0x0b) {   // KNX_CORE_FAMILY
+        TRACE("KNX-cpuevents is used");
+        cpuevent_desc[0].event_id = VTSS_EVID_NONHALTED_CLOCKTICKS;
+        cpuevent_desc[0].vft      = &vft_knx;
+        cpuevent_desc[0].name     = "CPU_CLK_UNHALTED";
+        cpuevent_desc[0].desc     = "CPU_CLK_UNHALTED";
+        cpuevent_desc[0].modifier = VTSS_EVMOD_ALL | counter_offset;
+        cpuevent_desc[0].selmsr   = KNX_CORE_PERFEVTSEL0;
+        cpuevent_desc[0].cntmsr   = KNX_CORE_PMC0;
+        cpuevent_desc[0].selmsk   = 0x53002A;
+
+        cpuevent_desc[1].event_id = VTSS_EVID_INSTRUCTIONS_RETIRED;
+        cpuevent_desc[1].vft      = &vft_knx;
+        cpuevent_desc[1].name     = "INSTRUCTIONS_EXECUTED";
+        cpuevent_desc[1].desc     = "INSTRUCTIONS_EXECUTED";
+        cpuevent_desc[1].modifier = VTSS_EVMOD_ALL | counter_offset;
+        cpuevent_desc[1].selmsr   = KNX_CORE_PERFEVTSEL0+1;
+        cpuevent_desc[1].cntmsr   = KNX_CORE_PMC0+1;
+        cpuevent_desc[1].selmsk   = 0x530016;
+    } else if (hardcfg.family == 0x06) {   // P6
+        TRACE("P6-cpuevents is used");
+        cpuevent_desc[0].event_id = VTSS_EVID_FIXED_INSTRUCTIONS_RETIRED;
+        cpuevent_desc[0].vft      = &vft_p6;
+        cpuevent_desc[0].name     = "INST_RETIRED.ANY";
+        cpuevent_desc[0].desc     = "INST_RETIRED.ANY";
+        cpuevent_desc[0].modifier = VTSS_EVMOD_ALL | counter_offset;
+        cpuevent_desc[0].selmsr   = IA32_FIXED_CTR_CTRL;
+        cpuevent_desc[0].cntmsr   = IA32_FIXED_CTR0;
+        cpuevent_desc[0].selmsk   = 0x0000000b;
+
+        cpuevent_desc[1].event_id = VTSS_EVID_FIXED_NONHALTED_CLOCKTICKS;
+        cpuevent_desc[1].vft      = &vft_p6;
+        cpuevent_desc[1].name     = "CPU_CLK_UNHALTED.THREAD";
+        cpuevent_desc[1].desc     = "CPU_CLK_UNHALTED.THREAD";
+        cpuevent_desc[1].modifier = VTSS_EVMOD_ALL | counter_offset;
+        cpuevent_desc[1].selmsr   = IA32_FIXED_CTR_CTRL;
+        cpuevent_desc[1].cntmsr   = IA32_FIXED_CTR0+1;
+        cpuevent_desc[1].selmsk   = 0x000000b0;
+
+        cpuevent_desc[2].event_id = VTSS_EVID_FIXED_NONHALTED_REFTICKS;
+        cpuevent_desc[2].vft      = &vft_p6;
+        cpuevent_desc[2].name     = "CPU_CLK_UNHALTED.REF";
+        cpuevent_desc[2].desc     = "CPU_CLK_UNHALTED.REF";
+        cpuevent_desc[2].modifier = VTSS_EVMOD_ALL | counter_offset;
+        cpuevent_desc[2].selmsr   = IA32_FIXED_CTR_CTRL;
+        cpuevent_desc[2].cntmsr   = IA32_FIXED_CTR0+2;
+        cpuevent_desc[2].selmsk   = 0x00000b00;
+
+        /* CPU BUG: broken fixed counters on some Meroms and Penryns */
+        if (hardcfg.model == 0x0f && hardcfg.stepping < 0x0b) {
+            ERROR("All fixed counters are broken");
+        } else if (hardcfg.model == 0x17) {
+            ERROR("CPU_CLK_UNHALTED.REF fixed counter is broken");
+        }
+
+        { /* check for read-only counter mode */
+            unsigned long long tmp, tmp1;
+
+            wrmsrl(IA32_PERFEVTSEL0, 0ULL);
+            wrmsrl(IA32_PMC0, 0ULL);
+            rdmsrl(IA32_PMC0, tmp);
+            tmp |= 0x7f00ULL;
+            wrmsrl(IA32_PMC0, tmp);
+            rdmsrl(IA32_PMC0, tmp1);
+            if (tmp1 != tmp) {
+                /* read-only counters, change the event VFT */
+                vft_p6.restart     = vf_p6_restart_ro;
+                vft_p6.freeze_read = vf_p6_freeze_read_ro;
+            }
+            wrmsrl(IA32_PMC0, 0ULL);
+        }
+    }
+    /// TODO: validate SNB and MFLD energy meters:
+    /// sysevent_type[vtss_sysevent_energy_xxx] = vtss_sysevent_end if not present
+    DEBUG_CPUEVT("family=%x, model=%x", hardcfg.family,hardcfg.model);
+    if (hardcfg.family == 0x06) {
+        if(hardcfg.model == VTSS_CPU_SNB ||
+           hardcfg.model == VTSS_CPU_IVB ||
+           hardcfg.model == VTSS_CPU_HSW ||
+           hardcfg.model == VTSS_CPU_BDW ||
+           hardcfg.model == VTSS_CPU_BDW_GT3 ||
+           hardcfg.model == VTSS_CPU_HSW_ULT ||
+           hardcfg.model == VTSS_CPU_HSW_GT3)
+        {
+            sysevent_type[vtss_sysevent_energy_dram] = vtss_sysevent_end;
+        } else if (hardcfg.model == VTSS_CPU_HSW_X) {
+            sysevent_type[vtss_sysevent_energy_core] = vtss_sysevent_end;
+            sysevent_type[vtss_sysevent_energy_gfx]  = vtss_sysevent_end;
+        } else if (hardcfg.model == VTSS_CPU_SNB_X  || hardcfg.model == VTSS_CPU_IVB_X) {
+            sysevent_type[vtss_sysevent_energy_gfx]  = vtss_sysevent_end;
+        } else {
+            sysevent_type[vtss_sysevent_energy_core] = vtss_sysevent_end;
+            sysevent_type[vtss_sysevent_energy_gfx]  = vtss_sysevent_end;
+            sysevent_type[vtss_sysevent_energy_pack] = vtss_sysevent_end;
+            sysevent_type[vtss_sysevent_energy_dram] = vtss_sysevent_end;
+        }
+    } else {
+        sysevent_type[vtss_sysevent_energy_core] = vtss_sysevent_end;
+        sysevent_type[vtss_sysevent_energy_gfx]  = vtss_sysevent_end;
+        sysevent_type[vtss_sysevent_energy_pack] = vtss_sysevent_end;
+        sysevent_type[vtss_sysevent_energy_dram] = vtss_sysevent_end;
+    }
+    ///if (hardcfg.family != 0x06 || hardcfg.model != 0x27)
+    sysevent_type[vtss_sysevent_energy_soc] = vtss_sysevent_end;
+#if 0
+    /// disable C-state residency events if not supported
+    if (hardcfg.family == 0x06) {
+        switch (hardcfg.model) {
+        /// NHM/WMR
+        case 0x1a:
+        case 0x1e:
+        case 0x1f:
+        case 0x2e:
+        case 0x25:
+        case 0x2c:
+            sysevent_type[vtss_sysevent_idle_c7] = vtss_sysevent_end;
+            break;
+        /// SNB/IVB
+        case 0x2a:
+        case 0x2d:
+        case 0x3a:
+            break;
+        /// Medfield/CedarTrail/CloverTrail
+        case 0x27:
+        /// TODO: make sure Cx MSRs are supported
+        ///case 0x35:
+        ///case 0x36:
+            break;
+        default:
+            sysevent_type[vtss_sysevent_idle_c3] = vtss_sysevent_end;
+            sysevent_type[vtss_sysevent_idle_c6] = vtss_sysevent_end;
+            sysevent_type[vtss_sysevent_idle_c7] = vtss_sysevent_end;
+        }
+    } else {
+        sysevent_type[vtss_sysevent_idle_c3] = vtss_sysevent_end;
+        sysevent_type[vtss_sysevent_idle_c6] = vtss_sysevent_end;
+        sysevent_type[vtss_sysevent_idle_c7] = vtss_sysevent_end;
+    }
+#else
+    /* TODO: Not implemeted. Turn off for Linux now all idle_* */
+    sysevent_type[vtss_sysevent_idle_time]   = vtss_sysevent_end;
+    sysevent_type[vtss_sysevent_idle_wakeup] = vtss_sysevent_end;
+    sysevent_type[vtss_sysevent_idle_c3]     = vtss_sysevent_end;
+    sysevent_type[vtss_sysevent_idle_c6]     = vtss_sysevent_end;
+    sysevent_type[vtss_sysevent_idle_c7]     = vtss_sysevent_end;
+#endif
+    return 0;
+}
+
+void vtss_cpuevents_fini(void)
+{
+    pmu_counter_no         = 0;
+    pmu_counter_width      = 0;
+    pmu_counter_width_mask = 0x000000ffffffffffULL;
+
+    pmu_fixed_counter_no         = 0;
+    pmu_fixed_counter_width      = 0;
+    pmu_fixed_counter_width_mask = 0x000000ffffffffffULL;
+
+    hardcfg.family = VTSS_UNKNOWN_ARCH;
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/cpuevents_knx.c b/drivers/misc/intel/sepdk/vtsspp/cpuevents_knx.c
new file mode 100644
index 000000000000..67e6ae1ba8ed
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/cpuevents_knx.c
@@ -0,0 +1,170 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+
+/// KNX event control virtual functions
+static void vf_knx_start(cpuevent_t* this)
+{
+}
+
+static void vf_knx_stop(cpuevent_t* this)
+{
+    int i;
+
+    for (i = 0; i < pmu_counter_no; i++) {
+        wrmsrl(KNX_CORE_PERFEVTSEL0 + i, 0ULL);
+        wrmsrl(KNX_CORE_PMC0        + i, 0ULL);
+    }
+}
+
+static void vf_knx_read(cpuevent_t* this)
+{
+}
+
+static void vf_knx_freeze(cpuevent_t* this)
+{
+    int i;
+
+    for (i = 0; i < pmu_counter_no; i++) {
+        wrmsrl(KNX_CORE_PERFEVTSEL0 + i, 0ULL);
+    }
+}
+
+/// with continuous counting mode
+static void vf_knx_restart(cpuevent_t* this)
+{
+    long long interval = this->frozen_count;
+
+    if (!this->interval)    /// no sampling
+    {
+        /// wrap the counters around
+        this->frozen_count &= CPU_EVTCNT_THRESHOLD - 1;
+        interval = -(interval & (CPU_EVTCNT_THRESHOLD - 1));
+    } else {
+        if (interval >= 0)      /// overflowed
+        {
+            /// use the programmed interval
+            this->frozen_count = interval = this->interval;
+        } else                  /// underflowed
+        {
+            /// use the residual count
+            this->frozen_count = interval = -interval;
+        }
+    }
+    /// ensure we do not count backwards
+    if (interval > this->interval) {
+        interval = this->interval;
+    }
+    /// set up the counter
+    TRACE("MSR(0x%x)<=0x%llx", this->cntmsr, -interval & pmu_counter_width_mask);
+    wrmsrl(this->cntmsr, -interval & pmu_counter_width_mask);
+    /// set up the control register 
+    /// TODO: use other modifier fields
+    TRACE("MSR(0x%x)<=0x%x", this->selmsr, (this->selmsk & ~VTSS_EVMOD_ALL) | (this->modifier & VTSS_EVMOD_ALL));
+    wrmsrl(this->selmsr, (this->selmsk & ~VTSS_EVMOD_ALL) | (this->modifier & VTSS_EVMOD_ALL));
+    if (this->extmsr) {
+        TRACE("MSR(0x%x)<=0x%llx", this->extmsr, this->extmsk);
+        wrmsrl(this->extmsr, this->extmsk);
+    }
+}
+
+static void vf_knx_freeze_read(cpuevent_t* this)
+{
+    long long interval = (this->frozen_count > 0) ? this->frozen_count : (long long)this->interval;
+    int shift = 64 - pmu_counter_width;
+
+    TRACE("MSR(0x%x)<=0x%llx", this->selmsr, 0LL);
+    wrmsrl(this->selmsr, 0ULL);
+    rdmsrl(this->cntmsr, this->frozen_count);
+    TRACE("MSR(0x%x)=>0x%llx, interval=0x%llx", this->cntmsr, this->frozen_count, (long long)this->interval);
+
+    /// convert the count to 64 bits
+    this->frozen_count = (this->frozen_count << shift) >> shift;
+
+    /// ensure we do not count backwards
+    if (this->frozen_count < -interval) {
+        this->frozen_count = -this->interval;
+        interval = (long long)this->interval;
+    }
+    if (!this->interval)    /// no sampling
+    {
+        if (this->frozen_count < interval)  /// HW and VM sanity check
+        {
+            interval = this->frozen_count;
+        }
+        this->sampled_count += this->frozen_count - interval;
+    } else {
+        /// update the accrued count by adding the signed values of current count and sampling interval
+        this->sampled_count += interval + this->frozen_count;
+    }
+    /// separately preserve counts of overflowed counters, and
+    /// uncomment to always save fixed counters (to show performance impact of synchronization on call tree)
+    if (this->frozen_count >= 0) {
+        this->count = this->sampled_count;
+    }
+}
+
+static int vf_knx_overflowed(cpuevent_t* this)
+{
+    if (this->frozen_count >= 0) {
+        return 1;               /// always signal overflow for no sampling mode and in case of real overflow
+    }
+    return 0;
+}
+
+static long long vf_knx_convert(cpuevent_t* this)
+{
+    return 0LL;
+}
+
+static void vf_knx_overflow_update(cpuevent_t* this)
+{
+}
+
+static void vf_knx_update_restart(cpuevent_t* this)
+{
+}
+
+static int vf_knx_select_muxgroup(cpuevent_t* this)
+{
+    return -1;
+}
+
+/// KNX virtual function tables
+static cpuevent_i vft_knx = {
+    vf_knx_start,
+    vf_knx_stop,
+    vf_knx_read,
+    vf_knx_freeze,
+    vf_knx_restart,
+    vf_knx_freeze_read,
+    vf_knx_overflowed,
+    vf_knx_convert,
+    vf_knx_overflow_update,
+    vf_knx_update_restart,
+    vf_knx_select_muxgroup
+};
diff --git a/drivers/misc/intel/sepdk/vtsspp/cpuevents_p6.c b/drivers/misc/intel/sepdk/vtsspp/cpuevents_p6.c
new file mode 100644
index 000000000000..e04de594b1e9
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/cpuevents_p6.c
@@ -0,0 +1,255 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+
+/// P6 event control virtual functions
+static void vf_p6_start(cpuevent_t* this)
+{
+}
+
+static void vf_p6_stop(cpuevent_t* this)
+{
+    int i;
+
+    for (i = 0; i < pmu_counter_no; i++) {
+        wrmsrl(IA32_PERFEVTSEL0 + i, 0ULL);
+        wrmsrl(IA32_PMC0        + i, 0ULL);
+    }
+    if (hardcfg.model >= 0x0f)
+        wrmsrl(IA32_FIXED_CTR_CTRL, 0ULL);
+}
+
+static void vf_p6_read(cpuevent_t* this)
+{
+}
+
+static void vf_p6_freeze(cpuevent_t* this)
+{
+    int i;
+
+    for (i = 0; i < pmu_counter_no; i++) {
+        wrmsrl(IA32_PERFEVTSEL0 + i, 0ULL);
+    }
+    if (hardcfg.model >= 0x0f)
+        wrmsrl(IA32_FIXED_CTR_CTRL, 0ULL);
+}
+
+/// with continuous counting mode
+static void vf_p6_restart(cpuevent_t* this)
+{
+    long long interval = this->frozen_count;
+    unsigned long long tmp;
+    unsigned long long msk;
+
+    if (!this->interval && !this->slave_interval)    /// no sampling
+    {
+        /// wrap the counters around
+        this->frozen_count &= CPU_EVTCNT_THRESHOLD - 1;
+        interval = -(interval & (CPU_EVTCNT_THRESHOLD - 1));
+    } else {
+        if (interval >= this->slave_interval)      /// overflowed
+        {
+            /// use the programmed interval
+            this->frozen_count = interval = this->interval;
+        } else                  /// underflowed
+        {
+            /// use the residual count
+            this->frozen_count = interval = -interval;
+
+            if (this->slave_interval)
+            {
+                this->frozen_count = -interval;
+            }
+        }
+    }
+    /// ensure we do not count backwards
+    if (interval > this->interval) {
+        interval = this->interval;
+    }
+    /// set up counters
+    if (this->selmsr == IA32_FIXED_CTR_CTRL) {
+        /// set up the counter
+        wrmsrl(this->cntmsr, -interval & pmu_fixed_counter_width_mask);
+
+        /// set up the control register 
+        rdmsrl(IA32_FIXED_CTR_CTRL, tmp);
+
+        msk = (((this->modifier & VTSS_EVMOD_ALL) >> 16) | 8) << (4 * ((event_modifier_t*)&this->modifier)->cnto);
+
+        wrmsrl(IA32_FIXED_CTR_CTRL, tmp | msk);
+    } else {
+        /// set up the counter
+        wrmsrl(this->cntmsr, -interval & pmu_counter_width_mask);
+        /// set up the control register 
+        /// TODO: use other modifier fields
+        wrmsrl(this->selmsr, (this->selmsk & ~VTSS_EVMOD_ALL) | (this->modifier & VTSS_EVMOD_ALL));
+    }
+    if (this->extmsr) {
+        wrmsrl(this->extmsr, this->extmsk);
+    }
+#if 0
+    /// save timestamp to enable computation of event distribution function
+    this->time[this->state_idx][0] = vtss_time_cpu();
+    this->flux[this->state_idx] = 0;
+#endif
+}
+
+static void vf_p6_freeze_read(cpuevent_t* this)
+{
+#if 0
+    int state_idx = this->state_idx;
+#endif
+    long long interval = (this->frozen_count > 0) ? this->frozen_count : (long long)this->interval;
+    int shift = 64 - pmu_counter_width;
+    int fixed_shift = 64 - pmu_fixed_counter_width;
+    //check overflow of all counters
+    //unsigned long long mask = (((1ULL << pmu_fixed_counter_no) - 1) << 32) | ((1ULL << pmu_counter_no) - 1);
+    //check only fixed counters
+    unsigned long long mask = (((1ULL << pmu_fixed_counter_no) - 1) << 32);
+    unsigned long long ovf;
+
+    rdmsrl(IA32_PERF_GLOBAL_STATUS, ovf);
+    ovf &= mask;
+
+    wrmsrl(this->selmsr, 0ULL);
+    rdmsrl(this->cntmsr, this->frozen_count);
+    TRACE("MSR(0x%x)=>0x%llx, interval=0x%llx", this->cntmsr, this->frozen_count, (long long)this->interval);
+
+    /// CPU BUG: Correction for broken fixed counters on some Meroms and Penryns
+    if (hardcfg.family == 0x06) {
+        if (hardcfg.model == 0x0f && hardcfg.stepping < 0x0b) {
+            if (this->selmsr == IA32_FIXED_CTR_CTRL) {
+                this->frozen_count = -interval;
+            }
+        } else if (hardcfg.model == 0x17) {
+            if (this->cntmsr == 0x30b) {
+                this->frozen_count = -interval;
+            }
+        }
+    }
+    /// convert the count to 64 bits
+    if (this->selmsr == IA32_FIXED_CTR_CTRL) {
+        this->frozen_count = (this->frozen_count << fixed_shift) >> fixed_shift;
+    } else {
+        this->frozen_count = (this->frozen_count << shift) >> shift;
+    }
+
+    /// ensure we do not count backwards
+    if (this->frozen_count < -interval) {
+        this->frozen_count = -this->interval;
+        interval = (long long)this->interval;
+    }
+    if (!this->interval)    /// no sampling
+    {
+        if (this->frozen_count < interval)  /// HW and VM sanity check
+        {
+            interval = this->frozen_count;
+        }
+        this->sampled_count += this->frozen_count - interval;
+#if 0
+        /// save event count to enable computation of event distribution function
+        this->flux[state_idx] += this->frozen_count - interval;
+#endif
+    } else {
+        /// update the accrued count by adding the signed values of current count and sampling interval
+        this->sampled_count += interval + this->frozen_count;
+#if 0
+        /// save event count to enable computation of event distribution function
+        this->flux[state_idx] = interval + this->frozen_count;
+#endif
+    }
+    /// separately preserve counts of overflowed counters, and
+    /// uncomment to always save fixed counters (to show performance impact of synchronization on call tree)
+    if ((this->frozen_count >= 0 && this->frozen_count >= this->slave_interval) || (this->selmsr == IA32_FIXED_CTR_CTRL && ovf)) {
+        this->count = this->sampled_count;
+    }
+#if 0
+    /// save timestamp & event count to enable computation of event distribution function
+    this->time[state_idx][1] = vtss_time_cpu();
+    /// toggle the state index
+    this->state_idx = state_idx ^ 1;
+#endif
+}
+
+static void vf_p6_restart_ro(cpuevent_t* this)
+{
+    rdmsrl(this->cntmsr, this->frozen_count);
+}
+
+static void vf_p6_freeze_read_ro(cpuevent_t* this)
+{
+    long long oldcnt = this->frozen_count;
+    long long newcnt;
+
+    rdmsrl(this->cntmsr, newcnt);
+
+    if (newcnt < oldcnt) {
+        this->count += CPU_EVTCNT_THRESHOLD;
+    }
+    this->count += newcnt - oldcnt;
+}
+
+static int vf_p6_overflowed(cpuevent_t* this)
+{
+    if (this->frozen_count >= 0) {
+        return 1;               // always signal overflow for no sampling mode and in case of real overflow
+    }
+    return 0;
+}
+
+static long long vf_p6_convert(cpuevent_t* this)
+{
+    return 0;
+}
+
+static void vf_p6_overflow_update(cpuevent_t* this)
+{
+}
+
+static void vf_p6_update_restart(cpuevent_t* this)
+{
+}
+
+static int vf_p6_select_muxgroup(cpuevent_t* this)
+{
+    return -1;
+}
+
+/// P6 virtual function tables
+static cpuevent_i vft_p6 = {
+    vf_p6_start,
+    vf_p6_stop,
+    vf_p6_read,
+    vf_p6_freeze,
+    vf_p6_restart,
+    vf_p6_freeze_read,
+    vf_p6_overflowed,
+    vf_p6_convert,
+    vf_p6_overflow_update,
+    vf_p6_update_restart,
+    vf_p6_select_muxgroup
+};
diff --git a/drivers/misc/intel/sepdk/vtsspp/cpuevents_sys.c b/drivers/misc/intel/sepdk/vtsspp/cpuevents_sys.c
new file mode 100644
index 000000000000..c0c819f87d95
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/cpuevents_sys.c
@@ -0,0 +1,343 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+
+/// system event control virtual functions
+static void vf_sys_start(cpuevent_t* this)
+{
+}
+
+static void vf_sys_stop(cpuevent_t* this)
+{
+}
+
+static void vf_sys_read(cpuevent_t* this)
+{
+}
+
+static void vf_sys_freeze(cpuevent_t* this)
+{
+}
+
+static void vf_sys_restart(cpuevent_t* this)
+{
+    switch (this->interval) {
+    case vtss_sysevent_sync_cs:
+        break;
+    case vtss_sysevent_preempt_cs:
+        break;
+    case vtss_sysevent_wait_time:
+        break;
+    case vtss_sysevent_inactive_time:
+        break;
+
+    case vtss_sysevent_energy_core:
+        rdmsrl(VTSS_MSR_PP0_ENERGY_STATUS, this->frozen_count);
+        this->frozen_count &= 0xffffffffLL;
+        break;
+
+    case vtss_sysevent_energy_gfx:
+        rdmsrl(VTSS_MSR_PP1_ENERGY_STATUS, this->frozen_count);
+        this->frozen_count &= 0xffffffffLL;
+        break;
+
+    case vtss_sysevent_energy_pack:
+        rdmsrl(VTSS_MSR_PKG_ENERGY_STATUS, this->frozen_count);
+        this->frozen_count &= 0xffffffffLL;
+        break;
+
+    case vtss_sysevent_energy_dram:
+        rdmsrl(VTSS_MSR_DRAM_ENERGY_STATUS, this->frozen_count);
+        this->frozen_count &= 0xffffffffLL;
+        break;
+
+    case vtss_sysevent_energy_soc:
+        break;
+#ifdef VTSS_SYSCALL_TRACE
+    case vtss_sysevent_syscall:
+        break;
+    case vtss_sysevent_syscall_time:
+        break;
+#endif
+    default:
+        break;
+    }
+}
+
+static void vf_sys_freeze_read(cpuevent_t* this)
+{
+    long long tmp;
+
+    switch (this->interval) {
+    case vtss_sysevent_sync_cs:
+        break;
+    case vtss_sysevent_preempt_cs:
+        break;
+    case vtss_sysevent_wait_time:
+        break;
+    case vtss_sysevent_inactive_time:
+        break;
+
+    case vtss_sysevent_energy_core:
+
+        rdmsrl(VTSS_MSR_PP0_ENERGY_STATUS, tmp);
+        tmp &= 0xffffffffLL;
+
+        if (tmp < this->frozen_count) {
+            tmp += 0x100000000LL;
+        }
+        this->count += (tmp - this->frozen_count) << 4;
+        break;
+
+    case vtss_sysevent_energy_gfx:
+
+        rdmsrl(VTSS_MSR_PP1_ENERGY_STATUS, tmp);
+        tmp &= 0xffffffffLL;
+
+        if (tmp < this->frozen_count) {
+            tmp += 0x100000000LL;
+        }
+        this->count += (tmp - this->frozen_count) << 4;
+        break;
+
+    case vtss_sysevent_energy_pack:
+
+        rdmsrl(VTSS_MSR_PKG_ENERGY_STATUS, tmp);
+        tmp &= 0xffffffffLL;
+
+        if (tmp < this->frozen_count) {
+            tmp += 0x100000000LL;
+        }
+        this->count += (tmp - this->frozen_count) << 4;
+        break;
+
+    case vtss_sysevent_energy_dram:
+
+        rdmsrl(VTSS_MSR_DRAM_ENERGY_STATUS, tmp);
+        tmp &= 0xffffffffLL;
+
+        if (tmp < this->frozen_count) {
+            tmp += 0x100000000LL;
+        }
+        this->count += (tmp - this->frozen_count) << 4;
+        break;
+
+    case vtss_sysevent_energy_soc:
+        break;
+#ifdef VTSS_SYSCALL_TRACE
+    case vtss_sysevent_syscall:
+        break;
+    case vtss_sysevent_syscall_time:
+        break;
+#endif
+    default:
+        break;
+    }
+}
+
+static int vf_sys_overflowed(cpuevent_t* this)
+{
+    return 0;
+}
+
+static long long vf_sys_convert(cpuevent_t* this)
+{
+    return 0;
+}
+
+static void vf_sys_overflow_update(cpuevent_t* this)
+{
+}
+
+/* this->tmp: positive - update and restart, negative - just restart
+ *  0 - reset counter,
+ *  1 - switch_to,
+ *  2 - preempt,
+ *  3 - sync,
+ * -1 - switch_to no update,
+ * -2 - preempt no update,
+ * -3 - sync no update
+ */
+static void vf_sys_update_restart(cpuevent_t* this)
+{
+    switch (this->interval) {
+    case vtss_sysevent_sync_cs:
+
+        if (this->tmp == 3) { /* sync */
+            this->count++;
+        }
+        break;
+
+    case vtss_sysevent_preempt_cs:
+
+        if (this->tmp == 2) { /* preempt */
+            this->count++;
+        }
+        break;
+
+    case vtss_sysevent_wait_time:
+
+        if (this->tmp == 1 && this->frozen_count) { /* switch_to */
+            this->count += vtss_time_cpu() - this->frozen_count;
+            this->frozen_count = 0;
+        } else if (abs(this->tmp) == 3) { /* sync */
+            this->frozen_count = vtss_time_cpu();
+        } else {
+            this->frozen_count = 0;
+        }
+        break;
+
+    case vtss_sysevent_inactive_time:
+
+        if (this->tmp == 1 && this->frozen_count) { /* switch_to */
+            this->count += vtss_time_cpu() - this->frozen_count;
+            this->frozen_count = 0;
+        } else if (abs(this->tmp) == 2) { /* preempt */
+            this->frozen_count = vtss_time_cpu();
+        } else {
+            this->frozen_count = 0;
+        }
+        break;
+
+    case vtss_sysevent_idle_time:
+
+        if (this->tmp == 1) { /* switch_to */
+            this->count += pcb_cpu.idle_duration;
+        }
+        break;
+
+    case vtss_sysevent_idle_wakeup:
+
+        if (this->tmp == 1) { /* switch_to */
+            if (pcb_cpu.idle_duration) {
+                this->count++;
+            }
+        }
+        break;
+
+    case vtss_sysevent_idle_c3:
+
+        if (this->tmp == 1) { /* switch_to */
+            this->count += pcb_cpu.idle_c3_residency;
+        }
+        break;
+
+    case vtss_sysevent_idle_c6:
+
+        if (this->tmp == 1) { /* switch_to */
+            this->count += pcb_cpu.idle_c6_residency;
+        }
+        break;
+
+    case vtss_sysevent_idle_c7:
+
+        if (this->tmp == 1) { /* switch_to */
+            this->count += pcb_cpu.idle_c7_residency;
+        }
+        break;
+
+    case vtss_sysevent_energy_core:
+        break;
+    case vtss_sysevent_energy_gfx:
+        break;
+    case vtss_sysevent_energy_pack:
+        break;
+    case vtss_sysevent_energy_dram:
+        break;
+    case vtss_sysevent_energy_soc:
+        break;
+#ifdef VTSS_SYSCALL_TRACE
+    case vtss_sysevent_syscall:
+
+        if (pcb_cpu.tcb_ptr) {
+            if (this->tmp == 1) { /* switch_to */
+                this->count += pcb_cpu.tcb_ptr->syscall_count;
+                pcb_cpu.tcb_ptr->syscall_count = 0;
+            } else {
+                pcb_cpu.tcb_ptr->syscall_count = 0;
+            }
+        }
+        break;
+
+    case vtss_sysevent_syscall_time:
+
+        if (pcb_cpu.tcb_ptr) {
+            if (this->tmp == 1) { /* switch_to */
+                this->count += pcb_cpu.tcb_ptr->syscall_duration;
+                pcb_cpu.tcb_ptr->syscall_duration = 0;
+            } else {
+                pcb_cpu.tcb_ptr->syscall_duration = 0;
+            }
+        }
+        break;
+#endif
+    default:
+        break;
+    }
+}
+
+static int vf_sys_select_muxgroup(cpuevent_t* this)
+{
+    return -1;
+}
+
+/// system virtual function tables
+static cpuevent_i vft_sys = {
+    vf_sys_start,
+    vf_sys_stop,
+    vf_sys_read,
+    vf_sys_freeze,
+    vf_sys_restart,
+    vf_sys_freeze_read,
+    vf_sys_overflowed,
+    vf_sys_convert,
+    vf_sys_overflow_update,
+    vf_sys_update_restart,
+    vf_sys_select_muxgroup
+};
+
+sysevent_e sysevent_type[] = {
+    vtss_sysevent_sync_cs,
+    vtss_sysevent_preempt_cs,
+    vtss_sysevent_wait_time,
+    vtss_sysevent_inactive_time,
+    vtss_sysevent_idle_time,
+    vtss_sysevent_idle_wakeup,
+    vtss_sysevent_idle_c3,
+    vtss_sysevent_idle_c6,
+    vtss_sysevent_idle_c7,
+    vtss_sysevent_energy_core,
+    vtss_sysevent_energy_gfx,
+    vtss_sysevent_energy_pack,
+    vtss_sysevent_energy_dram,
+    vtss_sysevent_energy_soc,
+#ifdef VTSS_SYSCALL_TRACE
+    vtss_sysevent_syscall,
+    vtss_sysevent_syscall_time,
+#endif
+    vtss_sysevent_end
+};
diff --git a/drivers/misc/intel/sepdk/vtsspp/cpumask_parselist_user.c b/drivers/misc/intel/sepdk/vtsspp/cpumask_parselist_user.c
new file mode 100644
index 000000000000..c91bddc041c7
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/cpumask_parselist_user.c
@@ -0,0 +1,162 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+/**
+ * This code was gotten from the freshest kernel's sources (3.4 branch)
+ * for compatibility with old kernels.
+ */
+#include <linux/ctype.h>
+
+/**
+ * __bitmap_parselist - convert list format ASCII string to bitmap
+ * @buf: read nul-terminated user string from this buffer
+ * @buflen: buffer size in bytes.  If string is smaller than this
+ *    then it must be terminated with a \0.
+ * @is_user: location of buffer, 0 indicates kernel space
+ * @maskp: write resulting mask here
+ * @nmaskbits: number of bits in mask to be written
+ *
+ * Input format is a comma-separated list of decimal numbers and
+ * ranges.  Consecutively set bits are shown as two hyphen-separated
+ * decimal numbers, the smallest and largest bit numbers set in
+ * the range.
+ *
+ * Returns 0 on success, -errno on invalid input strings.
+ * Error values:
+ *    %-EINVAL: second number in range smaller than first
+ *    %-EINVAL: invalid character in string
+ *    %-ERANGE: bit number specified too large for mask
+ */
+static int __bitmap_parselist(const char *buf, unsigned int buflen,
+		int is_user, unsigned long *maskp,
+		int nmaskbits)
+{
+	unsigned a, b;
+	int c, old_c, totaldigits;
+	const char __user __force *ubuf = (const char __user __force *)buf;
+	int exp_digit, in_range;
+
+	totaldigits = c = 0;
+	bitmap_zero(maskp, nmaskbits);
+	do {
+		exp_digit = 1;
+		in_range = 0;
+		a = b = 0;
+
+		/* Get the next cpu# or a range of cpu#'s */
+		while (buflen) {
+			old_c = c;
+			if (is_user) {
+				if (__get_user(c, ubuf++))
+					return -EFAULT;
+			} else
+				c = *buf++;
+			buflen--;
+			if (isspace(c))
+				continue;
+
+			/*
+			 * If the last character was a space and the current
+			 * character isn't '\0', we've got embedded whitespace.
+			 * This is a no-no, so throw an error.
+			 */
+			if (totaldigits && c && isspace(old_c))
+				return -EINVAL;
+
+			/* A '\0' or a ',' signal the end of a cpu# or range */
+			if (c == '\0' || c == ',')
+				break;
+
+			if (c == '-') {
+				if (exp_digit || in_range)
+					return -EINVAL;
+				b = 0;
+				in_range = 1;
+				exp_digit = 1;
+				continue;
+			}
+
+			if (!isdigit(c))
+				return -EINVAL;
+
+			b = b * 10 + (c - '0');
+			if (!in_range)
+				a = b;
+			exp_digit = 0;
+			totaldigits++;
+		}
+		if (!(a <= b))
+			return -EINVAL;
+		if (b >= nmaskbits)
+			return -ERANGE;
+		while (a <= b) {
+			set_bit(a, maskp);
+			a++;
+		}
+	} while (buflen && c == ',');
+	return 0;
+}
+
+/**
+ * bitmap_parselist_user()
+ *
+ * @ubuf: pointer to user buffer containing string.
+ * @ulen: buffer size in bytes.  If string is smaller than this
+ *    then it must be terminated with a \0.
+ * @maskp: pointer to bitmap array that will contain result.
+ * @nmaskbits: size of bitmap, in bits.
+ *
+ * Wrapper for bitmap_parselist(), providing it with user buffer.
+ *
+ * We cannot have this as an inline function in bitmap.h because it needs
+ * linux/uaccess.h to get the access_ok() declaration and this causes
+ * cyclic dependencies.
+ */
+static inline int bitmap_parselist_user(const char __user *ubuf,
+                        unsigned int ulen, unsigned long *maskp,
+                        int nmaskbits)
+{
+        if (!access_ok(VERIFY_READ, ubuf, ulen))
+                return -EFAULT;
+        return __bitmap_parselist((const char __force *)ubuf,
+                                        ulen, 1, maskp, nmaskbits);
+}
+
+/**
+ * cpumask_parselist_user - extract a cpumask from a user string
+ * @buf: the buffer to extract from
+ * @len: the length of the buffer
+ * @dstp: the cpumask to set.
+ *
+ * Returns -errno, or 0 for success.
+ */
+static inline int cpumask_parselist_user(const char __user *buf, int len,
+                                     struct cpumask *dstp)
+{
+        return bitmap_parselist_user(buf, len, cpumask_bits(dstp),
+                                                        nr_cpumask_bits);
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/dsa.c b/drivers/misc/intel/sepdk/vtsspp/dsa.c
new file mode 100644
index 000000000000..11e8aa3ff025
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/dsa.c
@@ -0,0 +1,321 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "dsa.h"
+#include "globals.h"
+
+#include <linux/percpu.h>
+#include <linux/slab.h>
+#ifdef VTSS_CONFIG_KPTI
+#include <asm/cpu_entry_area.h>
+#endif
+
+#define DS_AREA_MSR 0x0600
+
+static DEFINE_PER_CPU_SHARED_ALIGNED(unsigned long long, vtss_dsa_cpu_msr);
+static DEFINE_PER_CPU_SHARED_ALIGNED(vtss_dsa_t*, vtss_dsa_per_cpu);
+
+vtss_dsa_t* vtss_dsa_get(int cpu)
+{
+    return per_cpu(vtss_dsa_per_cpu, cpu);
+}
+
+void vtss_dsa_init_cpu(void)
+{
+    if (hardcfg.family == 0x06 || hardcfg.family == 0x0f) {
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,19,0)
+        vtss_dsa_t *dsa = __get_cpu_var(vtss_dsa_per_cpu);
+#else
+        vtss_dsa_t *dsa = *this_cpu_ptr(&vtss_dsa_per_cpu);
+#endif
+        if (IS_DSA_64ON32) {
+            dsa->v32.reserved[0] = dsa->v32.reserved[1] = NULL;
+            dsa->v32.reserved[2] = dsa->v32.reserved[3] = NULL;
+        } else {
+            dsa->v64.reserved[0] = dsa->v64.reserved[1] = NULL;
+        }
+        wrmsrl(DS_AREA_MSR, (size_t)dsa);
+    }
+}
+
+#ifdef VTSS_CONFIG_KPTI
+
+static int vtss_dsa_alloc_buffer(int cpu)
+{
+    per_cpu(vtss_dsa_per_cpu, cpu) = (vtss_dsa_t*)&get_cpu_entry_area(cpu)->cpu_debug_store;
+    return 0;
+}
+
+static void vtss_dsa_release_buffer(int cpu)
+{
+    per_cpu(vtss_dsa_per_cpu, cpu) = NULL;
+}
+
+#elif defined(VTSS_CONFIG_KAISER)
+
+static int vtss_dsa_alloc_buffer(int cpu)
+{
+    void *buffer;
+
+    per_cpu(vtss_dsa_per_cpu, cpu) = NULL;
+    buffer = vtss_kaiser_alloc_pages(sizeof(vtss_dsa_t), GFP_KERNEL, cpu);
+    if (unlikely(!buffer)) {
+        ERROR("Cannot allocate DSA buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+    per_cpu(vtss_dsa_per_cpu, cpu) = buffer;
+    TRACE("allocated buffer for %d cpu, buffer=%p", cpu, buffer);
+    return 0;
+}
+
+static void vtss_dsa_release_buffer(int cpu)
+{
+    void *buffer;
+
+    buffer = per_cpu(vtss_dsa_per_cpu, cpu);
+    vtss_kaiser_free_pages(buffer, sizeof(vtss_dsa_t));
+    TRACE("released buffer for %d cpu, buffer=%p", cpu, buffer);
+}
+
+#else
+
+static int vtss_dsa_alloc_buffer(int cpu)
+{
+    per_cpu(vtss_dsa_per_cpu, cpu) = NULL;
+    if ((per_cpu(vtss_dsa_per_cpu, cpu) = (vtss_dsa_t*)kmalloc_node(
+            sizeof(vtss_dsa_t), (GFP_KERNEL | __GFP_ZERO), cpu_to_node(cpu))) == NULL)
+    {
+        ERROR("Cannot allocate DSA buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+    return 0;
+}
+
+static void vtss_dsa_release_buffer(int cpu)
+{
+    if (per_cpu(vtss_dsa_per_cpu, cpu) != NULL)
+        kfree(per_cpu(vtss_dsa_per_cpu, cpu));
+}
+
+#endif
+
+static void vtss_dsa_on_each_cpu_init(void* ctx)
+{
+    if (hardcfg.family == 0x06 || hardcfg.family == 0x0f) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,19,0)
+        rdmsrl(DS_AREA_MSR, __get_cpu_var(vtss_dsa_cpu_msr));
+#else
+        rdmsrl(DS_AREA_MSR, *this_cpu_ptr(&vtss_dsa_cpu_msr));
+#endif
+    }
+}
+
+static void vtss_dsa_on_each_cpu_fini(void* ctx)
+{
+    if (hardcfg.family == 0x06 || hardcfg.family == 0x0f) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,19,0)
+        wrmsrl(DS_AREA_MSR, __get_cpu_var(vtss_dsa_cpu_msr));
+#else
+        wrmsrl(DS_AREA_MSR, *this_cpu_ptr(&vtss_dsa_cpu_msr));
+#endif
+    }
+}
+
+int vtss_dsa_init(void)
+{
+    int cpu;
+
+    on_each_cpu(vtss_dsa_on_each_cpu_init, NULL, SMP_CALL_FUNCTION_ARGS);
+    for_each_possible_cpu(cpu) {
+        if (vtss_dsa_alloc_buffer(cpu)) goto fail;
+    }
+    return 0;
+fail:
+    for_each_possible_cpu(cpu) {
+        vtss_dsa_release_buffer(cpu);
+    }
+    return VTSS_ERR_NOMEMORY;
+}
+
+void vtss_dsa_fini(void)
+{
+    int cpu;
+
+    on_each_cpu(vtss_dsa_on_each_cpu_fini, NULL, SMP_CALL_FUNCTION_ARGS);
+    for_each_possible_cpu(cpu) {
+        vtss_dsa_release_buffer(cpu);
+    }
+}
+
+#ifdef VTSS_CONFIG_KPTI
+#include <asm/tlbflush.h>
+#include <linux/kallsyms.h>
+
+static void (*vtss_cea_set_pte)(void *cea_vaddr, phys_addr_t pa, pgprot_t flags) = NULL;
+static void (*vtss_do_kernel_range_flush)(void *info) = NULL;
+
+int vtss_cea_init(void)
+{
+    if (vtss_cea_set_pte == NULL) {
+        vtss_cea_set_pte = (void*)vtss_kallsyms_lookup_name("cea_set_pte");
+        if (vtss_cea_set_pte == NULL) {
+            ERROR("Cannot find 'cea_set_pte' symbol");
+            return VTSS_ERR_INTERNAL;
+        }
+    }
+    if (vtss_do_kernel_range_flush == NULL) {
+        vtss_do_kernel_range_flush = (void*)vtss_kallsyms_lookup_name("do_kernel_range_flush");
+        if (vtss_do_kernel_range_flush == NULL) {
+            ERROR("Cannot find 'do_kernel_range_flush' symbol");
+            return VTSS_ERR_INTERNAL;
+        }
+    }
+    INFO("KPTI: enabled");
+    return 0;
+}
+
+void vtss_cea_update(void *cea, void *addr, size_t size, pgprot_t prot)
+{
+    unsigned long start = (unsigned long)cea;
+    struct flush_tlb_info info;
+    phys_addr_t pa;
+    size_t msz = 0;
+
+    pa = virt_to_phys(addr);
+
+    preempt_disable();
+    for (; msz < size; msz += PAGE_SIZE, pa += PAGE_SIZE, cea += PAGE_SIZE)
+        vtss_cea_set_pte(cea, pa, prot);
+
+    info.start = start;
+    info.end = start + size;
+    vtss_do_kernel_range_flush(&info);
+    preempt_enable();
+}
+
+void vtss_cea_clear(void *cea, size_t size)
+{
+    unsigned long start = (unsigned long)cea;
+    struct flush_tlb_info info;
+    size_t msz = 0;
+
+    preempt_disable();
+    for (; msz < size; msz += PAGE_SIZE, cea += PAGE_SIZE)
+        vtss_cea_set_pte(cea, 0, PAGE_NONE);
+
+    info.start = start;
+    info.end = start + size;
+    vtss_do_kernel_range_flush(&info);
+    preempt_enable();
+}
+
+void *vtss_cea_alloc_pages(size_t size, gfp_t flags, int cpu)
+{
+    unsigned int order = get_order(size);
+    int node = cpu_to_node(cpu);
+    struct page *page;
+
+    page = alloc_pages_node(node, flags | __GFP_ZERO, order);
+    return page ? page_address(page) : NULL;
+}
+
+void vtss_cea_free_pages(const void *buffer, size_t size)
+{
+    if (buffer)
+        free_pages((unsigned long)buffer, get_order(size));
+}
+#endif
+
+#ifdef VTSS_CONFIG_KAISER
+#include <linux/kaiser.h>
+#include <linux/mm.h>
+#include <linux/kallsyms.h>
+
+static int (*vtss_kaiser_add_mapping)(unsigned long addr, unsigned long size, pteval_t flags) = NULL;
+static void (*vtss_kaiser_remove_mapping)(unsigned long start, unsigned long size) = NULL;
+static int *vtss_kaiser_enabled_ptr = NULL;
+
+int vtss_kaiser_init(void)
+{
+    vtss_kaiser_enabled_ptr = (int*)vtss_kallsyms_lookup_name("kaiser_enabled");
+
+    if (vtss_kaiser_enabled_ptr && *vtss_kaiser_enabled_ptr) {
+        if (vtss_kaiser_add_mapping == NULL) {
+            vtss_kaiser_add_mapping = (void*)vtss_kallsyms_lookup_name("kaiser_add_mapping");
+            if (vtss_kaiser_add_mapping == NULL) {
+                ERROR("Cannot find 'kaiser_add_mapping' symbol");
+                return VTSS_ERR_INTERNAL;
+            }
+        }
+        if (vtss_kaiser_remove_mapping == NULL) {
+            vtss_kaiser_remove_mapping = (void*)vtss_kallsyms_lookup_name("kaiser_remove_mapping");
+            if (vtss_kaiser_remove_mapping == NULL) {
+                ERROR("Cannot find 'kaiser_remove_mapping' symbol");
+                return VTSS_ERR_INTERNAL;
+            }
+        }
+        INFO("KAISER: enabled");
+    }
+    else {
+        INFO("KAISER: disabled");
+    }
+    return 0;
+}
+
+void *vtss_kaiser_alloc_pages(size_t size, gfp_t flags, int cpu)
+{
+    unsigned int order = get_order(size);
+    int node = cpu_to_node(cpu);
+    struct page *page;
+    unsigned long addr;
+
+    page = alloc_pages_node(node, flags | __GFP_ZERO, order);
+    if (!page)
+        return NULL;
+    addr = (unsigned long)page_address(page);
+    if (vtss_kaiser_enabled_ptr && *vtss_kaiser_enabled_ptr) {
+        if (vtss_kaiser_add_mapping(addr, size, __PAGE_KERNEL | _PAGE_GLOBAL) < 0) {
+            __free_pages(page, order);
+            addr = 0;
+        }
+    }
+    return (void *)addr;
+}
+
+void vtss_kaiser_free_pages(const void *buffer, size_t size)
+{
+    if (!buffer)
+        return;
+    if (vtss_kaiser_enabled_ptr && *vtss_kaiser_enabled_ptr) {
+        vtss_kaiser_remove_mapping((unsigned long)buffer, size);
+    }
+    free_pages((unsigned long)buffer, get_order(size));
+}
+
+#endif
diff --git a/drivers/misc/intel/sepdk/vtsspp/globals.c b/drivers/misc/intel/sepdk/vtsspp/globals.c
new file mode 100644
index 000000000000..2402d919d040
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/globals.c
@@ -0,0 +1,506 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "globals.h"
+#include "apic.h"
+#include "time.h"
+
+#include <linux/utsname.h>
+#include <linux/module.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/slab.h>
+#include <asm/uaccess.h>
+#include <linux/kallsyms.h>
+
+/// processor control blocks
+#ifdef DEFINE_PER_CPU_SHARED_ALIGNED
+DEFINE_PER_CPU_SHARED_ALIGNED(vtss_pcb_t, vtss_pcb);
+#else
+DEFINE_PER_CPU(vtss_pcb_t, vtss_pcb);
+#endif
+
+/// trace format information to enable forward compatibility
+fmtcfg_t fmtcfg[2];
+
+/// system configuration
+vtss_syscfg_t syscfg;
+
+/// hardware configuration
+vtss_hardcfg_t hardcfg;
+
+vtss_iptcfg_t iptcfg;
+
+/// profiling configuration
+process_cfg_t reqcfg;
+
+/* time source for collection */
+int vtss_time_source = 0;
+
+/* time limit for collection */
+cycles_t vtss_time_limit = 0ULL;
+
+unsigned long vtss_syscall_rsp_ptr = 0;
+
+static void vtss_fmtcfg_init(void)
+{
+    /*
+     * leaf 1: base 
+     */
+    fmtcfg[0].rank = 0;
+    fmtcfg[0].and_mask = UEC_LEAF0 | UEC_LEAF1 | UEC_LEAF2 | UEC_LEAF3;
+    fmtcfg[0].cmp_mask = UEC_LEAF1;
+    fmtcfg[0].defcount = 0x20;
+
+    fmtcfg[0].defbit[0x00] = 4; /// UECL1_ACTIVITY      0x00000001
+    fmtcfg[0].defbit[0x01] = 4; /// UECL1_VRESIDX       0x00000002
+    fmtcfg[0].defbit[0x02] = 4; /// UECL1_CPUIDX        0x00000004
+    fmtcfg[0].defbit[0x03] = 8; /// UECL1_USRLVLID      0x00000008
+    fmtcfg[0].defbit[0x04] = 8; /// UECL1_CPUTSC        0x00000010
+    fmtcfg[0].defbit[0x05] = 8; /// UECL1_REALTSC       0x00000020
+    fmtcfg[0].defbit[0x06] = 1; /// UECL1_MUXGROUP      0x00000040
+    fmtcfg[0].defbit[0x07] = 8; /// UECL1_CPUEVENT      0x00000080
+    fmtcfg[0].defbit[0x08] = 8; /// UECL1_CHPSETEV      0x00000100
+    fmtcfg[0].defbit[0x09] = 8; /// UECL1_OSEVENT       0x00000200
+    fmtcfg[0].defbit[0x0a] = 8; /// UECL1_EXECADDR      0x00000400
+    fmtcfg[0].defbit[0x0b] = 8; /// UECL1_REFADDR       0x00000800
+    fmtcfg[0].defbit[0x0c] = 8; /// UECL1_EXEPHYSADDR   0x00001000
+    fmtcfg[0].defbit[0x0d] = 8; /// UECL1_REFPHYSADDR   0x00002000
+    fmtcfg[0].defbit[0x0e] = 4; /// UECL1_TPIDX         0x00004000
+    fmtcfg[0].defbit[0x0f] = 8; /// UECL1_TPADDR        0x00008000
+    fmtcfg[0].defbit[0x10] = 8; /// UECL1_PWREVENT      0x00010000
+    fmtcfg[0].defbit[0x11] = 8; /// UECL1_CPURECTSC     0x00020000
+    fmtcfg[0].defbit[0x12] = 8; /// UECL1_REALRECTSC    0x00040000
+    fmtcfg[0].defbit[0x13] = 81;    /// UECL1_PADDING       0x00080000
+    fmtcfg[0].defbit[0x14] = VTSS_FMTCFG_RESERVED;  /// UECL1_UNKNOWN0      0x00100000
+    fmtcfg[0].defbit[0x15] = VTSS_FMTCFG_RESERVED;  /// UECL1_UNKNOWN1      0x00200000
+    fmtcfg[0].defbit[0x16] = 82;    /// UECL1_SYSTRACE      0x00400000
+    fmtcfg[0].defbit[0x17] = 84;    /// UECL1_LARGETRACE    0x00800000
+    fmtcfg[0].defbit[0x18] = 82;    /// UECL1_USERTRACE     0x01000000
+    fmtcfg[0].defbit[0x19] = 0;
+    fmtcfg[0].defbit[0x1a] = 0;
+    fmtcfg[0].defbit[0x1b] = 0;
+    fmtcfg[0].defbit[0x1c] = 0;
+    fmtcfg[0].defbit[0x1d] = 0;
+    fmtcfg[0].defbit[0x1e] = 0;
+    fmtcfg[0].defbit[0x1f] = 0;
+
+    /*
+     * leaf 1: extended 
+     */
+    fmtcfg[1].rank = 1;
+    fmtcfg[1].and_mask = UEC_LEAF0 | UEC_LEAF1 | UEC_LEAF2 | UEC_LEAF3;
+    fmtcfg[1].cmp_mask = UEC_LEAF1;
+    fmtcfg[1].defcount = 0x20;
+
+    fmtcfg[1].defbit[0x00] = 8; /// UECL1_EXT_CPUFREQ   0x00000001
+    fmtcfg[1].defbit[0x01] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x02] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x03] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x04] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x05] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x06] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x07] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x08] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x09] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x0a] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x0b] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x0c] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x0d] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x0e] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x0f] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x10] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x11] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x12] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x13] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x14] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x15] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x16] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x17] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x18] = VTSS_FMTCFG_RESERVED;
+    fmtcfg[1].defbit[0x19] = 0;
+    fmtcfg[1].defbit[0x1a] = 0;
+    fmtcfg[1].defbit[0x1b] = 0;
+    fmtcfg[1].defbit[0x1c] = 0;
+    fmtcfg[1].defbit[0x1d] = 0;
+    fmtcfg[1].defbit[0x1e] = 0;
+    fmtcfg[1].defbit[0x1f] = 0;
+}
+
+static void vtss_syscfg_init(void)
+{
+    char utsname[2*(__NEW_UTS_LEN+1)+2];
+    vtss_syscfg_t *sysptr = &syscfg;
+    struct new_utsname *u = init_utsname();
+
+    /// sysinfo
+    syscfg.version = 1;
+    syscfg.major = (short)0;
+    syscfg.minor = (short)0;
+    syscfg.spack = (short)0;
+    syscfg.extra = (short)0;
+#if defined(CONFIG_X86_32)
+    syscfg.type  = VTSS_LINUX_IA32;
+#elif defined(CONFIG_X86_64)
+#if defined(VTSS_ARCH_KNX)
+    syscfg.type  = VTSS_LINUX_KNC;
+#else
+    syscfg.type  = VTSS_LINUX_EM64T;
+#endif
+#else
+    syscfg.type  = VTSS_UNKNOWN_ARCH;
+#endif
+
+    /// host name
+    TRACE("u->nodename='%s'", u->nodename);
+    sysptr->len = 1 + strlen(u->nodename);
+    memcpy(sysptr->host_name, u->nodename, sysptr->len);
+    sysptr = (vtss_syscfg_t*)((char*)sysptr + sysptr->len + sizeof(short));
+
+    /// platform brand name
+    TRACE("u->sysname='%s'", u->sysname);
+    TRACE("u->machine='%s'", u->machine);
+    snprintf(utsname, sizeof(utsname)-1, "%s-%s", u->sysname, u->machine);
+    sysptr->len = 1 + strlen(utsname);
+    memcpy(sysptr->brand_name, utsname, sysptr->len);
+    sysptr = (vtss_syscfg_t*)((char*)sysptr + sysptr->len + sizeof(short));
+
+    /// system ID string
+    TRACE("u->release='%s'", u->release);
+    TRACE("u->version='%s'", u->version);
+    snprintf(utsname, sizeof(utsname)-1, "%s %s", u->release, u->version);
+    sysptr->len = 1 + strlen(utsname);
+    memcpy(sysptr->host_name, utsname, sysptr->len);
+    sysptr = (vtss_syscfg_t*)((char*)sysptr + sysptr->len + sizeof(short));
+
+    /// root directory
+    sysptr->len = 2; /* 1 + strlen("/") */
+    memcpy(sysptr->host_name, "/", sysptr->len);
+    sysptr = (vtss_syscfg_t*)((char*)sysptr + sysptr->len + sizeof(short));
+
+    syscfg.record_size = (int)((char *)sysptr - (char *)&syscfg + (char *)&syscfg.len - (char *)&syscfg);
+}
+
+union cpuid_01H_eax
+{
+    struct
+    {
+        unsigned int stepping:4;
+        unsigned int model:4;
+        unsigned int family:4;
+        unsigned int type:2;
+        unsigned int reserved1:2;
+        unsigned int model_ext:4;
+        unsigned int family_ext:8;
+        unsigned int reserved2:4;
+    } split;
+    unsigned int full;
+};
+
+union cpuid_01H_ebx
+{
+    struct
+    {
+        unsigned int brand_index:8;
+        unsigned int cache_line_size:8;
+        unsigned int unit_no:8;
+        unsigned int reserved:8;
+    } split;
+    unsigned int full;
+};
+
+union cpuid_04H_eax
+{
+    struct
+    {
+        unsigned int reserved:14;
+        unsigned int smt_no:12;
+        unsigned int core_no:6;
+    } split;
+    unsigned int full;
+};
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
+#define vtss_ktime_equal(cmp1, cmp2) ktime_equal(cmp1, cmp2)
+#else
+#define vtss_ktime_equal(cmp1, cmp2) ((cmp1) == (cmp2))
+#endif
+
+static void vtss_hardcfg_init(void)
+{
+    int cpu;
+    union cpuid_01H_eax eax1;
+    union cpuid_01H_ebx ebx1;
+    union cpuid_04H_eax eax4;
+    unsigned int ebx, ecx, edx;
+    int node_no   = 0;
+    int pack_no   = 0;
+    int core_no   = 0;
+    int thread_no = 0;
+    int ht_supported = 0;
+    int max_cpu_id = 0;
+
+    /* Global variable vtss_time_source affects result of vtss_*_real() */
+    vtss_time_source = 0;
+    if (vtss_ktime_equal(KTIME_MONOTONIC_RES, KTIME_LOW_RES)) {
+        INFO("An accuracy of kernel timer is not enough. Switch to TSC.");
+        vtss_time_source = 1;
+    }
+    hardcfg.timer_freq = vtss_freq_real(); /* should be after change vtss_time_source */
+    hardcfg.cpu_freq   = vtss_freq_cpu();
+    hardcfg.version = 0x0002;
+    // for 32 bits is like 0xC0000000
+    // for 64 bits is like 0xffff880000000000
+    hardcfg.maxusr_address = PAGE_OFFSET; /*< NOTE: Will be changed in vtss_record_configs() */
+    /// initialize execution mode, OS version, and CPU ID parameters
+#if defined(CONFIG_X86_32)
+    hardcfg.mode    = 32;
+    hardcfg.os_type = VTSS_LINUX_IA32;
+#elif defined(CONFIG_X86_64)
+    hardcfg.mode    = 64;
+#if defined(VTSS_ARCH_KNX)
+    hardcfg.os_type = VTSS_LINUX_KNC;
+#else
+    hardcfg.os_type = VTSS_LINUX_EM64T;
+#endif
+#else
+    hardcfg.mode    = 0;
+    hardcfg.os_type = VTSS_UNKNOWN_ARCH;
+#endif
+    hardcfg.os_major = 0;
+    hardcfg.os_minor = 0;
+    hardcfg.os_sp    = 0;
+
+    cpuid(0x01, &eax1.full, &ebx1.full, &ecx, &edx);
+    if ((hardcfg.family = eax1.split.family) == 0x0f)
+        hardcfg.family += eax1.split.family_ext;
+    hardcfg.model = eax1.split.model;
+    if (eax1.split.family == 0x06 || eax1.split.family == 0x0f)
+        hardcfg.model += (eax1.split.model_ext << 4);
+    hardcfg.stepping = eax1.split.stepping;
+    ht_supported = ((edx >> 28) & 1) ? 1 : 0;
+    TRACE("CPUID(family=%02x, model=%02x, stepping=%02x, ht=%d)", hardcfg.family, hardcfg.model, hardcfg.stepping, ht_supported);
+
+    cpuid(0x04, &eax4.full, &ebx, &ecx, &edx);
+
+    for_each_present_cpu(cpu) {
+        if (cpu_present(cpu)) {
+            if (cpu > max_cpu_id) {
+                max_cpu_id = cpu;
+            }
+        }
+    }
+    if (max_cpu_id + 1 > num_present_cpus()) {
+        hardcfg.cpu_no = max_cpu_id + 1;
+    }
+    else {
+        hardcfg.cpu_no = num_present_cpus();
+    }
+    /* TODO: determine the number of nodes */
+    node_no = 1;
+    if (hardcfg.cpu_no == 1) {
+        thread_no = core_no = pack_no = 1;
+    } else {
+        if ((hardcfg.family == 0x0f && hardcfg.model >= 0x04 && hardcfg.stepping >= 0x04) ||
+            (hardcfg.family == 0x06 && hardcfg.model >= 0x0f))
+        {
+            thread_no = eax4.split.smt_no  + 1;
+            core_no   = eax4.split.core_no + 1;
+        } else if (hardcfg.family == 0x0f) { // P4
+            thread_no = ebx1.split.unit_no;
+            core_no   = 1;
+        } else if (hardcfg.family == 0x0b) { // KNX_CORE
+            thread_no = 4;
+            core_no   = eax4.split.core_no ? (eax4.split.core_no + 1) : (hardcfg.cpu_no / thread_no);
+        } else {
+            thread_no = ebx1.split.unit_no;
+            core_no   = eax4.split.core_no + 1;
+        }
+        thread_no = thread_no ? thread_no : 1;
+        core_no   = core_no   ? core_no   : 1;
+        pack_no   = hardcfg.cpu_no / (core_no * thread_no * node_no);
+    }
+    TRACE("cpu_no=%d, node_no=%d, pack_no=%d, core_no=%d, thread_no=%d",
+          hardcfg.cpu_no, node_no, pack_no, core_no, thread_no);
+
+    /*
+     * disable the driver for P4 as it is not going
+     * to be supported in the future
+     */
+    if (hardcfg.family == 0x0f) { // P4
+        hardcfg.family = VTSS_UNKNOWN_ARCH;
+    }
+
+    /*
+     * build cpu map - distribute the current thread to all CPUs
+     * to compute CPU IDs for asymmetric system configurations
+     */
+    for_each_present_cpu(cpu) {
+        struct cpuinfo_x86 *c = &cpu_data(cpu);
+
+        hardcfg.cpu_map[cpu].node   = cpu_to_node(cpu);
+        hardcfg.cpu_map[cpu].pack   = c->phys_proc_id;
+        hardcfg.cpu_map[cpu].core   = c->cpu_core_id;
+        hardcfg.cpu_map[cpu].thread = c->initial_apicid & (thread_no - 1);
+        TRACE("cpu[%d]: node=%d, pack=%d, core=%d, thread=%d",
+                cpu, hardcfg.cpu_map[cpu].node, hardcfg.cpu_map[cpu].pack,
+                hardcfg.cpu_map[cpu].core, hardcfg.cpu_map[cpu].thread);
+    }
+}
+
+#define extract_bits(val, pos, len) (((val) >> (pos)) & ((1 << (len)) - 1))
+static inline long long read_msr(int idx)
+{
+    long long val;
+    rdmsrl(idx, val);
+    return val;
+}
+static void vtss_iptcfg_init(void)
+{
+    unsigned int eax, ebx, ecx, edx;
+    memset(&iptcfg, 0, sizeof(iptcfg));
+
+    if(hardcfg.family == 0x06 && (hardcfg.model == 0x4e /* SKL */ || hardcfg.model == 0x5e /* SKL */ || 
+       hardcfg.model == 0x55 /* SKX */ || hardcfg.model == 0x5c /* GLM */ || hardcfg.model == 0x5f /* DNV */ || 
+       hardcfg.model == 0x7a /* GLP */ || hardcfg.model == 0x42 /* CNL */ ||
+       hardcfg.model == 0x9e /* KBL */ || hardcfg.model == 0x8e /* KBL */))
+    {
+      iptcfg.version = 0;
+      iptcfg.fratio = extract_bits((unsigned)read_msr(0xce), 8, 8);
+      cpuid(0x15, &eax, &ebx, &ecx, &edx);
+      iptcfg.ctcnom = ebx;
+      iptcfg.tscdenom = eax;
+//                              read_cpuid21((long long*)&iptcfg.ctcnom);
+      iptcfg.mtcfreq = 0;
+    }
+}
+
+#if defined(CONFIG_KALLSYMS)
+#if(LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0))
+unsigned long vtss_kallsyms_lookup_name(char *name)
+{
+    return kallsyms_lookup_name(name);
+}
+#else
+struct vtss_kallsyms_data
+{
+    char *name;
+    unsigned long ptr;
+};
+
+static int vtss_kallsyms_lookup_callback(void* data, const char *name, struct module *mod, unsigned long addr)
+{
+    struct vtss_kallsyms_data *kdata = data;
+
+    if (name) {
+        if (kdata) {
+            if (!strcmp(name, kdata->name)) {
+                kdata->ptr = addr;
+                return 1;
+            }
+        }
+    }
+    return 0;
+}
+
+unsigned long vtss_kallsyms_lookup_name(char *name)
+{
+    struct vtss_kallsyms_data kdata;
+
+    kdata.name = name;
+    kdata.ptr = 0;
+    kallsyms_on_each_symbol(vtss_kallsyms_lookup_callback, (void*)&kdata);
+    TRACE("symbol: %s=0x%lx", name, kdata.ptr);
+    return kdata.ptr;
+}
+#endif
+#else
+unsigned long vtss_kallsyms_lookup_name(char *name)
+{
+    return NULL;
+}
+#endif
+
+static void vtss_lookup_old_rsp( void )
+{
+    vtss_syscall_rsp_ptr = vtss_kallsyms_lookup_name("old_rsp");
+    if (!vtss_syscall_rsp_ptr) vtss_syscall_rsp_ptr = vtss_kallsyms_lookup_name("per_cpu__old_rsp");
+    if (!vtss_syscall_rsp_ptr) vtss_syscall_rsp_ptr = vtss_kallsyms_lookup_name("rsp_scratch");
+}
+
+void vtss_globals_fini(void)
+{
+    int cpu;
+
+#ifndef VTSS_USE_NMI
+    vtss_apic_fini();
+#endif
+    for_each_possible_cpu(cpu) {
+        vtss_pcb_t* ppcb = &pcb(cpu);
+        if (ppcb->scratch_ptr != NULL)
+            kfree(ppcb->scratch_ptr);
+        ppcb->scratch_ptr = NULL;
+    }
+}
+
+int vtss_globals_init(void)
+{
+    int cpu;
+
+    memset(&syscfg,  0, sizeof(vtss_syscfg_t));
+    memset(&hardcfg, 0, sizeof(vtss_hardcfg_t));
+    memset(&fmtcfg,  0, sizeof(fmtcfg_t)*2);
+    memset(&reqcfg,  0, sizeof(process_cfg_t));
+    for_each_possible_cpu(cpu) {
+        vtss_pcb_t* ppcb = &pcb(cpu);
+        memset(ppcb, 0, sizeof(vtss_pcb_t));
+        ppcb->scratch_ptr = kmalloc_node(VTSS_DYNSIZE_SCRATCH, GFP_KERNEL, cpu_to_node(cpu));
+        if (ppcb->scratch_ptr == NULL)
+            goto fail;
+    }
+#ifndef VTSS_USE_NMI
+    vtss_apic_init(); /* Need for vtss_hardcfg_init() */
+#endif
+    vtss_syscfg_init();
+    vtss_hardcfg_init();
+    vtss_iptcfg_init();
+    vtss_fmtcfg_init();
+    vtss_lookup_old_rsp();
+    return 0;
+
+fail:
+    for_each_possible_cpu(cpu) {
+        vtss_pcb_t* ppcb = &pcb(cpu);
+        if (ppcb->scratch_ptr != NULL)
+            kfree(ppcb->scratch_ptr);
+        ppcb->scratch_ptr = NULL;
+    }
+    ERROR("NO memory");
+    return VTSS_ERR_NOMEMORY;
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/ipt.c b/drivers/misc/intel/sepdk/vtsspp/ipt.c
new file mode 100644
index 000000000000..9d222245f9ff
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/ipt.c
@@ -0,0 +1,867 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+//#include <linux/dma-mapping.h>
+#include <linux/slab.h>
+#include <asm/io.h>
+#include "vtss_config.h"
+#include "globals.h"
+#include "time.h"
+#include "ipt.h"
+#include "iptdec.h"
+/**
+// Intel Processor Trace functionality
+*/
+#if 0
+int vtss_ipt_init(void)
+{
+    int i;
+    int res = 0;
+    dma_addr_t dma_addr;
+    /// for each CPU:
+    ///   allocate ToPA page
+    ///   allocate output buffer
+    ///   free all buffers in case of error
+    for(i = 0; i < hardcfg.cpu_no; i++)
+    {
+        if((pcb(i).topa_virt = dma_alloc_coherent(NULL, IPT_BUF_SIZE, &dma_addr, GFP_KERNEL)))
+        {
+            pcb(i).topa_phys = (unsigned long long)dma_addr;
+        }
+        else
+        {
+            res = VTSS_ERR_NOMEMORY;
+        }
+        if((pcb(i).iptbuf_virt = dma_alloc_coherent(NULL, IPT_BUF_SIZE, &dma_addr, GFP_KERNEL)))
+        {
+            pcb(i).iptbuf_phys = (unsigned long long)dma_addr;
+        }
+        else
+        {
+            res = VTSS_ERR_NOMEMORY;
+        }
+    }
+    /// check for errors and free all buffers if any
+    return res;
+}
+#endif
+int vtss_ipt_init(void)
+{
+    int i, j;
+    int res = 0;
+    /// for each CPU:
+    ///   allocate ToPA page
+    ///   allocate output buffer
+    ///   free all buffers in case of error
+    for(i = 0; i < hardcfg.cpu_no; i++)
+    {
+        if((pcb(i).topa_virt = kmalloc((size_t)IPT_BUF_SIZE, GFP_KERNEL)))
+        {
+            pcb(i).topa_phys = (unsigned long long)virt_to_phys(pcb(i).topa_virt);
+        }
+        else
+        {
+            ERROR("Cannot allocate IPT ToPA buffer");
+            res = VTSS_ERR_NOMEMORY;
+        }
+        if((pcb(i).iptbuf_virt = kmalloc(IPT_BUF_SIZE*IPT_BUF_NO, GFP_KERNEL)))
+        {
+            for (j = 0; j < IPT_BUF_NO; j++)
+            {
+                pcb(i).iptbuf_phys[j] = (unsigned long long)virt_to_phys(pcb(i).iptbuf_virt+ j * IPT_BUF_SIZE);
+            }
+        }
+        else
+        {
+            ERROR("Cannot allocate IPT output buffer");
+            res = VTSS_ERR_NOMEMORY;
+        }
+    }
+    if (res == 0) INFO("IPT: enabled");
+    /// check for errors and free all buffers if any
+    return res;
+}
+
+static inline long long read_msr(int idx)
+{
+    long long val;
+    rdmsrl(idx, val);
+    return val;
+}
+static void vtss_init_ipt(void)
+{
+    long long tmp = read_msr(IPT_CONTROL_MSR);
+
+    wrmsrl(IPT_CONTROL_MSR, tmp & ~1L);
+
+    wrmsrl(IPT_CONTROL_MSR, 0);
+    wrmsrl(IPT_STATUS_MSR, 0);
+    wrmsrl(IPT_OUT_BASE_MSR, 0);
+    wrmsrl(IPT_OUT_MASK_MSR, 0);
+}
+
+#if 0
+void vtss_ipt_fini(void)
+{
+    int i;
+
+    for(i = 0; i < hardcfg.cpu_no; i++)
+    {
+        if(pcb(i).topa_virt)
+        {
+//            MmFreeNonCachedMemory(pcb(i).topa_virt, IPT_BUF_SIZE);
+            dma_free_coherent(NULL, IPT_BUF_SIZE, pcb(i).topa_virt, (dma_addr_t)pcb(i).topa_phys);
+            pcb(i).topa_virt = 0;
+            pcb(i).topa_phys = 0;
+        }
+        if(pcb(i).iptbuf_virt)
+        {
+//            MmFreeNonCachedMemory(pcb[i].iptbuf_virt, IPT_BUF_SIZE);
+            dma_free_coherent(NULL, IPT_BUF_SIZE, pcb(i).iptbuf_virt, (dma_addr_t)pcb(i).iptbuf_phys);
+            pcb(i).iptbuf_virt = 0;
+            pcb(i).iptbuf_phys = 0;
+        }
+    }
+}
+#endif
+
+static void vtss_ipt_disable_on_each_cpu(void* ctx)
+{
+    vtss_disable_ipt();
+}
+
+void vtss_ipt_fini(void)
+{
+    int i;
+    on_each_cpu(vtss_ipt_disable_on_each_cpu, NULL, SMP_CALL_FUNCTION_ARGS);
+    for(i = 0; i < hardcfg.cpu_no; i++)
+    {
+        if(pcb(i).topa_virt)
+        {
+            kfree(pcb(i).topa_virt);
+            pcb(i).topa_virt = 0;
+            pcb(i).topa_phys = 0;
+        }
+        if(pcb(i).iptbuf_virt)
+        {
+            kfree(pcb(i).iptbuf_virt);
+            pcb(i).iptbuf_virt = 0;
+            memset(pcb(i).iptbuf_phys, 0, sizeof(pcb(i).iptbuf_phys));
+        }
+    }
+}
+
+int vtss_has_ipt_overflowed(void)
+{
+    return 0;
+}
+
+extern int vtss_lbr_no;
+extern int vtss_lbr_msr_ctl;
+extern int vtss_lbr_msr_from;
+extern int vtss_lbr_msr_to;
+extern int vtss_lbr_msr_tos;
+extern int vtss_lbr_msr_sel;
+
+void vtss_enable_ipt(unsigned int mode, int is_kernel)
+{
+    int i;
+    vtss_pcb_t* pcbp = &pcb_cpu;
+
+    long long tmp = read_msr(IPT_CONTROL_MSR);
+    long long msr_val = 0x2500;
+
+    TRACE("enable IPT");
+    wrmsrl(IPT_CONTROL_MSR, tmp & ~1L);
+
+    /// disable LBRs and BTS
+    wrmsrl(VTSS_DEBUGCTL_MSR, 0);
+
+    if(hardcfg.family == 0x06 && (hardcfg.model == 0x4e /* SKL */ || hardcfg.model == 0x5e /* SKL */ || 
+                                  hardcfg.model == 0x55 /* SKX */ || hardcfg.model == 0x5c /* GLM */ || hardcfg.model == 0x5f /* DNV */ || 
+                                  hardcfg.model == 0x7a /* GLP */ || hardcfg.model == 0x42 /* CNL */ ||
+                                  hardcfg.model == 0x9e /* KBL */ || hardcfg.model == 0x8e /* KBL */))
+    {
+        /// form ToPA, and initialize status, base and mask pointers and control MSR
+        for(i = 0; i < IPT_BUF_NO; i++)
+        {
+            ((unsigned long long*)pcbp->topa_virt)[i] = pcbp->iptbuf_phys[i];
+        }
+        if(mode & vtss_iptmode_full)
+        {
+            ((unsigned long long*)pcbp->topa_virt)[IPT_BUF_NO / 4 * 3] |= 0x04;    /// INT
+            ((unsigned long long*)pcbp->topa_virt)[IPT_BUF_NO - 1] |= 0x10;    /// STOP
+        }
+        else
+        {
+            ((unsigned long long*)pcbp->topa_virt)[0] |= 0x10;    /// STOP
+        }
+        
+        ((unsigned long long*)pcbp->topa_virt)[i] = pcbp->topa_phys | 0x1;
+        if ((mode & vtss_iptmode_time)) msr_val |= 0x2; //PSB+TSC+CYC
+    }
+    else
+    {
+        /// form ToPA, and initialize status, base and mask pointers and control MSR
+        if (mode & vtss_iptmode_full)
+        {
+            ((unsigned long long*)pcbp->topa_virt)[0] = pcbp->iptbuf_phys[0] | 0x14;    /// STOP | INT ///Full-PT addition
+        }
+        else
+        {
+            ((unsigned long long*)pcbp->topa_virt)[0] = pcbp->iptbuf_phys[0] | 0x10;    /// STOP
+        }
+        
+        ((unsigned long long*)pcbp->topa_virt)[1] = pcbp->topa_phys | 0x1;
+    }
+    
+    wrmsrl(IPT_OUT_MASK_MSR, 0x7f);
+    wrmsrl(IPT_OUT_BASE_MSR, pcbp->topa_phys);
+    wrmsrl(IPT_STATUS_MSR, 0);
+    
+    msr_val |= (is_kernel) ? 0x4/*kernel mode*/ : 0x8/*user mode*/;
+    if (mode & vtss_iptmode_ring0) msr_val |= 0x4; // + kernel-mode
+
+    wrmsrl(IPT_CONTROL_MSR, msr_val);
+    wrmsrl(IPT_CONTROL_MSR, msr_val+1);
+
+}
+
+void vtss_disable_ipt(void)
+{
+    long long tmp = read_msr(IPT_CONTROL_MSR);
+
+    wrmsrl(IPT_CONTROL_MSR, tmp & ~1L);
+    /// clear control MSR
+    wrmsrl(IPT_CONTROL_MSR, 0);
+}
+
+void vtss_dump_ipt(struct vtss_transport_data* trnd, int tidx, int cpu, int is_safe)
+{
+    unsigned short size;
+
+    /// form IPT record and save the contents of the output buffer (from base to current mask pointer)
+
+    if((reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) &&
+        hardcfg.family == 0x06 && (hardcfg.model == 0x3d /* BDW */ || hardcfg.model == 0x47 /* BDW */ || hardcfg.model == 0x56 /* BDW-DE */ || hardcfg.model == 0x4f /* BDW-DE */ || 
+                                   hardcfg.model == 0x4e /* SKL */ || hardcfg.model == 0x5e /* SKL */ || hardcfg.model == 0x55 /* SKX */ || 
+                                   hardcfg.model == 0x9e /* KBL */ || hardcfg.model == 0x8e /* KBL */ ||
+                                   hardcfg.model == 0x5c /* GLM */ || hardcfg.model == 0x5f /* DNV */ ||
+                                   hardcfg.model == 0x7a /* GLP */ || hardcfg.model == 0x42 /* CNL */))
+    {
+#ifdef VTSS_USE_UEC
+        ipt_trace_record_t iptrec;
+#else
+        ipt_trace_record_t* iptrec;
+        void* entry;
+#endif
+        TRACE("IPT before reset: Control = %llX; Status = %llX; Base = %llX; Mask = %llX",
+                read_msr(IPT_CONTROL_MSR), read_msr(IPT_STATUS_MSR), read_msr(IPT_OUT_BASE_MSR), read_msr(IPT_OUT_MASK_MSR));
+        //vtss_disable_ipt();
+        size = (unsigned short)(((unsigned long long)read_msr(IPT_OUT_MASK_MSR) >> 32) & 0xffff);
+        size += (unsigned short)(((unsigned long long)read_msr(IPT_OUT_MASK_MSR) & 0xffffff80L) << 5);
+
+
+#if 0
+        if (reqcfg.ipt_cfg.mode & vtss_iptmode_full)  /// TODO: use vtss_iptmode_mark and configure appropriately via GUI
+        {
+            unsigned char* src = pcb_cpu.iptbuf_virt;
+            unsigned char* dst = src + IPT_BUF_SIZE * IPT_BUF_NO;
+            if(decode_pt(src, size, dst, IPT_BUF_SIZE * IPT_BUF_NO, vtss_iptmode_mark) == -1)
+            {
+                /// stop PT collection
+                reqcfg.trace_cfg.trace_flags  &= ~VTSS_CFGTRACE_IPT;
+            }
+        }
+#endif
+#ifdef VTSS_USE_UEC
+        /// [flagword][residx][cpuidx][tsc][systrace(bts)]
+        iptrec.flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_SYSTRACE;
+        iptrec.residx = (unsigned int)tidx;
+        preempt_disable();
+        iptrec.cpuidx = (unsigned int)smp_processor_id();
+        preempt_enable_no_resched();
+        iptrec.cputsc = vtss_time_cpu();
+        iptrec.type = UECSYSTRACE_IPT;
+        iptrec.size = size + 4;
+
+        if (vtss_transport_record_write(trnd, &iptrec, sizeof(ipt_trace_record_t), pcb_cpu.iptbuf_virt, size, is_safe)) {
+            ERROR("vtss_transport_record_write() FAIL");
+            vtss_init_ipt();
+            return;
+        }
+
+
+#else
+        iptrec = (ipt_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(ipt_trace_record_t) + size);
+        if (unlikely(!iptrec)) {
+            TRACE("vtss_transport_record_reserve() FAIL");
+            vtss_init_ipt();
+            return;
+        }
+        /// [flagword][residx][cpuidx][tsc][systrace(bts)]
+        iptrec->flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_SYSTRACE;
+        iptrec->residx = (unsigned int)tidx;
+        preempt_disable();
+        iptrec->cpuidx = (unsigned int)smp_processor_id();
+        preempt_enable_no_resched();
+        iptrec->cputsc = vtss_time_cpu();
+        iptrec->size = (unsigned short)(size + sizeof(iptrec->size) + sizeof(iptrec->type));
+        iptrec->type = UECSYSTRACE_IPT;
+        memcpy(++iptrec, pcb_cpu.iptbuf_virt, size);
+        if (vtss_transport_record_commit(trnd, entry, is_safe)){
+            ERROR("vtss_transport_record_write() FAIL");
+            vtss_init_ipt();
+            return;
+        }
+#endif
+        vtss_init_ipt();
+        TRACE("IPT after reset: Control = %llX; Status = %llX; Base = %llX; Mask = %llX",
+            read_msr(IPT_CONTROL_MSR), read_msr(IPT_STATUS_MSR), read_msr(IPT_OUT_BASE_MSR), read_msr(IPT_OUT_MASK_MSR));
+    }
+}
+
+///     ********************************************     ///
+/// --- Intel PT decoding part - derived from libipt --- ///
+///     ********************************************     ///
+
+/* A psb packet contains a unique 2-byte repeating pattern.
+* There are only two ways to fill up a 64bit work with such a pattern.
+*/
+const unsigned long long psb_unique_pattern[] = {
+        ((unsigned long long)pt_psb_lohi | (unsigned long long)pt_psb_lohi << 16 |
+        (unsigned long long)pt_psb_lohi << 32 | (unsigned long long)pt_psb_lohi << 48),
+        ((unsigned long long)pt_psb_hilo | (unsigned long long)pt_psb_hilo << 16 |
+        (unsigned long long)pt_psb_hilo << 32 | (unsigned long long)pt_psb_hilo << 48)
+};
+
+/* Synchronizes to the next PSB packet in case of packet decoding error */
+int sync_forward_to_psb(const unsigned char *begin, 
+                        const unsigned char *end, 
+                        const unsigned char *pos, 
+                        unsigned char **sync)
+{
+        unsigned char hi, lo;
+        const unsigned char *cur;
+        unsigned long long val;
+        int psb_pattern_size;
+        uintptr_t raw;
+
+        if (pos == *sync)
+        {
+            pos += ptps_psb;
+        }
+
+        if (!((begin <= pos) && (pos < end)))
+        {
+            return -pte_internal;
+        }
+
+        /* We search for a full 64bit word. It's OK to skip the cur one. */
+    psb_pattern_size = sizeof(*psb_unique_pattern);
+    raw = (uintptr_t)(pos + psb_pattern_size -1);
+    raw /= psb_pattern_size;
+    raw *= psb_pattern_size;
+    pos = (const unsigned char *)raw;
+
+    /* Search for the psb payload pattern in the buffer. */
+    for (;;)
+    {
+        cur = pos;
+        pos += sizeof(unsigned long long);
+
+        if (pos >= end)
+        {
+            return -pte_eos;
+        }
+
+        val = *(const unsigned long long *)cur;
+
+        if ((val != psb_unique_pattern[0]) && (val != psb_unique_pattern[1]))
+        {
+            continue;
+        }
+
+        /* We found a 64bit word's worth of psb payload pattern. */
+        /* Navigate to the end of the psb payload pattern.
+        *
+        * Beware that PSB is an extended opcode. We must not confuse the extend
+        * opcode of the following packet as belonging to the PSB.
+        */
+        cur = pos;
+
+        if (*pos != pt_psb_hi)
+        {
+            pos++;
+        }
+
+        for (; (pos + 1) < end; pos += 2)
+        {
+            hi = pos[0];
+            lo = pos[1];
+
+            if (hi != pt_psb_hi)
+            {
+                break;
+            }
+            if (lo != pt_psb_lo)
+            {
+                break;
+            }
+        }
+        /*
+        * We're right after the psb payload and within the buffer.
+        * Navigate to the expected beginning of the psb packet.
+        */
+        pos -= ptps_psb;
+
+        /* Check if we're still inside the buffer. */
+        if (pos < begin)
+        {
+            pos = cur;
+            continue;
+        }
+        /* Check that this is indeed a psb packet we're at. */
+        if (pos[0] != pt_opc_psb || pos[1] != pt_ext_psb)
+        {
+            pos = cur;
+            continue;
+        }
+
+        *sync = (unsigned char*) pos;
+        break;
+    }
+
+    return 0;
+}
+
+/* decodes PT packets given in 'buffer' of length 'size'*/
+int decode_pt(unsigned char* buffer, size_t size, unsigned char* dst, size_t dst_size, uint32_t mode)
+{
+    unsigned char *end, *pos, *start, *sync;
+    unsigned char ipc, opc, ext, ext2;
+    unsigned char cyc, shl;
+    unsigned long long value;
+    unsigned long long signbit, mask, bits;
+    int ipsize = 0;
+    int curr_dst_size = 0;
+
+    struct pt_last_ip last_ip;
+
+    int state = 0;
+
+    memset(&last_ip, 0, sizeof(last_ip));
+
+    start = buffer;
+    end = buffer + size;
+    sync = buffer;
+
+    for (pos = buffer; pos < end;)
+    {
+        start = pos;
+
+        opc = *pos;
+
+        switch (opc)
+        {
+        case pt_opc_pad:
+            //pt_decode_pad;
+            pos = pos + ptps_pad;
+            break;
+
+        case pt_opc_mode:
+            //pt_decode_mode;
+            pos = pos + ptps_mode;
+            break;
+
+        case pt_opc_tsc:
+            //pt_decode_tsc;
+            pos = pos + ptps_tsc;
+            break;
+
+        case pt_opc_mtc:
+            //pt_decode_mtc;
+            pos = pos + ptps_mtc;
+            break;
+
+        case pt_opc_ext:
+
+            pos++;
+
+            if (pos == end)
+            {
+                return curr_dst_size;
+            }
+            ext = *pos;
+
+            switch(ext)
+            {
+                default:
+                    //pt_decode_unknown; may need to PSB sync
+                    if (!sync_forward_to_psb(buffer, end, pos, &sync))
+                    {
+                        pos = sync;
+                        last_ip.ip = 0ull;
+                        last_ip.have_ip = 0;
+                        last_ip.suppressed = 0;
+                    }
+                    else
+                    {
+                        return curr_dst_size;
+                    }
+                    break;
+
+                case pt_ext_psb:
+                    //pt_decode_psb;
+                    pos = start + ptps_psb;
+                    break;
+
+                case pt_ext_ovf:
+                    //pt_decode_ovf;
+                    pos = start + ptps_ovf;
+                    break;
+
+                case pt_ext_tnt_64:
+                    //pt_decode_tnt_64;
+                    pos = start + ptps_tnt_64;
+                    break;
+
+                case pt_ext_psbend:
+                    //pt_decode_psbend;
+                    pos = start + ptps_psbend;
+                    break;
+
+                case pt_ext_cbr:
+                    //pt_decode_cbr;
+                    pos = start + ptps_cbr;
+                    break;
+
+                case pt_ext_pip:
+                    //pt_decode_pip;
+                    pos = start + ptps_pip;
+                    break;
+
+                case pt_ext_tma:
+                    //pt_decode_tma;
+                    pos = start + ptps_tma;
+                    break;
+
+                case pt_ext_stop:
+                    //pt_decode_stop;
+                    pos = start + ptps_stop;
+                    break;
+
+                case pt_ext_vmcs:
+                    //pt_decode_vmcs;
+                    pos = start + ptps_vmcs;
+                    break;
+
+                case pt_ext_ext2:
+
+                    pos++;
+                    if (pos == end)
+                    {
+                        return curr_dst_size;
+                    }
+                    ext2 = *pos;
+
+                    switch (ext2)
+                    {
+                        /// TODO: save MNT packet
+                        case pt_ext2_mnt:
+                            //pt_decode_mnt;
+                            pos = start + ptps_mnt;
+
+                            if(mode & vtss_iptmode_ucode)
+                            {
+                                if(dst_size - curr_dst_size >= 8)
+                                {
+                                    *(unsigned long long*)&dst[curr_dst_size] = *(unsigned long long*)&start[3];
+                                    curr_dst_size += 8;
+                                }
+                                else
+                                {
+                                    /// set the last element to 0 to indicate an overflow
+                                    *(unsigned long long*)&dst[curr_dst_size - 8] = 0;
+
+                                    return curr_dst_size;
+                                }
+                            }
+                            break;
+
+                        default:
+                            //pt_decode_unknown; may need to PSB sync
+                            if (!sync_forward_to_psb(buffer, end, pos, &sync))
+                            {
+                                pos = sync;
+                                last_ip.ip = 0ull;
+                                last_ip.have_ip = 0;
+                                last_ip.suppressed = 0;
+                            }
+                            else
+                            {
+                                return curr_dst_size;
+                            }
+                            break;
+                    }
+                    break;
+            }
+            break;
+
+        default:
+            /* Check opcodes that require masking. */
+            if ((opc & pt_opm_tnt_8) == pt_opc_tnt_8)
+            {
+                /// reset stop-mark-tracking state if not TIP
+                state = 0;
+
+                pos = pos + ptps_tnt_8;
+                break;
+            }
+
+            if ((opc & pt_opm_cyc) == pt_opc_cyc)
+            {
+                //pt_decode_cyc;
+                /* The first byte contains the opcode and part of the payload.
+                * We already checked that this first byte is within bounds.
+                */
+                cyc = *pos++;
+                ext = cyc & pt_opm_cyc_ext;
+                cyc >>= pt_opm_cyc_shr;
+                value = cyc;
+                shl = (8 - pt_opm_cyc_shr);
+
+                while (ext)
+                {
+                    if (pos >= end)
+                    {
+                        return curr_dst_size;
+                    }
+                    bits = *pos++;
+                    ext = (unsigned char)(bits & pt_opm_cycx_ext);
+
+                    bits >>= pt_opm_cycx_shr;
+                    bits <<= shl;
+
+                    shl += (8 - pt_opm_cycx_shr);
+
+                    if (sizeof(value) * 8 < shl)
+                    {
+                        break; // -pte_bad_packet, may need to PSB sync
+                    }
+                    value |= bits;
+                }
+                break;
+            }
+
+            if ((opc & pt_opm_tip) == pt_opc_tip ||
+                (opc & pt_opm_fup) == pt_opc_fup ||
+                (opc & pt_opm_tip) == pt_opc_tip_pge ||
+                (opc & pt_opm_tip) == pt_opc_tip_pgd)
+            {
+                //pt_decode_tip; pt_decode_fup; pt_decode_tip_pge; pt_decode_tip_pgd 
+
+                ipc = (*pos++ >> pt_opm_ipc_shr) & pt_opm_ipc_shr_mask;
+
+                switch ((enum pt_ip_compression)ipc)
+                {
+                    case pt_ipc_suppressed:
+
+                        ipsize = 0;
+                        break;
+
+                    case pt_ipc_update_16:
+
+                        ipsize = 2;
+                        break;
+
+                    case pt_ipc_update_32:
+
+                        ipsize = 4;
+                        break;
+
+                    case pt_ipc_update_48:
+                    case pt_ipc_sext_48:
+
+                        ipsize = 6;
+                        break;
+
+                    case pt_ipc_full:
+
+                        ipsize = 8;
+                        break;
+
+                    default:
+                        // -pte_bad_packet; may need PSB sync
+                        break;
+                }
+
+                if (pos + ipsize > end)
+                {
+                    return curr_dst_size;
+                }
+                value = 0;
+
+                if (ipsize)
+                {
+                    int idx;
+
+                    for (idx = 0; idx < ipsize; ++idx)
+                    {
+                        unsigned long long byte = *pos++;
+                        byte <<= (idx * 8);
+                        value |= byte;
+                    }
+                }
+
+                switch ((enum pt_ip_compression)ipc)
+                {
+                    case pt_ipc_suppressed:
+
+                        last_ip.suppressed = 1;
+                        break;
+
+                    case pt_ipc_sext_48:
+
+                        signbit = 1ull << (48 - 1);
+                        mask = ~0ull << 48;
+
+                        last_ip.ip = value & signbit ? value | mask : value & ~mask; //sext(ip, 48);
+                        last_ip.have_ip = 1;
+                        last_ip.suppressed = 0;
+                        break;
+
+                    case pt_ipc_update_16:
+
+                        last_ip.ip = (last_ip.ip & ~0xffffull) | (value & 0xffffull);
+                        last_ip.have_ip = 1;
+                        last_ip.suppressed = 0;
+                        break;
+
+                    case pt_ipc_update_32:
+
+                        last_ip.ip = (last_ip.ip & ~0xffffffffull) | (value & 0xffffffffull);
+                        last_ip.have_ip = 1;
+                        last_ip.suppressed = 0;
+                        break;
+
+                    case pt_ipc_update_48:
+
+                        last_ip.ip = (last_ip.ip & ~0xffffffffffffull) | (value & 0xffffffffffffull);
+                        last_ip.have_ip = 1;
+                        last_ip.suppressed = 0;
+                        break;
+
+                    case pt_ipc_full:
+
+                        last_ip.ip = value;
+                        last_ip.have_ip = 1;
+                        last_ip.suppressed = 0;
+                        break;
+                }
+
+                /// TODO: copy IP to an output buffer
+                /* Prints only TIP packets */
+                if ((opc & pt_opm_tip) == pt_opc_tip)
+                {
+                    if(mode & vtss_iptmode_tips)
+                    {
+                        if(dst_size - curr_dst_size >= 8)
+                        {
+                            *(unsigned long long*)&dst[curr_dst_size] = last_ip.ip;
+                            curr_dst_size += 8;
+                        }
+                        else
+                        {
+                            /// set the last element to 0 to indicate an overflow
+                            *(unsigned long long*)&dst[curr_dst_size - 8] = 0;
+
+                            return curr_dst_size;
+                        }
+                    }
+                    /// locate a stop-mark to stop PT collection
+                    if(mode & vtss_iptmode_mark)
+                    {
+                        if((last_ip.ip & 0x0f) - 1 == state)
+                        {
+                            if(state == 2)
+                            {
+                                /// stop-mark indication
+                                return -1;
+                            }
+                            state++;
+                        }
+                        else
+                        {
+                            state = 0;
+                        }
+                    }
+                }
+                /// reset stop-mark-tracking state if not TIP
+                else
+                {
+                    state = 0;
+                }
+                break;
+            }
+
+            //pt_decode_unknown; may need PSB sync or just keep iterating
+            if (!sync_forward_to_psb(buffer, end, pos, &sync))
+            {
+                pos = sync;
+                last_ip.ip = 0ull;
+                last_ip.have_ip = 0;
+                last_ip.suppressed = 0;
+            }
+            else
+            {
+                return curr_dst_size;
+            }
+            break;
+        }
+    }
+
+    /* case 1: pos == end; decoding the buffer ends normally
+    *  case 2: pos == end; decoding the buffer has ended in the middle of opcode
+               - solved by returning immediately after detection
+    *  case 3: pos > end; decoding the buffer has ended in the middle of a packet
+    */
+
+    /// but we do not differentiate between any of those cases for now...
+
+    if (pos == end)
+    {
+        return curr_dst_size;
+    }
+    if (pos > end)
+    {
+        return curr_dst_size;
+    }
+
+    return curr_dst_size;
+}
+
diff --git a/drivers/misc/intel/sepdk/vtsspp/lbr.c b/drivers/misc/intel/sepdk/vtsspp/lbr.c
new file mode 100644
index 000000000000..2740d49f7b68
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/lbr.c
@@ -0,0 +1,385 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "globals.h"
+#include "transport.h"
+#include "unwind.h"
+#include "lbr.h"
+#include "record.h"
+#include "time.h"
+
+#define DEBUGCTL_MSR        0x01d9
+#define LBR_ENABLE_MASK_P4  0x0021
+#define LBR_ENABLE_MASK_P6  0x0201  ///0x0001
+#define LBR_ENABLE_MASK_HSW 0x0201
+#define LBR_SELECT_MASK_HSW 0x03c5
+
+int vtss_lbr_no       = 0;
+int vtss_lbr_msr_ctl  = 0;
+int vtss_lbr_msr_from = 0;
+int vtss_lbr_msr_to   = 0;
+int vtss_lbr_msr_tos  = 0;
+int vtss_lbr_msr_sel  = 0;
+
+/// clear stack trace record
+static long long read_msr(int idx)
+{
+    long long val;
+    rdmsrl(idx, val);
+    return val;
+}
+
+int vtss_stack_record_lbr(struct vtss_transport_data* trnd, stack_control_t* stk, pid_t tid, int cpu, int is_safe)
+{
+    int rc = 0;
+    int i, j, k;
+    int lbridx;
+
+    int sign;
+    int prefix;
+    size_t value;
+    size_t offset;
+
+    size_t ip = 0;
+
+    char* compressed = stk->compressed;
+
+    i = 0;
+    /// loop through all LBRs, form a 'clear' stack record, and save it
+    if(vtss_lbr_no && !vtss_lbr_msr_ctl)
+    {
+        lbridx = read_msr(vtss_lbr_msr_tos) & (vtss_lbr_no - 1);
+
+        for(i = 0, k = 0; k < vtss_lbr_no && i < (stk->size >> 1); k++)
+        {
+            value = (size_t)((read_msr(vtss_lbr_msr_from + lbridx) << 16) >> 16);
+
+            if(!value)
+            {
+                break;
+            }
+
+            offset = ip;
+            ip = value;
+            prefix = 0;
+            value -= offset;
+
+            sign = (value & (((size_t)1) << ((sizeof(size_t) << 3) - 1))) ? 0xff : 0;
+
+            for(j = sizeof(size_t) - 1; j >= 0; j--)
+            {
+                if(((value >> (j << 3)) & 0xff) != sign)
+                {
+                    break;
+                }
+            }
+            prefix |= sign ? 0x40 : 0;
+            prefix |= j + 1;
+            compressed[i++] = (unsigned char)prefix;
+
+            for(; j >= 0; j--)
+            {
+                compressed[i++] = (unsigned char)(value & 0xff);
+                value >>= 8;
+            }
+
+            lbridx = lbridx ? lbridx - 1 : vtss_lbr_no - 1;
+        }
+        /// save the resuling stack record
+        if(i == VTSS_DYNSIZE_SCRATCH)
+        {
+            /// stack buffer overflowed
+            strcat(stk->dbgmsg, "No room for LBR stacks");
+            vtss_record_debug_info(trnd, stk->dbgmsg, 0);
+            return -EFAULT;
+        }
+        else
+        {
+
+#ifdef VTSS_USE_UEC
+
+            clrstk_trace_record_t stkrec;
+
+            /// save current alt. stack in UEC: [flagword - 4b][residx][cpuidx - 4b][tsc - 8b]
+            ///                                 ...[sampled address - 8b][systrace{sts}]
+            ///                                                          [length - 2b][type - 2b]...
+            stkrec.flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_EXECADDR | UECL1_SYSTRACE;
+            stkrec.residx = tid;
+            stkrec.cpuidx = cpu;
+            stkrec.cputsc = vtss_time_cpu();
+            stkrec.execaddr = (unsigned long long)stk->user_ip.szt;
+
+            stkrec.size = sizeof(stkrec.size) + sizeof(stkrec.type) + sizeof(stkrec.merge_node) + (unsigned short)i;
+            stkrec.type = (sizeof(void*) == 8) ? UECSYSTRACE_CLEAR_STACK64 : UECSYSTRACE_CLEAR_STACK32;
+            stkrec.merge_node = 0xffffffff;
+
+            if (vtss_transport_record_write(trnd, &stkrec, sizeof(stkrec), compressed, i, is_safe))
+            {
+                TRACE("STACK_record_write() FAIL");
+                rc = -EFAULT;
+            }
+
+#else  // VTSS_USE_UEC 
+
+            void* entry;
+            clrstk_trace_record_t* stkrec = (clrstk_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(clrstk_trace_record_t) + i);
+
+            if(likely(stkrec))
+            {
+                /// save current alt. stack in UEC: [flagword - 4b][residx][cpuidx - 4b][tsc - 8b]
+                ///                                 ...[sampled address - 8b][systrace{sts}]
+                ///                                                          [length - 2b][type - 2b]...
+                stkrec->flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_EXECADDR | UECL1_SYSTRACE;
+                stkrec->residx   = tid;
+                stkrec->cpuidx   = cpu;
+                stkrec->cputsc   = vtss_time_cpu();
+                stkrec->execaddr = (unsigned long long)stk->user_ip.szt;
+
+                stkrec->size = sizeof(stkrec->size) + sizeof(stkrec->type) + sizeof(stkrec->merge_node) + (unsigned short)i;
+                stkrec->type = (sizeof(void*) == 8) ? UECSYSTRACE_CLEAR_STACK64 : UECSYSTRACE_CLEAR_STACK32;
+                stkrec->merge_node = 0xffffffff;
+                memcpy((char*)stkrec + sizeof(clrstk_trace_record_t), stk->compressed, i);
+                rc = vtss_transport_record_commit(trnd, entry, is_safe);
+            }
+            else
+            {
+                TRACE("STACK_record_write() FAIL");
+                rc = -EFAULT;
+            }
+
+#endif //  VTSS_USE_UEC
+
+        }
+    }
+    return rc;
+}
+
+void* vtss_lbr_correct_ip(void* ip)
+{
+/* TODO: Temporary turn off for investigation */
+#if 0
+    int lbr_idx;
+    long long msr_val; /* Should be signed for ((val << 1) >> 1) */
+
+    if (vtss_lbr_no && !vtss_lbr_msr_ctl && (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_LASTBR)) {
+        rdmsrl(vtss_lbr_msr_tos, msr_val);
+        lbr_idx = msr_val ? (int)msr_val - 1 : vtss_lbr_no - 1;
+        rdmsrl(vtss_lbr_msr_to + lbr_idx, msr_val);
+        TRACE("ip=0x%p, to=0x%llX", ip, msr_val);
+        if ((size_t)ip == (size_t)msr_val) {
+            rdmsrl(vtss_lbr_msr_from + lbr_idx, msr_val);
+            TRACE("from=0x%llX", msr_val);
+            return (void*)(size_t)((msr_val << 1) >> 1);
+        } else
+            return (void*)((char*)ip - 1);
+    }
+#endif
+    return ip;
+}
+
+/* start LBR collection on the processor */
+void vtss_lbr_enable(lbr_control_t* lbrctl)
+{
+    unsigned long long msr_val;
+
+    if ((hardcfg.family == 0x06 || hardcfg.family == 0x0f) && vtss_lbr_no) {
+        if (vtss_lbr_msr_sel) {
+            wrmsrl(vtss_lbr_msr_sel, 0ULL);
+        }
+        rdmsrl(DEBUGCTL_MSR, msr_val);
+
+        msr_val |= (hardcfg.family == 0x0f) ? LBR_ENABLE_MASK_P4 : 
+                   (hardcfg.model == 0x3c || hardcfg.model == 0x45) ? LBR_ENABLE_MASK_HSW : LBR_ENABLE_MASK_P6;
+
+        wrmsrl(DEBUGCTL_MSR, 0);
+
+        if(hardcfg.family == 0x06 && (hardcfg.model == 0x3c /* HSW */ || hardcfg.model == 0x45 /* HSW */|| 
+                                  hardcfg.model == 0x3d /* BDW */ || hardcfg.model == 0x47 /* BDW */ || hardcfg.model == 0x56 /* BDW-DE */ || hardcfg.model == 0x4f /* BDW-DE */ || 
+                                  hardcfg.model == 0x4e /* SKL */ || hardcfg.model == 0x5e /* SKL */ || hardcfg.model == 0x55 /* SKX */ || 
+                                  hardcfg.model == 0x9e /* KBL */ || hardcfg.model == 0x8e /* KBL */))
+        {
+            /// restore LBR stack
+            int i, j;
+
+            for(i = 0, j = 0; i < vtss_lbr_no; i++, j += 2)
+            {
+                /// TODO: temporarily disabled until arch resolves LBR restoration
+                if(hardcfg.model == 0x4e /* SKL */ || hardcfg.model == 0x5e /* SKL */ || hardcfg.model == 0x55 /* SKX */ || 
+                   hardcfg.model == 0x9e /* KBL */ || hardcfg.model == 0x8e /* KBL */)
+                {
+                    wrmsrl(vtss_lbr_msr_from + i, lbrctl->lbrstk[j + 0]);
+                    wrmsrl(vtss_lbr_msr_to + i, lbrctl->lbrstk[j + 1]);
+                }
+            }
+            wrmsrl(vtss_lbr_msr_tos, lbrctl->lbrtos);
+
+            /// enable LBR call stack
+            wrmsrl(vtss_lbr_msr_sel, LBR_SELECT_MASK_HSW);
+        }
+        wrmsrl(DEBUGCTL_MSR, msr_val);
+    }
+}
+
+/* stop LBR collection on the processor */
+void vtss_lbr_disable(void)
+{
+    unsigned long long msr_val;
+
+    if (hardcfg.family == 0x06 || hardcfg.family == 0x0f) {
+        rdmsrl(DEBUGCTL_MSR, msr_val);
+        msr_val &= (hardcfg.family == 0x0f) ? ~LBR_ENABLE_MASK_P4 : (hardcfg.model == 0x3c || hardcfg.model == 0x45) ? ~LBR_ENABLE_MASK_HSW : ~LBR_ENABLE_MASK_P6;
+        wrmsrl(DEBUGCTL_MSR, msr_val);
+    }
+}
+
+/* stop LBR collection on the processor and save LBR stack */
+void vtss_lbr_disable_save(lbr_control_t* lbrctl)
+{
+    unsigned long long msr_val;
+
+    if (hardcfg.family == 0x06 || hardcfg.family == 0x0f) {
+        rdmsrl(DEBUGCTL_MSR, msr_val);
+        msr_val &= (hardcfg.family == 0x0f) ? ~LBR_ENABLE_MASK_P4 : (hardcfg.model == 0x3c || hardcfg.model == 0x45) ? ~LBR_ENABLE_MASK_HSW : ~LBR_ENABLE_MASK_P6;
+        wrmsrl(DEBUGCTL_MSR, msr_val);
+    }
+    /// save LBR stack
+    if(hardcfg.family == 0x06 && (hardcfg.model == 0x3c /* HSW */ || hardcfg.model == 0x45 /* HSW */|| 
+                                  hardcfg.model == 0x3d /* BDW */ || hardcfg.model == 0x47 /* BDW */ || hardcfg.model == 0x56 /* BDW-DE */ || hardcfg.model == 0x4f /* BDW-DE */ || 
+                                  hardcfg.model == 0x4e /* SKL */ || hardcfg.model == 0x5e /* SKL */ || hardcfg.model == 0x55 /* SKX */ ||
+                                  hardcfg.model == 0x9e /* KBL */ || hardcfg.model == 0x8e /* KBL */))
+    {
+        int i, j;
+
+        for(i = 0, j = 0; i < vtss_lbr_no; i++, j += 2)
+        {
+            lbrctl->lbrstk[j + 0] = read_msr(vtss_lbr_msr_from + i);
+            lbrctl->lbrstk[j + 1] = read_msr(vtss_lbr_msr_to + i);
+        }
+
+        lbrctl->lbrtos = read_msr(vtss_lbr_msr_tos);
+    }
+}
+
+/* initialize the architectural LBR parameters */
+int vtss_lbr_init(void)
+{
+    /* zero the LBR configuration by default */
+    vtss_lbr_no       = 0;
+    vtss_lbr_msr_ctl  = 0;
+    vtss_lbr_msr_from = 0;
+    vtss_lbr_msr_to   = 0;
+    vtss_lbr_msr_tos  = 0;
+    vtss_lbr_msr_sel  = 0;
+
+    /* test the current architecture */
+    if (hardcfg.family == 0x06) {
+        switch (hardcfg.model) {
+            /* SKL/SKX/KBL */
+        case 0x4e:
+        case 0x5e:
+        case 0x55:
+        case 0x9e:
+        case 0x8e:
+            vtss_lbr_no       = 32;
+            vtss_lbr_msr_from = 0x0680;
+            vtss_lbr_msr_to   = 0x06c0;
+            vtss_lbr_msr_tos  = 0x01c9;
+            vtss_lbr_msr_sel  = 0x01c8;
+            break;
+        /* NHM/SNB/IVB/HSW/BDW */
+        case 0x1a:
+        case 0x1e:
+        case 0x1f:
+        case 0x2e:
+        case 0x25:
+        case 0x2c:
+        case 0x2a:
+        case 0x2d:
+        case 0x3a:
+        case 0x3c:
+        case 0x45:
+            vtss_lbr_no       = 16;
+            vtss_lbr_msr_from = 0x0680;
+            vtss_lbr_msr_to   = 0x06c0;
+            vtss_lbr_msr_tos  = 0x01c9;
+            vtss_lbr_msr_sel  = 0x01c8;
+            break;
+        /* Atoms */
+        case 0x1c:
+        case 0x35:
+        case 0x36:
+            vtss_lbr_no       = 8;
+            vtss_lbr_msr_from = 0x40;
+            vtss_lbr_msr_to   = 0x60;
+            vtss_lbr_msr_tos  = 0x01c9;
+            break;
+        /* Core2s */
+        case 0x1d:
+        case 0x17:
+        case 0x0f:
+            vtss_lbr_no       = 4;
+            vtss_lbr_msr_from = 0x40;
+            vtss_lbr_msr_to   = 0x60;
+            vtss_lbr_msr_tos  = 0x01c9;
+            break;
+        default:
+            if (hardcfg.model >= 0x02 && hardcfg.model < 0x0f) {
+                vtss_lbr_no       = 8;
+                vtss_lbr_msr_ctl  = 0x40;
+                vtss_lbr_msr_tos  = 0x01c9;
+            }
+            break;
+        }
+    } else if (hardcfg.family == 0x0f) {
+        if (hardcfg.model >= 0x03) {
+            vtss_lbr_no       = 16;
+            vtss_lbr_msr_from = 0x0680;
+            vtss_lbr_msr_to   = 0x06c0;
+            vtss_lbr_msr_tos  = 0x01da;
+        } else {
+            vtss_lbr_no       = 4;
+            vtss_lbr_msr_ctl  = 0x01db;
+            vtss_lbr_msr_tos  = 0x01da;
+        }
+    }
+    TRACE("no=%d, ctl=0x%X, from=0x%X, to=0x%X, tos=0x%X",
+          vtss_lbr_no, vtss_lbr_msr_ctl, vtss_lbr_msr_from, vtss_lbr_msr_to, vtss_lbr_msr_tos);
+    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_LASTBR)
+        INFO("LBR: stack size: %d", vtss_lbr_no);
+    return 0;
+}
+
+static void vtss_lbr_on_each_cpu_func(void* ctx)
+{
+    vtss_lbr_disable();
+}
+
+void vtss_lbr_fini(void)
+{
+    on_each_cpu(vtss_lbr_on_each_cpu_func, NULL, SMP_CALL_FUNCTION_ARGS);
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/memory_pool.c b/drivers/misc/intel/sepdk/vtsspp/memory_pool.c
new file mode 100644
index 000000000000..2c64da8e4618
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/memory_pool.c
@@ -0,0 +1,746 @@
+/*
+  Copyright (C) 2018 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+
+#include "vtss_config.h"
+#include "globals.h"
+#include "memory_pool.h"
+
+#include <linux/delay.h> // for msleep_interruptible()
+#include <linux/nmi.h>   // for touch_nmi_watchdog()
+#include <linux/slab.h>
+#include <linux/hardirq.h> //in_atomic for kernel 2.6.32
+
+#define DEBUG_MP TRACE
+
+// when number of cores too big we need many 
+// allocated buffers to access from different 
+// cores whithout delay
+#define VTSS_ALLOC_ORDER_MIN 4 
+#define VTSS_ALLOC_ORDER_ATOMIC 8 
+#define VTSS_ALLOC_ORDER_MAX 10 
+
+#define VTSS_NR_CHUNKS_MIN 0x8
+#define VTSS_NR_CHUNKS_MAX 0x100
+
+#define VTSS_WAIT_TIME 100
+
+//block of memory reserved in preallocated pages
+struct vtss_memory_block
+{
+    struct vtss_memory_block* prev;
+    struct vtss_memory_block* next;
+    size_t size;
+
+    atomic_t is_free;
+    char memblock[0];            //  placeholder memory blocks, should be the last in the structure
+};
+
+// block of memory header
+struct vtss_memchunk
+{
+    struct list_head    list;
+
+    struct vtss_memory_block* head; // list of allocated blocks
+    struct vtss_memory_block* tail; // list of allocated blocks
+
+    atomic_t busy;
+
+    size_t size;                   // allocated size - sizeof(vtss_memchunk)
+    atomic_t free_space;
+    
+    gfp_t flags;
+    
+    char memblocks[0];            //  placeholder memory blocks, should be the last in the structure
+};
+
+static LIST_HEAD(g_memchunk_list);
+
+static atomic_t g_vtss_memchunk_list_write = ATOMIC_INIT(0);
+static atomic_t g_vtss_memchunk_list_read = ATOMIC_INIT(0);
+
+static atomic_t g_vtss_chunk_list_length = ATOMIC_INIT(0);
+
+static inline void vtss_memchunk_list_write_lock(void)
+{
+    int i = 0;
+    while (atomic_cmpxchg(&g_vtss_memchunk_list_write, 0, 1))
+    {
+        touch_nmi_watchdog();
+        if (i == VTSS_WAIT_TIME) DEBUG_MP("Cannot lock for write");
+        i++;
+    }
+    i = 0;
+    while (atomic_read(&g_vtss_memchunk_list_read))
+    {
+        touch_nmi_watchdog();
+        if (i == VTSS_WAIT_TIME) DEBUG_MP("Read lock is not free");
+        i++;
+    }
+}
+
+static inline void vtss_memchunk_list_write_unlock(void)
+{
+    atomic_set(&g_vtss_memchunk_list_write,0);
+}
+
+static inline void vtss_memchunk_list_read_lock(void)
+{
+    int i = 0;
+    do
+    {
+        while (atomic_read(&g_vtss_memchunk_list_write));
+        atomic_inc(&g_vtss_memchunk_list_read);
+        if (atomic_read(&g_vtss_memchunk_list_write)) atomic_dec(&g_vtss_memchunk_list_read);
+        else break;
+        touch_nmi_watchdog();
+        if (i == VTSS_WAIT_TIME) DEBUG_MP("Cannot lock for read");
+        i++;
+    }
+    while (1);
+}
+
+static inline void vtss_memchunk_list_read_unlock(void)
+{
+    atomic_dec(&g_vtss_memchunk_list_read);
+}
+
+static atomic_t g_vtss_kernel_task_in_progress = ATOMIC_INIT(0);
+static atomic_t g_vtss_mempool_init = ATOMIC_INIT(0);
+
+static void vtss_init_memchunk(struct vtss_memchunk* chunk)
+{
+    memset(chunk, 0, sizeof(struct vtss_memchunk));
+    atomic_set(&chunk->busy,0);
+    chunk->flags = GFP_NOWAIT;
+}
+
+void vtss_memory_pool_clear(void)
+{
+    struct list_head* p = NULL;
+    struct list_head* tmp = NULL;
+
+    list_for_each_safe(p, tmp, &g_memchunk_list)
+    {
+        struct vtss_memchunk* chunk = list_entry(p, struct vtss_memchunk, list);
+        struct vtss_memory_block* temp = NULL;
+        touch_nmi_watchdog();
+        if (chunk == NULL)
+        {
+             ERROR("Chunk in list is NULL");
+             continue;
+        }
+        if (atomic_read(&chunk->busy) != 0)
+        {
+            ERROR("Chunk is busy");
+            continue;
+        }
+        if (atomic_read(&chunk->free_space) != chunk->size)
+        {
+           ERROR("Memoty leaks detected, free_space = %zx, size = %zx", (size_t)atomic_read(&chunk->free_space), chunk->size);
+           atomic_set(&chunk->free_space,chunk->size);
+        }
+#if 0
+        temp = chunk->head;
+        while (temp && atomic_read(&temp->is_free) != 1)
+        {
+            ERROR("Error in memory pool logic detected, temp = %p", temp);
+            temp = temp->next;
+        }
+#endif
+        chunk->head = NULL;
+        chunk->tail = NULL;
+
+    }
+    return;
+}
+
+struct vtss_memchunk* vtss_create_memchunk(gfp_t flags, unsigned int size)
+{
+    struct vtss_memchunk* chunk = NULL;
+    unsigned int order = get_order(size) >= VTSS_ALLOC_ORDER_MIN ? get_order(size) : VTSS_ALLOC_ORDER_MIN;
+    int i = 0;
+
+    if (atomic_read(&g_vtss_chunk_list_length) >= VTSS_NR_CHUNKS_MAX)
+    {
+        DEBUG_MP("Max number of memory chunks is reached");
+        return NULL;
+    }
+    do
+    {
+        chunk = (struct vtss_memchunk*)__get_free_pages(flags | (flags == GFP_NOWAIT ? __GFP_NORETRY : 0) | __GFP_NOWARN, order);
+        if (!chunk)
+        {
+            ERROR("cannot allocate order = %d", order);
+        }
+    }
+    while ((!chunk) && (--order) >= VTSS_ALLOC_ORDER_MIN);
+
+    DEBUG_MP("chunk = %p  allocated, order = %x", chunk, order);
+
+    if (chunk == NULL)
+    {
+       ERROR("Not enough memory to create memchunk");
+       return NULL;
+    }
+    atomic_inc(&g_vtss_chunk_list_length);
+    vtss_init_memchunk(chunk);
+    chunk->flags = flags;
+    chunk->size = (PAGE_SIZE<<order) - sizeof(struct vtss_memchunk);
+    atomic_set(&chunk->free_space,chunk->size);
+    DEBUG_MP("Chunk created: order = 0x%x, (PAGE_SIZE<<order) = %lx, sizeof(vtss_memchunk)= 0x%llx, sizeof(*chunk)=%llx, &chunk->memblocks = %llx, chunk->memblocks = %llx,chunk->size = %zu",
+              order, (PAGE_SIZE<<order), (unsigned long long)sizeof(struct vtss_memchunk), (unsigned long long)sizeof(*chunk), (unsigned long long)&chunk->memblocks, (unsigned long long)chunk->memblocks,chunk->size);
+    i = 0;
+    vtss_memchunk_list_write_lock();
+    list_add(&chunk->list, &g_memchunk_list);
+    vtss_memchunk_list_write_unlock();
+    return chunk;
+}
+
+struct vtss_memchunk_alloc_data
+{
+    gfp_t flags;
+    size_t size;
+};
+
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_create_memchunk_work(struct work_struct *work)
+#else
+static void vtss_create_memchunk_work(void *work)
+#endif
+{
+    struct vtss_work* my_work = (struct vtss_work*)work;
+    struct vtss_memchunk_alloc_data* alloc_data = NULL;
+    struct vtss_memchunk* chunk = NULL;
+    DEBUG_MP("Creating new chunc async.");
+    if (!my_work)
+    {
+        ERROR("Empty work");
+        return;
+    }
+    if (!atomic_read(&g_vtss_mempool_init))
+    {
+        vtss_kfree(my_work);
+        ERROR("mempoolnot init");
+        atomic_dec(&g_vtss_kernel_task_in_progress);
+        return;
+    }
+    alloc_data = (struct vtss_memchunk_alloc_data*)my_work->data;
+    if (!alloc_data)
+    {
+        ERROR("Unknown parameters");
+        vtss_kfree(my_work);
+        atomic_dec(&g_vtss_kernel_task_in_progress);
+        return;
+    }
+    chunk = vtss_create_memchunk(alloc_data->flags, alloc_data->size);
+    vtss_kfree(my_work);
+    atomic_dec(&g_vtss_kernel_task_in_progress);
+    DEBUG_MP("done!");
+}
+
+struct vtss_memchunk* vtss_create_memchunk_async(gfp_t flags, unsigned int size)
+{
+
+    if (in_atomic())
+    {
+        struct vtss_memchunk_alloc_data data;
+        // create async
+        DEBUG_MP("The attempt to allocate memory in irqs disabled mode. Start async");
+        data.flags = flags;
+        data.size = size;
+        atomic_inc(&g_vtss_kernel_task_in_progress);
+        if (atomic_read(&g_vtss_kernel_task_in_progress) > 1)
+        { 
+            DEBUG_MP("Cannot create async, g_vtss_kernel_task_in_progress = %d", atomic_read(&g_vtss_kernel_task_in_progress));
+            atomic_dec(&g_vtss_kernel_task_in_progress);
+            return NULL;
+        }
+        if (vtss_queue_work(-1, vtss_create_memchunk_work, &data, sizeof(data)))
+        {
+            DEBUG_MP("cannot create !!!");
+            atomic_dec(&g_vtss_kernel_task_in_progress);
+        }
+        // no need to wait memory creation. caller decides if it's reasonable                                                                                                                                                                                                         }
+        //while (atomic_read(&g_vtss_kernel_task_in_progress));
+        return NULL;
+    }
+    DEBUG_MP("creating chunk...");
+    return vtss_create_memchunk(flags, size);
+}
+
+void vtss_destroy_memchunk(struct vtss_memchunk* chunk)
+{
+    DEBUG_MP("chunk = %p, size = 0x%lx, deleting....", chunk, (unsigned long)chunk - (unsigned long)&chunk->memblocks + chunk->size);
+
+    free_pages((unsigned long)chunk, get_order(sizeof(struct vtss_memchunk) + chunk->size));
+
+    DEBUG_MP("done");
+}
+
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_destroy_memchunk_work(struct work_struct *work)
+#else
+static void vtss_destroy_memchunk_work(void *work)
+#endif
+{
+    struct vtss_work* my_work = (struct vtss_work*)work;
+    struct vtss_memchunk* chunk = NULL;
+    if (!my_work)
+    {
+        ERROR("Empty work");
+        return;
+    }
+    if (!atomic_read(&g_vtss_mempool_init))
+    {
+        vtss_kfree(my_work);
+        atomic_dec(&g_vtss_kernel_task_in_progress);
+        return;
+    }
+    chunk = (struct vtss_memchunk*)my_work->data;
+    if (!chunk)
+    {
+        ERROR("Nothing to delete");
+        vtss_kfree(my_work);
+        atomic_dec(&g_vtss_kernel_task_in_progress);
+        return;
+    }
+    vtss_destroy_memchunk(chunk);
+    vtss_kfree(my_work);
+    atomic_dec(&g_vtss_kernel_task_in_progress);
+}
+
+void vtss_destroy_memchunk_async(struct vtss_memchunk* chunk)
+{
+    if (in_atomic())
+    {
+        DEBUG_MP("The attempt to deallocate memory in irqs disabled mode. Start async");
+        atomic_inc(&g_vtss_kernel_task_in_progress);
+        if (vtss_queue_work(-1, vtss_destroy_memchunk_work, &chunk, sizeof(chunk)))
+        {
+            DEBUG_MP("failed create work");
+            atomic_dec(&g_vtss_kernel_task_in_progress);
+        }
+        return;
+    }
+    vtss_destroy_memchunk(chunk);
+}
+
+static void vtss_delete_chunk_list(void)
+{
+    struct list_head* p = NULL;
+    struct list_head* tmp = NULL;
+
+    vtss_memchunk_list_write_lock();
+    list_for_each_safe(p, tmp, &g_memchunk_list)
+    {
+        struct vtss_memchunk* chunk = list_entry(p, struct vtss_memchunk, list);
+        touch_nmi_watchdog();
+        if (chunk == NULL)
+        {
+             ERROR("Chunk in list is NULL");
+             continue;
+        }
+        list_del(p);
+        atomic_dec(&g_vtss_chunk_list_length);
+        DEBUG_MP("chunk %p destroying ....", chunk);
+        vtss_destroy_memchunk(chunk);
+        DEBUG_MP("done.");
+    }
+    INIT_LIST_HEAD(&g_memchunk_list);
+    vtss_memchunk_list_write_unlock();
+    return;
+}
+
+int vtss_memory_pool_init(void)
+{
+    int i = 0;
+
+    int nr_chunks = num_present_cpus();
+    unsigned long long prealloc_buf_size = PAGE_SIZE << VTSS_ALLOC_ORDER_MAX;
+    struct vtss_memchunk *chunk = NULL;
+    nr_chunks += 4;
+    nr_chunks = nr_chunks > VTSS_NR_CHUNKS_MAX ? VTSS_NR_CHUNKS_MAX : nr_chunks;
+    nr_chunks = nr_chunks < VTSS_NR_CHUNKS_MIN ? VTSS_NR_CHUNKS_MIN : nr_chunks;
+    DEBUG_MP("prealloc_buf_size = %llx,  num_present_cpus = %d", prealloc_buf_size, num_present_cpus());
+
+    vtss_memchunk_list_write_lock();
+    INIT_LIST_HEAD(&g_memchunk_list);
+    vtss_memchunk_list_write_unlock();
+    
+    atomic_set(&g_vtss_chunk_list_length, 0);
+    
+    for (i = 0; i < nr_chunks - 5; i++)
+    {
+        chunk = vtss_create_memchunk(GFP_NOWAIT, prealloc_buf_size);
+        if (!chunk)
+        {
+            ERROR("Not enough memory for GFP_NOWAIT chunk[%d]", i);
+            return -1;
+        }
+    }
+    chunk = vtss_create_memchunk(GFP_KERNEL, prealloc_buf_size);
+    if (!chunk)
+    {
+        ERROR("Not enough memory for GFP_KERNEL chunk");
+        return -1;
+    }
+    chunk = vtss_create_memchunk(GFP_KERNEL, prealloc_buf_size);
+    if (!chunk)
+    {
+        ERROR("Not enough memory for GFP_KERNEL chunk");
+        return -1;
+    }
+    chunk = vtss_create_memchunk(GFP_KERNEL, prealloc_buf_size);
+    if (!chunk)
+    {
+        ERROR("Not enough memory for GFP_KERNEL chunk");
+        return -1;
+    }
+    chunk = vtss_create_memchunk(GFP_ATOMIC, PAGE_SIZE<<VTSS_ALLOC_ORDER_ATOMIC);
+    if (!chunk)
+    {
+        ERROR("Not enough memory for GFP_ATOMIC chunk");
+        return -1;
+    }
+
+    chunk = vtss_create_memchunk(GFP_ATOMIC, PAGE_SIZE<<VTSS_ALLOC_ORDER_ATOMIC);
+    if (!chunk)
+    {
+        ERROR("Not enough memory for GFP_ATOMIC chunk");
+        return -1;
+    }
+/*    chunk = vtss_create_memchunk(GFP_ATOMIC, PAGE_SIZE<<VTSS_ALLOC_ORDER_MIN);
+    if (!chunk)
+    {
+        ERROR("Not enough memory for GFP_ATOMIC chunk");
+        return -1;
+    }*/
+    atomic_inc(&g_vtss_mempool_init);
+
+    return 0;
+}
+
+void vtss_memory_pool_fini(void)
+{
+    int i = 0;
+    atomic_dec(&g_vtss_mempool_init);
+
+    while (atomic_read(&g_vtss_kernel_task_in_progress))
+    {
+         i++;
+         if (i == VTSS_WAIT_TIME) ERROR("Awaiting unfinishing kernel tasks...");
+         msleep_interruptible(1);
+         touch_nmi_watchdog();
+    };
+    DEBUG_MP("Ok. No active kernel tasks.");
+    i = 0;
+    vtss_delete_chunk_list();
+    DEBUG_MP("empty memory pool");
+
+    return;
+}
+
+static struct vtss_memory_block* vtss_find_free_block(struct vtss_memchunk* chunk, size_t size)
+{
+    struct vtss_memory_block* block = NULL;
+    struct vtss_memory_block* temp = NULL;
+    unsigned long start_addr = 0;
+    unsigned long end_addr = 0;
+    
+    //search from tail
+    DEBUG_MP("start");
+    temp = chunk->tail;
+    atomic_sub(size + sizeof(struct vtss_memory_block), &chunk->free_space);
+    while (temp && atomic_read(&temp->is_free))
+    {
+        DEBUG_MP("finding tail, temp = %p", temp);
+        temp = temp->prev;
+        if (temp) temp->next = NULL;
+        chunk->tail = temp;
+    }
+    if (temp)
+    {
+        start_addr = (unsigned long)temp + sizeof(struct vtss_memory_block) + temp->size;
+        DEBUG_MP("(unsigned long)temp + sizeof(*temp) + temp->size;%lx", start_addr);
+        
+        end_addr = (unsigned long)(&chunk->memblocks) + chunk->size;
+        if (end_addr - start_addr >= size + sizeof(struct vtss_memory_block))
+        {
+            block = (struct vtss_memory_block* )start_addr;
+            block->prev = temp;
+            block->next = NULL;
+            block->size = size;
+            atomic_set(&block->is_free, 0);
+            temp->next = block;
+            chunk->tail = block;
+            return block;
+         }
+    }
+    else
+    {
+        chunk->head = NULL;
+    }
+
+    //search from head
+    temp = chunk->head;
+
+    start_addr = (unsigned long)(chunk) + sizeof(struct vtss_memchunk);
+
+
+    if (!temp)
+    {
+        if (chunk->size >= size + sizeof(struct vtss_memory_block))
+        {
+            block = (struct vtss_memory_block* )start_addr;
+            block->prev = NULL;
+            block->next = NULL;
+            block->size = size;
+            atomic_set(&block->is_free, 0);
+        }
+        chunk->head = chunk->tail = block;
+        DEBUG_MP("Chunk is empty. Return %p", block);
+        if (!block)
+        {
+           ERROR("Error in alghorithm");
+           atomic_add(size+sizeof(struct vtss_memory_block), &chunk->free_space);
+        }
+        return block;
+    }
+
+    //search free space in the middle and merge garbage
+    end_addr = (unsigned long)(temp);
+
+    while (temp)
+    {
+        //DEBUG_MP("Searching free space in the middle of the chunk");
+        if (atomic_read(&temp->is_free))
+        {
+            if (temp->prev) temp->prev->next = temp->next;
+            if (temp->next) temp->next->prev = temp->prev;
+            if (temp == chunk->head) chunk->head = temp->next;
+            if (temp == chunk->tail) chunk->tail = temp->prev;
+            temp = temp->next;
+            end_addr = (temp) ? (unsigned long)(temp) : (unsigned long)(chunk) + sizeof(struct vtss_memchunk)+ chunk->size;
+            continue;
+        }
+        if (end_addr - start_addr >= size + sizeof(struct vtss_memory_block))
+        {
+            block = (struct vtss_memory_block* )(start_addr);
+            if (temp->prev)
+            {
+                block->prev = temp->prev;
+                temp->prev->next = block;
+            }
+            else
+            {
+                block->prev = NULL;
+                chunk->head = block;
+            }
+            temp->prev = block;
+            block->next = temp;
+            block->size = size;
+            atomic_set(&block->is_free, 0);
+            return block;
+        }
+        start_addr = end_addr = (unsigned long)(temp) + sizeof(struct vtss_memory_block)+ temp->size;
+        temp = temp->next;
+        end_addr = (temp) ? (unsigned long)(temp) : (unsigned long)(chunk) + sizeof(struct vtss_memchunk)+ chunk->size;
+    }
+
+    if (end_addr - start_addr >= size + sizeof(struct vtss_memory_block))
+    {
+        DEBUG_MP("Return tail block that was deallocated in different thread");
+        block = (struct vtss_memory_block* )start_addr;
+        block->prev = chunk->tail;
+        if (chunk->tail) chunk->tail->next = block;
+        block->next = NULL;
+        block->size = size;
+        atomic_set(&block->is_free, 0);
+        chunk->tail = block;
+        if (!chunk->head) chunk->head = block;
+    }
+    if (!block) atomic_add(size+sizeof(struct vtss_memory_block), &chunk->free_space);
+    return block;
+}
+
+unsigned long vtss_get_free_block(gfp_t gfp_mask, size_t size)
+{
+    struct list_head* p = NULL;
+    struct list_head* tmp = NULL;
+    unsigned long block_addr = 0;
+    vtss_memchunk_list_read_lock();
+    list_for_each_safe(p, tmp, &g_memchunk_list)
+    {
+        struct vtss_memchunk* chunk = list_entry(p, struct vtss_memchunk, list);
+        touch_nmi_watchdog();
+        if (chunk == NULL)
+        {
+             ERROR("Chunk in list is NULL");
+             continue;
+        }
+        if (chunk->flags != gfp_mask)
+        {
+            //DEBUG_MP("GFP FLAGS are different");
+            continue;
+        }
+        if (atomic_cmpxchg(&chunk->busy, 0, 1))
+        {
+            DEBUG_MP("Chunk is busy");
+            continue;
+        };
+        if (atomic_read(&chunk->free_space) > size + sizeof(struct vtss_memory_block))
+        {
+            struct vtss_memory_block* block =  vtss_find_free_block(chunk, size);
+            if (block)
+            {
+                block_addr = (unsigned long)block + sizeof(struct vtss_memory_block);
+            }
+        }
+        atomic_set(&chunk->busy, 0);
+        if (block_addr > 0)
+        {
+            if (block_addr + size >= (unsigned long)chunk + chunk->size + sizeof(struct vtss_memchunk))
+            {
+                 DEBUG_MP("ERROR!!! chunk = %p, &chunk->memblocks = %lx, chunk->size = %zx, block_addr = %lx,size = %lx, memblock+size=%lx", chunk, (unsigned long)&chunk->memblocks, chunk->size, block_addr, size, (unsigned long)(&chunk->memblocks) + chunk->size);
+                 vtss_free_block(block_addr, size);
+                 block_addr = 0;
+            }
+            break;
+        }
+    }
+    vtss_memchunk_list_read_unlock();
+    
+    DEBUG_MP("getting block size = %zu, found block_addr = %lx", size, block_addr);
+    return block_addr;
+}
+
+void vtss_free_block(unsigned long block_addr, size_t size)
+{
+    struct list_head* p = NULL;
+    struct list_head* tmp = NULL;
+    unsigned long addr = block_addr - sizeof(struct vtss_memory_block);
+
+    DEBUG_MP("free addr = %lx, block_addr = %lx", addr, block_addr);
+    vtss_memchunk_list_read_lock();
+    list_for_each_safe(p, tmp, &g_memchunk_list)
+    {
+        struct vtss_memchunk* chunk = list_entry(p, struct vtss_memchunk, list);
+        touch_nmi_watchdog();
+        if (chunk == NULL)
+        {
+             ERROR("Chunk in list is NULL");
+             continue;
+        }
+        if ((unsigned long)&chunk->memblocks[0] <= addr && addr < (unsigned long)chunk + sizeof(struct vtss_memchunk) + chunk->size)
+        {
+           struct vtss_memory_block* block = (struct vtss_memory_block*)(addr); // - offsetof(struct vtss_memory_block, memblock))
+           struct vtss_memory_block* block_next = NULL;
+           size_t removed_size = 0;
+           do
+           {
+               DEBUG_MP("Block will be marked as free: %p, size = %zx",block, block->size );
+               if (atomic_read(&block->is_free))
+               {
+                   ERROR("Freeing block several times, block = %p", block);
+               }
+               removed_size = removed_size + block->size + sizeof(struct vtss_memory_block);
+               if (size > removed_size)
+               {
+                  DEBUG_MP("removing wrong size");
+               }
+               block_next = block->next;
+               atomic_set(&block->is_free, 1);
+               block = block_next;
+           }
+           while (block && size > removed_size);
+           DEBUG_MP("removed_size = %zx", removed_size );
+           atomic_add(removed_size, &chunk->free_space);
+           break;
+        }
+    }
+    vtss_memchunk_list_read_unlock();
+    return;
+}
+
+static unsigned long vtss_try_get_free_block(gfp_t flags, size_t size, int cnt)
+{
+    void* block = NULL;
+    if (flags != GFP_ATOMIC) //to avoid recursion
+    {
+        vtss_create_memchunk_async(flags, size+sizeof(struct vtss_memchunk)+sizeof(struct vtss_memory_block));
+    }
+    while (!block && (cnt--) > 0)
+    {
+        block = (void*)vtss_get_free_block(flags, size);
+        DEBUG_MP("Cannot get block! trying again");
+    }
+    return (unsigned long) block;
+}
+
+unsigned long vtss_get_free_pages_internal(gfp_t gfp_mask, unsigned int order)
+{
+    unsigned long block = vtss_get_free_block(gfp_mask, PAGE_SIZE<<order);
+    if (block == 0)
+    {
+        block = vtss_try_get_free_block(gfp_mask, PAGE_SIZE<<order, 3);
+    }
+    return block;
+}
+
+void vtss_free_pages_internal(unsigned long addr, unsigned int order)
+{
+   vtss_free_block(addr, PAGE_SIZE<<order);
+   return;
+}
+
+unsigned long vtss_get_free_page_internal(gfp_t gfp_mask)
+{
+    return vtss_get_free_block(gfp_mask, PAGE_SIZE);
+}
+
+void vtss_free_page_internal(unsigned long addr)
+{
+   vtss_free_block(addr, PAGE_SIZE);
+   return;
+}
+
+void* vtss_kmalloc_internal(size_t size, gfp_t flags)
+{
+    void* block = (void*)vtss_get_free_block(flags, size);
+    if (!block)
+    {
+        DEBUG_MP("attempt to create async, flags = %x\n", (int)flags);
+        block = (void*)vtss_try_get_free_block(flags, size, 1000);
+    }
+    return block;
+}
+
+void vtss_kfree_internal(const void * item)
+{
+    vtss_free_block((unsigned long) item, 0);
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/module.c b/drivers/misc/intel/sepdk/vtsspp/module.c
new file mode 100644
index 000000000000..71f4ec04c216
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/module.c
@@ -0,0 +1,941 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "module.h"
+#include "regs.h"
+#include "collector.h"
+#include "globals.h"
+
+#include <linux/string.h>
+#include <linux/timer.h>
+#include <linux/delay.h>
+#include <linux/kallsyms.h>
+#include <linux/sched.h>
+#include <linux/pid.h>
+#include <linux/slab.h>
+#include <linux/kprobes.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,12,0)
+#include <linux/uaccess.h>
+#endif
+#include <asm/uaccess.h>
+
+#define VTSS_MODULE_AUTHOR "Copyright (C) 2010-2015 Intel Corporation"
+#define VTSS_MODULE_NAME   "vtss++ kernel module (" VTSS_TO_STR(VTSS_VERSION_STRING) ")"
+
+#define VTSS_DEBUG_MODULE TRACE
+
+int uid = 0;
+module_param(uid, int, S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH);
+MODULE_PARM_DESC(uid, "An user id for profiling");
+
+int gid = 0;
+module_param(gid, int, S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH);
+MODULE_PARM_DESC(gid, "A group id for profiling");
+
+int mode = 0;
+module_param(mode, int, S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH);
+MODULE_PARM_DESC(mode, "A mode for files in procfs");
+
+
+#ifdef VTSS_DEBUG_TRACE
+static char debug_trace_name[64] = "";
+static int  debug_trace_size     = 0;
+module_param_string(trace, debug_trace_name, sizeof(debug_trace_name), S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH);
+MODULE_PARM_DESC(trace, "Turn on trace output from functions starting with this name");
+
+
+
+
+
+int vtss_check_trace(const char* func_name, int* flag)
+{
+    return (debug_trace_size && !strncmp(func_name, debug_trace_name, debug_trace_size)) ? 1 : -1;
+}
+#endif
+
+
+
+#ifdef VTSS_TRACE_EVENTS_SCHED
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0)
+
+#define VTSS_TRACE_POINT(name) \
+struct tracepoint* vtss_tracepoint_##name_ptr = (struct tracepoint*)kallsyms_lookup_name("__tracepoint_"#name);
+
+#define VTSS_REGISTER_TRACE(name) { \
+    VTSS_TRACE_POINT(name) \
+    VTSS_DEBUG_MODULE("ptr for the name = %s: %p", #name, vtss_tracepoint_##name_ptr); \
+    rc = (vtss_tracepoint_##name_ptr) ? tracepoint_probe_register(vtss_tracepoint_##name_ptr, tp_##name VTSS_TP_DATA) : -1; \
+    VTSS_DEBUG_MODULE("rc for tracepoint name = %s: %d", "__tracepoint_"#name, (int)rc); \
+}
+
+#define VTSS_UNREGISTER_TRACE(name) { \
+    VTSS_TRACE_POINT(name) \
+    rc = (vtss_tracepoint_##name_ptr) ? tracepoint_probe_unregister(vtss_tracepoint_##name_ptr, tp_##name VTSS_TP_DATA) : -1; \
+}
+
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0)
+
+#define VTSS_REGISTER_TRACE(name) register_trace_##name(tp_##name VTSS_TP_DATA);
+#define VTSS_UNREGISTER_TRACE(name) unregister_trace_##name(tp_##name VTSS_TP_DATA);
+
+#endif
+#endif
+/* ---- probe symbols ---- */
+
+#if defined(CONFIG_X86_32)
+#define VTSS_SYMBOL_SCHED_SWITCH  "__switch_to"
+#define VTSS_SYMBOL_SCHED_SWITCH_AUX  "context_switch"
+#elif defined(CONFIG_X86_64)
+#define VTSS_SYMBOL_SCHED_SWITCH  "context_switch"
+#define VTSS_SYMBOL_SCHED_SWITCH_AUX  "__switch_to"
+#endif
+
+#define VTSS_SYMBOL_PROC_FORK     "do_fork"
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+#define VTSS_SYMBOL_PROC_EXEC     "do_execve"
+#else
+// from the version 3.9 do_execve is inlined into sys_execve, and probe is broken because of this.
+// lp tried to use sys_execve instead, but this crashes ia32 bit systems
+// so, we are using do_execve_common for 32 bit.
+// The only possible solution dor intel64 is use sys_execve
+#if defined(CONFIG_X86_32) && (LINUX_VERSION_CODE < KERNEL_VERSION(3,16,0))
+#define VTSS_SYMBOL_PROC_EXEC     "do_execve"
+#else
+#define VTSS_SYMBOL_PROC_EXEC     "sys_execve"
+#define VTSS_EXEC_PROBE_USER 1
+#endif
+#endif
+
+#if defined(CONFIG_COMPAT)
+#define VTSS_SYMBOL_PROC_COMPAT_EXEC     "compat_do_execve"
+#define VTSS_SYMBOL_PROC_COMPAT_EXEC1   "compat_sys_execve"
+#endif
+static int compat_exec_probe_user = 0;
+
+#define VTSS_SYMBOL_PROC_EXIT     "do_exit"
+#define VTSS_SYMBOL_MMAP_REGION   "mmap_region"
+#ifdef VTSS_SYSCALL_TRACE
+#define VTSS_SYMBOL_SYSCALL_ENTER "syscall_trace_enter"
+#define VTSS_SYMBOL_SYSCALL_LEAVE "syscall_trace_leave"
+#endif
+#if defined(CONFIG_TRACEPOINTS) && defined(VTSS_TRACE_EVENTS_SCHED)
+#include <trace/events/sched.h>
+#ifdef DECLARE_TRACE_NOARGS
+#define VTSS_TP_DATA   , NULL
+#define VTSS_TP_PROTO  void *cb_data __attribute__ ((unused)),
+#else  /* DECLARE_TRACE_NOARGS */
+#define VTSS_TP_DATA
+#define VTSS_TP_PROTO
+#endif /* DECLARE_TRACE_NOARGS */
+#endif /* CONFIG_TRACEPOINTS && VTSS_TRACE_EVENTS_SCHED */
+#ifdef VTSS_AUTOCONF_TRACE_SCHED_RQ
+#define VTSS_TP_RQ struct rq* rq,
+#else  /* VTSS_AUTOCONF_TRACE_SCHED_RQ */
+#define VTSS_TP_RQ
+#endif /* VTSS_AUTOCONF_TRACE_SCHED_RQ */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,4,0)
+#define PREEMPT bool preempt,
+#else
+#define PREEMPT
+#endif
+
+#if defined(CONFIG_TRACEPOINTS) && defined(VTSS_TRACE_EVENTS_SCHED)
+static void tp_sched_switch(VTSS_TP_PROTO PREEMPT VTSS_TP_RQ struct task_struct *prev, struct task_struct *next)
+{
+   void* prev_bp = NULL;
+   void* prev_ip = NULL;
+   if (prev == current && current != 0)
+   {
+       unsigned long bp;
+       vtss_get_current_bp(bp);
+       prev_bp = (void*)bp;//(void*)_RET_IP_;//(void*)bp;
+       prev_ip = (void*)_THIS_IP_ ;
+   }
+   vtss_sched_switch(prev, next, prev_bp, prev_ip);
+}
+#endif
+
+static void jp_sched_switch(VTSS_TP_RQ struct task_struct *prev, struct task_struct *next)
+{
+   void* prev_bp = NULL;
+   void* prev_ip = NULL;
+   if (prev == current && current !=0 )
+   {
+       unsigned long bp;
+       vtss_get_current_bp(bp);
+       prev_bp = (void*)bp;
+       prev_ip = (void*)_THIS_IP_ ;
+   }
+    vtss_sched_switch(prev, next, prev_bp, prev_ip);
+    jprobe_return();
+}
+
+#if defined(CONFIG_TRACEPOINTS) && defined(VTSS_TRACE_EVENTS_SCHED)
+static void tp_sched_process_fork(VTSS_TP_PROTO struct task_struct *task, struct task_struct *child)
+{
+    vtss_target_fork(task, child);
+}
+#endif
+
+
+static int rp_sched_process_fork_enter(struct kretprobe_instance *ri, struct pt_regs *regs)
+{
+    /* Skip kernel threads or if no memory */
+    return (current->mm == NULL) ? 1 : 0;
+}
+
+static int rp_sched_process_fork_leave(struct kretprobe_instance *ri, struct pt_regs *regs)
+{
+    pid_t pid = (pid_t)regs_return_value(regs);
+
+    if (pid) {
+        struct task_struct *task = NULL;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,31)
+        struct pid *p_pid = find_get_pid(pid);
+        task = pid_task(p_pid, PIDTYPE_PID);
+        put_pid(p_pid);
+#else /* < 2.6.31 */
+        rcu_read_lock();
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+        task = find_task_by_vpid(pid);
+#else /* < 2.6.24 */
+        task = find_task_by_pid(pid);
+#endif /* 2.6.24 */
+        rcu_read_unlock();
+#endif /* 2.6.31 */
+        vtss_target_fork(current, task);
+    }
+    return 0;
+}
+
+/* per-instance private data */
+struct rp_sched_process_exec_data
+{
+    char filename[VTSS_FILENAME_SIZE];
+    char config[VTSS_FILENAME_SIZE];
+    int ppid;
+};
+
+#if 0
+void vtss_show_regs(struct pt_regs *regs)
+{
+    unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L, fs, gs, shadowgs;
+    unsigned long d0, d1, d2, d3, d6, d7;
+    unsigned int fsindex, gsindex;
+    unsigned int ds, cs, es;
+
+    printk(KERN_DEFAULT "RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->ip);
+    printk("ip = %lx\n", regs->ip);
+    printk(KERN_DEFAULT "RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss,
+                         regs->sp, regs->flags);
+    printk(KERN_DEFAULT "RAX: %016lx RBX: %016lx RCX: %016lx\n",
+        regs->ax, regs->bx, regs->cx);
+    printk(KERN_DEFAULT "RDX: %016lx RSI: %016lx RDI: %016lx\n",
+        regs->dx, regs->si, regs->di);
+    printk(KERN_DEFAULT "RBP: %016lx R08: %016lx R09: %016lx\n",
+        regs->bp, regs->r8, regs->r9);
+    printk(KERN_DEFAULT "R10: %016lx R11: %016lx R12: %016lx\n",
+        regs->r10, regs->r11, regs->r12);
+    printk(KERN_DEFAULT "R13: %016lx R14: %016lx R15: %016lx\n",
+        regs->r13, regs->r14, regs->r15);
+     asm("movl %%ds,%0" : "=r" (ds));
+     asm("movl %%cs,%0" : "=r" (cs));
+     asm("movl %%es,%0" : "=r" (es));
+     asm("movl %%fs,%0" : "=r" (fsindex));
+     asm("movl %%gs,%0" : "=r" (gsindex));
+     rdmsrl(MSR_FS_BASE, fs);
+     rdmsrl(MSR_GS_BASE, gs);
+     rdmsrl(MSR_KERNEL_GS_BASE, shadowgs);
+     cr0 = read_cr0();
+     cr2 = read_cr2();
+     cr3 = read_cr3();
+     cr4 = read_cr4();
+     printk(KERN_DEFAULT "FS:  %016lx(%04x) GS:%016lx(%04x) knlGS:%016lx\n",
+           fs, fsindex, gs, gsindex, shadowgs);
+     printk(KERN_DEFAULT "CS:  %04x DS: %04x ES: %04x CR0: %016lx\n", cs, ds,
+                     es, cr0);
+     printk(KERN_DEFAULT "CR2: %016lx CR3: %016lx CR4: %016lx\n", cr2, cr3,
+                     cr4);
+/*     get_debugreg(d0, 0);
+     get_debugreg(d1, 1);
+     get_debugreg(d2, 2);
+     get_debugreg(d3, 3);
+     get_debugreg(d6, 6);
+     get_debugreg(d7, 7);*/
+}
+#endif
+
+
+// The reason of creating the function below and copping all context from envp rp_sched_process_exec_enter
+// is the crash during attempt to get environment in the case when compat_do_execve called.
+// TODO: solve the problem and remove this workaround.
+static int rp_sched_process_exec_compat_enter(struct kretprobe_instance *ri, struct pt_regs *regs)
+{
+    int i;
+    size_t size = 0;
+    char *filename = NULL;
+    //char **envp = NULL;
+    struct rp_sched_process_exec_data *data = (struct rp_sched_process_exec_data*)ri->data;
+    if (current == NULL){
+        ERROR("current = NULL");
+        return 1; /* Skip kernel threads or if no memory */
+    }
+    if (current->mm == NULL){
+        ERROR("current->mm = NULL");
+        return 1; /* Skip kernel threads or if no memory */
+    }
+    if (regs == NULL){
+        ERROR("regs==NULL");
+        return 1;
+    }
+//    vtss_show_regs(regs);
+#if defined(CONFIG_X86_32)
+    filename =  (char*)REG(ax, regs);
+    //envp     = (char**)REG(cx, regs);
+#elif defined(CONFIG_X86_64)
+    filename =  (char*)REG(di, regs);
+    //envp     = (char**)REG(dx, regs);
+#endif
+    //return 0;
+    if (filename != NULL) {
+        char* p = filename;
+        if (compat_exec_probe_user){
+            i = 0;
+            while((i < VTSS_FILENAME_SIZE - 1) && (vtss_copy_from_user(&data->filename[i], p, 1)==0)) {
+                if (data->filename[i] == '/' ) i = 0;
+                else if (data->filename[i] == '\0' ) break;
+                else i++;
+                p++;
+            }
+            size = i;
+        } else {
+            p = strrchr(filename, '/');
+            p = p ? p+1 : filename;
+            TRACE("filename: '%s' => '%s'", filename, p);
+            size = min((size_t)VTSS_FILENAME_SIZE-1, (size_t)strlen(p));
+            memcpy(data->filename, p, size);
+        }
+    }
+    data->filename[size] = '\0';
+    size = 0;
+#if 0
+    for (i = 0; envp[i] != NULL; i++) {
+        TRACE("env[%d]: '%s'\n", i, envp[i]);
+        if (!strncmp(envp[i], "INTEL_VTSS_PROFILE_ME=", 22 /*==strlen("INTEL_VTSS_PROFILE_ME=")*/)) {
+            char *config = envp[i]+22; /*==strlen("INTEL_VTSS_PROFILE_ME=")*/
+            size = min((size_t)VTSS_FILENAME_SIZE-1, (size_t)strlen(config));
+            memcpy(data->config, config, size);
+            break;
+        }
+    }
+#endif
+    data->config[size] = '\0';
+    data->ppid = TASK_TID(ri->task);
+    TRACE("ri=0x%p, data=0x%p, filename='%s', config='%s'", ri, data, data->filename, data->config);
+    vtss_target_exec_enter(ri->task, data->filename, data->config);
+    return 0;
+}
+
+
+static int rp_sched_process_exec_enter(struct kretprobe_instance *ri, struct pt_regs *regs)
+{
+    int i;
+    size_t size = 0;
+    char *filename, **envp;
+    struct rp_sched_process_exec_data *data = (struct rp_sched_process_exec_data*)ri->data;
+
+    if (current->mm == NULL)
+        return 1; /* Skip kernel threads or if no memory */
+#if defined(CONFIG_X86_32)
+    filename =  (char*)REG(ax, regs);
+    envp     = (char**)REG(cx, regs);
+#elif defined(CONFIG_X86_64)
+    filename =  (char*)REG(di, regs);
+    envp     = (char**)REG(dx, regs);
+#endif
+    if (filename != NULL) {
+        char* p = filename;
+#ifdef VTSS_EXEC_PROBE_USER
+        i = 0;
+        while((i < VTSS_FILENAME_SIZE - 1) && (vtss_copy_from_user(&data->filename[i], p, 1)==0)) {
+            if (data->filename[i] == '/' ) i = 0;
+            else if (data->filename[i] == '\0' ) break;
+            else i++;
+            p++;
+        }
+        size = i;
+#else
+        p = strrchr(filename, '/');
+        p = p ? p+1 : filename;
+        TRACE("filename: '%s' => '%s'", filename, p);
+        size = min((size_t)VTSS_FILENAME_SIZE-1, (size_t)strlen(p));
+        memcpy(data->filename, p, size);
+#endif
+    }
+    data->filename[size] = '\0';
+    size = 0;
+#ifdef VTSS_EXEC_PROBE_USER
+    if (envp){
+        char *envp_k;
+        const char* intel_profile_me = "INTEL_VTSS_PROFILE_ME=";
+        while((vtss_copy_from_user(&envp_k, envp, (sizeof(char*)))==0) && envp_k != NULL) {
+            i = 0;
+            while((i < 22 ) && (vtss_copy_from_user(&data->config[i], envp_k, 1)==0)) {
+                if (data->config[i] != intel_profile_me[i] ) break;
+                else i++;
+                envp_k++;
+            }
+            if (i != 22) break;
+            i = 0;
+            while((i < VTSS_FILENAME_SIZE - 1) && (vtss_copy_from_user(&data->config[i], envp_k, 1)==0)) {
+                if (data->config[i] == '\0' ) break;
+                else i++;
+                envp_k++;
+            }
+            envp++;
+        }
+    }
+#else
+    if (envp) for (i = 0; envp[i] != NULL; i++) {
+        TRACE("env[%d]: '%s'\n", i, envp[i]);
+        if (!strncmp(envp[i], "INTEL_VTSS_PROFILE_ME=", 22 /*==strlen("INTEL_VTSS_PROFILE_ME=")*/)) {
+            char *config = envp[i]+22; /*==strlen("INTEL_VTSS_PROFILE_ME=")*/
+            size = min((size_t)VTSS_FILENAME_SIZE-1, (size_t)strlen(config));
+            memcpy(data->config, config, size);
+            break;
+        }
+    }
+#endif
+    data->config[size] = '\0';
+    data->ppid = TASK_TID(ri->task);
+    TRACE("ri=0x%p, data=0x%p, filename='%s', config='%s'", ri, data, data->filename, data->config);
+    vtss_target_exec_enter(ri->task, data->filename, data->config);
+    return 0;
+}
+static int rp_sched_process_exec_leave(struct kretprobe_instance *ri, struct pt_regs *regs)
+{
+    struct rp_sched_process_exec_data *data = (struct rp_sched_process_exec_data*)ri->data;
+    int rc = regs_return_value(regs);
+    if (!data){
+     ERROR("!!!!data is null");
+     return 0;
+    }
+    if (data->filename)TRACE("ri=0x%p, data=0x%p, filename='%s', config='%s', rc=%d", ri, data, data->filename, data->config, rc);
+    vtss_target_exec_leave(ri->task, data->filename, data->config, rc, data->ppid);
+    return 0;
+}
+
+#if defined(CONFIG_TRACEPOINTS) && defined(VTSS_TRACE_EVENTS_SCHED)
+static void tp_sched_process_exit(VTSS_TP_PROTO struct task_struct *task)
+{
+    vtss_target_exit(task);
+}
+#endif
+
+static int kp_sched_process_exit(struct kprobe *p, struct pt_regs *regs)
+{
+    vtss_target_exit(current);
+    return 0;
+}
+
+/*
+unsigned long mmap_region(
+    struct file*  file,
+    unsigned long addr,
+    unsigned long len,
+    unsigned long flags,
+    unsigned int  vm_flags,
+    unsigned long pgoff
+);
+*/
+
+/* per-instance private data */
+struct rp_mmap_region_data
+{
+    struct file*  file;
+    unsigned long addr;
+    unsigned long size;
+    unsigned long pgoff;
+    unsigned int  flags;
+};
+
+static int vtss_mmap_count = 0;
+static int rp_mmap_region_enter(struct kretprobe_instance *ri, struct pt_regs *regs)
+{
+    struct rp_mmap_region_data *data = (struct rp_mmap_region_data*)ri->data;
+
+    if (current->mm == NULL)
+        return 1; /* Skip kernel threads or if no memory */
+#if defined(CONFIG_X86_32)
+    data->file  = (struct file*)REG(ax, regs);
+    data->addr  = REG(dx, regs);
+    data->size  = REG(cx, regs);
+    /* get the rest from stack */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+    data->flags = ((int32_t*)&REG(sp, regs))[2]; /* vm_flags */
+    data->pgoff = data->file ? ((int32_t*)&REG(sp, regs))[3] : 0;
+#else
+    data->flags = ((int32_t*)&REG(sp, regs))[1]; /* vm_flags */
+    data->pgoff = data->file ? ((int32_t*)&REG(sp, regs))[2] : 0;
+#endif
+#elif defined(CONFIG_X86_64)
+    data->file  = (struct file*)REG(di, regs);
+    data->addr  = REG(si, regs);
+    data->size  = REG(dx, regs);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+    data->flags = REG(r8, regs); /* vm_flags */
+    data->pgoff = data->file ? REG(r9, regs) : 0;
+#else
+    data->flags = REG(cx, regs); /* vm_flags */
+    data->pgoff = data->file ? REG(r8, regs) : 0;
+#endif
+#endif
+    if (vtss_mmap_count < 100){
+    vtss_mmap_count++;
+    TRACE("ri=0x%p, data=0x%p: (0x%p, 0x%lx, %lu, %lu, 0x%x)", ri, data, data->file, data->addr, data->size, data->pgoff, data->flags);
+    }
+    return 0;
+}
+
+static int rp_mmap_region_leave(struct kretprobe_instance *ri, struct pt_regs *regs)
+{
+    struct rp_mmap_region_data *data = (struct rp_mmap_region_data*)ri->data;
+    unsigned long rc = regs_return_value(regs);
+
+//    TRACE("ri=0x%p, data=0x%p: rc=0x%lx", ri, data, rc);
+//#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+    if ((rc == data->addr) &&
+        (data->flags & (VM_EXEC|VM_MAYEXEC)) && !(data->flags & VM_WRITE) &&
+        data->file && data->file->f_path.dentry)
+    {
+        if (vtss_mmap_count < 100)TRACE("file=0x%p, addr=0x%lx, pgoff=%lu, size=%lu", data->file, data->addr, data->pgoff, data->size);
+        vtss_mmap(data->file, data->addr, data->pgoff, data->size);
+    } else
+    {
+        if (vtss_mmap_count < 100)TRACE("Address range was not added to the map, addr=0x%lx, pgoff=%lu, size=%lu, rc = %lx", data->addr, data->pgoff, data->size, rc);
+    }
+/*#else
+    if (rc == data->addr)
+    {
+        if (vtss_mmap_count < 100) TRACE("file=0x%p, addr=0x%lx, pgoff=%lu, size=%lu", data->file, data->addr, data->pgoff, data->size);
+        
+        vtss_mmap_reload(data->file, data->addr);
+    } else
+    {
+        if (vtss_mmap_count < 100)TRACE("Address range was not added to the map, addr=0x%lx, pgoff=%lu, size=%lu, rc = %lx", data->addr, data->pgoff, data->size, rc);
+    }
+
+
+#endif*/
+    return 0;
+}
+
+#ifdef VTSS_SYSCALL_TRACE
+
+static int kp_syscall_enter(struct kprobe *p, struct pt_regs *regs)
+{
+    struct pt_regs* sregs;
+
+    if (current->mm == NULL)
+        return 1; /* Skip kernel threads or if no memory */
+#if defined(CONFIG_X86_32)
+    sregs = (struct pt_regs*)REG(ax, regs);
+#elif defined(CONFIG_X86_64)
+    sregs = (struct pt_regs*)REG(di, regs);
+#endif
+    vtss_syscall_enter(sregs);
+    return 0;
+}
+
+static int kp_syscall_leave(struct kprobe *p, struct pt_regs *regs)
+{
+    struct pt_regs* sregs;
+
+    if (current->mm == NULL)
+        return 1; /* Skip kernel threads or if no memory */
+#if defined(CONFIG_X86_32)
+    sregs = (struct pt_regs*)REG(ax, regs);
+#elif defined(CONFIG_X86_64)
+    sregs = (struct pt_regs*)REG(di, regs);
+#endif
+    vtss_syscall_leave(sregs);
+    return 0;
+}
+
+#endif /* VTSS_SYSCALL_TRACE */
+
+/* ------------------------------------------------------------------------- */
+/* Helpers macros */
+#ifdef VTSS_AUTOCONF_KPROBE_FLAGS
+#define _SET_KPROBE_FLAGS(name) name.flags = 0;
+#else
+#define _SET_KPROBE_FLAGS(name)
+#endif
+
+#ifdef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+#define _SET_SYMBOL_NAME(symbol) .symbol_name = symbol,
+#define _SET_KP_SYMBOL_NAME(symbol) .kp.symbol_name = symbol,
+#define _LOOKUP_SYMBOL_NAME(name,symbol) /* empty */
+#else
+#define _SET_SYMBOL_NAME(symbol) /* empty */
+#define _SET_KP_SYMBOL_NAME(symbol) /* empty */
+#define _LOOKUP_SYMBOL_NAME(name,symbol) \
+    name.addr = (kprobe_opcode_t*)kallsyms_lookup_name(symbol); \
+    if (!name.addr) { \
+        ERROR("Unable to find symbol '%s'", symbol); \
+        rc = -1; \
+    } else
+#endif
+
+/* ------------------------------------------------------------------------- */
+/* Define kprobe stub */
+#define DEFINE_KP_STUB(name,symbol) \
+static struct kprobe _kp_##name = { \
+    .pre_handler   = kp_##name, \
+    .post_handler  = NULL, \
+    .fault_handler = NULL, \
+    _SET_SYMBOL_NAME(symbol) \
+    .addr = (kprobe_opcode_t*)NULL \
+}; \
+static int probe_##name(void) \
+{ \
+    int rc = 0; \
+    _REGISTER_TRACE(name) \
+    { \
+        _LOOKUP_SYMBOL_NAME(_kp_##name,symbol) \
+        { \
+            _SET_KPROBE_FLAGS(_kp_##name) \
+            rc = register_kprobe(&_kp_##name); \
+            if (rc) INFO("register_kprobe('%s') failed: %d", symbol, rc); \
+        } \
+    } \
+    return rc; \
+} \
+static int unprobe_##name(void) \
+{ \
+    int rc = 0; \
+    _UNREGISTER_TRACE(name) \
+    if (_kp_##name.addr) unregister_kprobe(&_kp_##name); \
+    _kp_##name.addr = NULL; \
+    return rc; \
+}
+
+/* ------------------------------------------------------------------------- */
+/* Define jprobe stub */
+#define DEFINE_JP_STUB(name,symbol,symbol_aux) \
+static struct jprobe _jp_##name = { \
+    _SET_KP_SYMBOL_NAME(symbol) \
+    .kp.addr = (kprobe_opcode_t*)NULL, \
+    .entry = (kprobe_opcode_t*)jp_##name \
+}; \
+static struct jprobe _jp_##name_aux = { \
+    _SET_KP_SYMBOL_NAME(symbol_aux) \
+    .kp.addr = (kprobe_opcode_t*)NULL, \
+    .entry = (kprobe_opcode_t*)jp_##name \
+}; \
+static int used_##name_aux = 0;\
+static int probe_##name(void) \
+{ \
+    int rc = 0; \
+    used_##name_aux = 0;\
+    _REGISTER_TRACE(name) \
+    { \
+        _LOOKUP_SYMBOL_NAME(_jp_##name.kp,symbol) \
+        { \
+            _SET_KPROBE_FLAGS(_jp_##name.kp) \
+            rc = register_jprobe(&_jp_##name); \
+        } \
+        if (rc){\
+        used_##name_aux = 1;\
+        _LOOKUP_SYMBOL_NAME(_jp_##name_aux.kp,symbol_aux) \
+        { \
+            _SET_KPROBE_FLAGS(_jp_##name_aux.kp) \
+            rc = register_jprobe(&_jp_##name_aux); \
+        }\
+        } \
+        if (rc) ERROR("register_jprobe('%s') failed: %d", symbol, rc); \
+    } \
+    return rc; \
+} \
+static int unprobe_##name(void) \
+{ \
+    int rc = 0; \
+    _UNREGISTER_TRACE(name) \
+    if (used_##name_aux == 0){\
+        if (_jp_##name.kp.addr) unregister_jprobe(&_jp_##name); \
+        _jp_##name.kp.addr = NULL; \
+    } else {\
+        if (_jp_##name_aux.kp.addr) unregister_jprobe(&_jp_##name_aux); \
+        _jp_##name_aux.kp.addr = NULL; \
+    }\
+    return rc; \
+}
+
+/* ------------------------------------------------------------------------- */
+/* Define kretprobe stub */
+#define DEFINE_RP_STUB(name,symbol,size) \
+static struct kretprobe _rp_##name = { \
+    _SET_KP_SYMBOL_NAME(symbol) \
+    .kp.addr       = (kprobe_opcode_t*)NULL, \
+    .entry_handler = rp_##name##_enter, \
+    .handler       = rp_##name##_leave, \
+    .data_size     = size, \
+    .maxactive     = 16 /* probe up to 16 instances concurrently */ \
+}; \
+static int probe_##name(void) \
+{ \
+    int rc = 0; \
+    _REGISTER_TRACE(name) \
+    { \
+        _LOOKUP_SYMBOL_NAME(_rp_##name.kp,symbol) \
+        { \
+            _SET_KPROBE_FLAGS(_rp_##name.kp) \
+            rc = register_kretprobe(&_rp_##name); \
+            if (rc) ERROR("register_kretprobe('%s') failed: %d", symbol, rc); \
+        } \
+    } \
+    return rc; \
+} \
+static int unprobe_##name(void) \
+{ \
+    int rc = 0; \
+    _UNREGISTER_TRACE(name) \
+    if (_rp_##name.kp.addr) unregister_kretprobe(&_rp_##name); \
+    _rp_##name.kp.addr = NULL; \
+    if (_rp_##name.nmissed) ERROR("Missed probing %d instances of '%s'", _rp_##name.nmissed, symbol); \
+    return rc; \
+}
+
+/* ------------------------------------------------------------------------- */
+
+/* stubs without tracepoints */
+#define _REGISTER_TRACE(name)   /* empty */
+#define _UNREGISTER_TRACE(name) /* empty */
+
+DEFINE_RP_STUB(sched_process_exec, VTSS_SYMBOL_PROC_EXEC,   sizeof(struct rp_sched_process_exec_data))
+DEFINE_RP_STUB(mmap_region,        VTSS_SYMBOL_MMAP_REGION, sizeof(struct rp_mmap_region_data))
+
+#ifdef VTSS_SYSCALL_TRACE
+DEFINE_KP_STUB(syscall_enter,      VTSS_SYMBOL_SYSCALL_ENTER)
+DEFINE_KP_STUB(syscall_leave,      VTSS_SYMBOL_SYSCALL_LEAVE)
+#endif
+
+/* ------------------------------------------------------------------------- */
+/* stubs with tracepoints */
+#if defined(CONFIG_TRACEPOINTS) && defined(VTSS_TRACE_EVENTS_SCHED)
+#undef _REGISTER_TRACE
+#undef _UNREGISTER_TRACE
+#define _REGISTER_TRACE(name) \
+    VTSS_REGISTER_TRACE(name)\
+    if (rc) INFO("Unable register tracepoint: %d", rc); \
+    if (rc)
+#define _UNREGISTER_TRACE(name) \
+    VTSS_UNREGISTER_TRACE(name)
+#endif
+
+DEFINE_JP_STUB(sched_switch, VTSS_SYMBOL_SCHED_SWITCH, VTSS_SYMBOL_SCHED_SWITCH_AUX)
+DEFINE_RP_STUB(sched_process_fork, VTSS_SYMBOL_PROC_FORK, 0)
+DEFINE_KP_STUB(sched_process_exit, VTSS_SYMBOL_PROC_EXIT)
+
+/* ------------------------------------------------------------------------- */
+/* kernel module notifier */
+static int vtss_kmodule_notifier(struct notifier_block *block, unsigned long val, void *data)
+{
+    struct module *mod = (struct module*)data;
+    const char *name = mod->name;
+#ifdef VTSS_AUTOCONF_MODULE_CORE_LAYOUT
+    unsigned long module_core = (unsigned long)mod->core_layout.base;
+    unsigned long core_size = mod->core_layout.size;
+#else
+    unsigned long module_core = (unsigned long)mod->module_core;
+    unsigned long core_size = mod->core_size;
+#endif
+    if (val == MODULE_STATE_COMING) {
+        TRACE("MODULE_STATE_COMING: name='%s', module_core=0x%lx, core_size=%lu", name, module_core, core_size);
+        vtss_kmap(current, name, module_core, 0, core_size);
+    } else if (val == MODULE_STATE_GOING) {
+        TRACE("MODULE_STATE_GOING:  name='%s'", name);
+    }
+    return NOTIFY_DONE;
+}
+
+static struct notifier_block vtss_kmodules_nb = {
+    .notifier_call = &vtss_kmodule_notifier
+};
+
+static int probe_kmodules(void)
+{
+    return register_module_notifier(&vtss_kmodules_nb);
+}
+
+static int unprobe_kmodules(void)
+{
+    return unregister_module_notifier(&vtss_kmodules_nb);
+}
+
+#ifdef CONFIG_COMPAT
+//kretprobes
+static struct kretprobe _rp_sched_process_exec_compat = {
+#ifdef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+    .kp.symbol_name = VTSS_SYMBOL_PROC_COMPAT_EXEC,
+#endif
+    .kp.addr       = (kprobe_opcode_t*)NULL,
+    .entry_handler = rp_sched_process_exec_compat_enter,
+    .handler       = rp_sched_process_exec_leave,
+    .data_size     = sizeof(struct rp_sched_process_exec_data),
+    .maxactive     = 16 /* probe up to 16 instances concurrently */
+};
+
+//DEFINE_KRETPROBE_STRUCT(sched_process_exec, VTSS_SYMBOL_PROC_COMPAT_EXEC,   sizeof(struct rp_sched_process_exec_data))
+int probe_sched_process_exec_compat( void )
+{
+    int rc = 0;
+    compat_exec_probe_user = 0;
+#ifndef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+    int used_exec1_symbol = 0;
+    _rp_sched_process_exec_compat.addr = (kprobe_opcode_t*)kallsyms_lookup_name(VTSS_SYMBOL_PROC_COMPAT_EXEC);
+    if (!_rp_sched_process_exec_compat.addr) {
+        INFO("Lookup the name of kretprobe %s failed. Trying to find %s name", VTSS_SYMBOL_PROC_COMPAT_EXEC, VTSS_SYMBOL_PROC_COMPAT_EXEC1 );
+        _rp_sched_process_exec_compat.addr = (kprobe_opcode_t*)kallsyms_lookup_name(VTSS_SYMBOL_PROC_COMPAT_EXEC1);
+        used_exec1_symbol = 1;
+        if (!_rp_sched_process_exec_compat.addr) {
+             ERROR("Unable to find symbol '%s'", VTSS_SYMBOL_PROC_COMPAT_EXEC1);
+             return -1;
+        }
+        compat_exec_probe_user = 1;
+    }
+#endif
+    _SET_KPROBE_FLAGS(_rp_sched_process_exec_compat.kp)
+    rc = register_kretprobe(&_rp_sched_process_exec_compat);
+    if (rc){
+#ifdef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+         TRACE("Registering the prob on %s failed. Trying to register %s.", VTSS_SYMBOL_PROC_COMPAT_EXEC, VTSS_SYMBOL_PROC_COMPAT_EXEC1);
+         _rp_sched_process_exec_compat.kp.symbol_name = VTSS_SYMBOL_PROC_COMPAT_EXEC1;
+         rc = register_kretprobe(&_rp_sched_process_exec_compat);
+         compat_exec_probe_user = 1;
+         if (rc)
+             ERROR("register_kretprobe('%s') failed: %d", VTSS_SYMBOL_PROC_COMPAT_EXEC1, rc);
+#else
+         ERROR("register_kretprobe('%s') failed: %d", (used_exec1_symbol == 0) ? VTSS_SYMBOL_PROC_COMPAT_EXEC :  VTSS_SYMBOL_PROC_COMPAT_EXEC1, rc);
+#endif
+    }
+    return rc;
+}
+
+int unprobe_sched_process_exec_compat( void )
+{
+    int rc = 0;
+    if (_rp_sched_process_exec_compat.kp.addr) unregister_kretprobe(&_rp_sched_process_exec_compat);
+    _rp_sched_process_exec_compat.kp.addr = NULL;
+    if (_rp_sched_process_exec_compat.nmissed) INFO("Missed probing %d instances of '%s'", _rp_sched_process_exec_compat.nmissed, VTSS_SYMBOL_PROC_COMPAT_EXEC);
+    return rc;
+}
+#endif
+
+int vtss_probe_init(void)
+{
+    int rc = 0;
+#ifdef VTSS_SYSCALL_TRACE
+    rc |= probe_syscall_leave();
+    if(rc == 0) {
+        rc |= probe_syscall_enter();
+    }
+    else rc = 0;
+#endif
+    rc |= probe_sched_process_exit();
+    rc |= probe_sched_process_fork();
+#ifdef CONFIG_COMPAT
+    rc = probe_sched_process_exec_compat();
+#endif
+    rc |= probe_sched_process_exec();
+    rc |= probe_mmap_region();
+    rc |= probe_kmodules();
+#if !defined(CONFIG_PREEMPT_NOTIFIERS) || !defined(VTSS_USE_PREEMPT_NOTIFIERS)
+    rc |= probe_sched_switch();
+#endif
+    vtss_mmap_count = 0;
+    return rc;
+}
+
+void vtss_probe_fini(void)
+{
+#if !defined(CONFIG_PREEMPT_NOTIFIERS) || !defined(VTSS_USE_PREEMPT_NOTIFIERS)
+    unprobe_sched_switch();
+#endif
+    unprobe_kmodules();
+    unprobe_mmap_region();
+    unprobe_sched_process_exec();
+#ifdef CONFIG_COMPAT
+    unprobe_sched_process_exec_compat();
+#endif
+    unprobe_sched_process_fork();
+    unprobe_sched_process_exit();
+#ifdef VTSS_SYSCALL_TRACE
+    unprobe_syscall_enter();
+    unprobe_syscall_leave();
+#endif
+#if defined(CONFIG_TRACEPOINTS) && defined(VTSS_TRACE_EVENTS_SCHED)
+    tracepoint_synchronize_unregister();
+#endif
+}
+
+/* ----- module init/fini ----- */
+
+void cleanup_module(void)
+{
+    vtss_fini();
+    printk(VTSS_MODULE_NAME " unregistered\n");
+}
+
+int init_module(void)
+{
+    int rc = 0;
+
+#ifdef VTSS_DEBUG_TRACE
+    if (*debug_trace_name != '\0')
+        debug_trace_size = strlen(debug_trace_name);
+#endif
+
+    rc = vtss_init();
+
+    if (!rc) {
+        printk(VTSS_MODULE_NAME " registered\n");
+    } else {
+        printk(VTSS_MODULE_NAME " initialization falied\n");
+        vtss_fini();
+    }
+    return rc;
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR(VTSS_MODULE_AUTHOR);
+MODULE_DESCRIPTION(VTSS_MODULE_NAME);
diff --git a/drivers/misc/intel/sepdk/vtsspp/nmiwd.c b/drivers/misc/intel/sepdk/vtsspp/nmiwd.c
new file mode 100644
index 000000000000..bbe551accf49
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/nmiwd.c
@@ -0,0 +1,236 @@
+/*  Copyright (C) 2014-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+*/
+#include "vtss_config.h"
+#include "nmiwd.h"
+#include "vtsserr.h"
+
+#include <linux/cred.h>
+#include <linux/watchdog.h>
+#include <linux/namei.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/sched.h>
+#include <asm/uaccess.h>
+#include <linux/proc_fs.h> 
+#include <linux/kthread.h>
+#include <linux/delay.h>
+
+static int vtss_wd_state = 0; //disabled
+atomic_t vtss_nmiwd_sync = ATOMIC_INIT(1);
+
+static ssize_t vtss_kernel_write(struct file *file, const char *buf, size_t count, loff_t pos)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+    unsigned long res = 5;
+    mm_segment_t old_fs = get_fs();
+    set_fs(get_ds());
+    res = vfs_write(file, (const char __user *)buf, count, &pos);
+    set_fs(old_fs);
+
+    return res;
+#else
+    return kernel_write(file, buf, count, &pos);
+#endif
+}
+
+ssize_t vtss_kernel_read(struct file *file, void *buf, size_t count, loff_t pos)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,14,0)
+    return kernel_read(file, pos, buf, count);
+#else
+    return kernel_read(file, buf, count, &pos);
+#endif
+}
+
+static struct file* vtss_open_nmi_wd(void)
+{
+    struct file* fd = NULL;
+    fd = filp_open("/proc/sys/kernel/nmi_watchdog", O_RDWR, 0);
+    return fd;
+}
+void vtss_close(struct file* fd)
+{
+    filp_close(fd,0);
+}
+
+int vtss_nmi_watchdog_disable_thread(void* data)
+{
+    char c = '0';
+    struct file* fd = NULL;
+    int ret_code = 1;
+    struct cred* new;
+
+    int* mode_ptr = NULL;
+    TRACE("vtss_wd_state = %d", vtss_wd_state);
+    if (!data) {
+        ERROR("Internal error!");
+        return VTSS_ERR_INTERNAL;
+    }
+    mode_ptr = data;
+
+    new = prepare_kernel_cred(NULL);
+    if (new != NULL) {
+        commit_creds(new);
+    } else {
+        ERROR("Cannot prepare kernel creds");
+    }
+    // The function vtss_open_nmi_wd cannot be called when irqs disabled. 
+    // Otherwise can lead deadlock in smp_call_function_single!
+    fd = vtss_open_nmi_wd();
+    if (IS_ERR(fd)) {
+        TRACE("Watchdog device not enabled");
+        ret_code = 2;
+        goto exit_mark_no_file;
+    }
+    while (!atomic_dec_and_test(&vtss_nmiwd_sync)) {
+        atomic_inc(&vtss_nmiwd_sync);
+    }
+    TRACE("1 vtss_wd_state = %d", vtss_wd_state);
+    vtss_wd_state = vtss_wd_state + 1;
+    //check watchdog state
+    vtss_kernel_read(fd, &c, 1, 0);
+
+    if (c == '1' && vtss_wd_state != 1) {
+        ERROR("wixing counter");
+        vtss_wd_state = 1;
+    }
+    if (vtss_wd_state == 1) {
+
+        if (c == '0') {
+            TRACE("Watchdog device not enabled");
+            vtss_wd_state = vtss_wd_state - 1;
+            ret_code = 2;
+            goto unlock_exit_mark;
+        }
+        vtss_kernel_write(fd, "0", 1, 0);
+        TRACE("writing 0");
+        vtss_kernel_read(fd, &c, 1, 0);
+        if (c!='0') {
+            ERROR("Internal error: Watchdog is not disabled yet! Add wait into the code");
+        }
+    } else {
+        TRACE("was disabled already");
+    }
+    if (*mode_ptr == 1) {
+        vtss_wd_state = vtss_wd_state - 1;
+        if (vtss_wd_state != 0) {
+            TRACE("Watchdog state will not be changed as VTSS collection is runing");
+            ret_code = 2;
+        }
+    }
+unlock_exit_mark:
+    atomic_inc(&vtss_nmiwd_sync);
+    vtss_close(fd);
+exit_mark_no_file:
+    *mode_ptr = *mode_ptr+ret_code; //done!
+    TRACE("vtss_wd_state = %d", vtss_wd_state);
+    return ret_code;
+}
+
+int vtsspp_nmi_root_thread_create(int (*threadfn)(void *data), const char* name, int mode)
+{
+    int thread_ret = mode;
+    int count = 10000;
+    struct task_struct *task = NULL;
+
+    task = kthread_run(threadfn,(void*)&thread_ret, name);
+
+    TRACE("task tid=%d, task->state=%lx, task->exit_state=%lx", task->tgid, task->state, (unsigned long)task->exit_state);
+
+    while (thread_ret == mode && count > 0) {
+        msleep_interruptible(10);
+        TRACE("in wait, task->state=%lx, task->exit_state=%lx", task->state, (unsigned long)task->exit_state);
+        count--;
+    }
+    if (thread_ret == mode) {
+        ERROR("Watchtdog operations are still in progress");
+    }
+
+    return thread_ret - mode - 1;
+}
+
+int vtss_nmi_watchdog_disable(int mode)
+{
+    int ret;
+    ret = vtsspp_nmi_root_thread_create(&vtss_nmi_watchdog_disable_thread, "vtsspp_nmiwd_0", mode);
+    return ret;
+}
+
+int vtss_nmi_watchdog_enable_thread(void* data)
+{
+    struct file* fd = NULL;
+    int ret_code = 1;
+    struct cred* new;
+
+    int* mode_ptr = NULL;
+    if (!data) {
+        ERROR("Internal error!");
+        return VTSS_ERR_INTERNAL;
+    }
+    mode_ptr = data;
+
+    new = prepare_kernel_cred(NULL);
+
+    if (new != NULL) {
+        commit_creds(new);
+    }
+    fd = vtss_open_nmi_wd();
+    if (IS_ERR(fd)) {
+        TRACE("Watchdog device not enabled at the end of vtss collection");
+        ret_code = 2;
+        goto exit_mark_no_file;
+    }
+    while (!atomic_dec_and_test(&vtss_nmiwd_sync)) {
+        atomic_inc(&vtss_nmiwd_sync);
+    }
+    if (vtss_wd_state == 0 && *mode_ptr == 0) {
+        TRACE("Watchdog device was not enabled before collection");
+        ret_code = 2;
+        goto unlock_exit_mark;
+    }
+    if (*mode_ptr == 0) vtss_wd_state = vtss_wd_state - 1;
+    if (vtss_wd_state == 0) {
+        vtss_kernel_write(fd, "1", 1, 0);
+    }
+    else if (*mode_ptr == 1) {
+        TRACE("NMI watchdog timer cannot be enabled as VTSS collection is not finshed yet");
+        ret_code = 2;
+    }
+unlock_exit_mark:
+    atomic_inc(&vtss_nmiwd_sync);
+    vtss_close(fd);
+exit_mark_no_file:
+    *mode_ptr = *mode_ptr+ret_code; //done!
+    TRACE("vtss_wd_state = %d", vtss_wd_state);
+    return ret_code;
+}
+
+int vtss_nmi_watchdog_enable(int mode)
+{
+    int ret;
+    ret = vtsspp_nmi_root_thread_create(&vtss_nmi_watchdog_enable_thread, "vtsspp_nmiwd_1", mode);
+    return ret;
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/pebs.c b/drivers/misc/intel/sepdk/vtsspp/pebs.c
new file mode 100644
index 000000000000..2792fc42008c
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/pebs.c
@@ -0,0 +1,308 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "pebs.h"
+#include "dsa.h"
+#include "globals.h"
+
+#include <linux/percpu.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+
+#define PEBS_ENABLE_MSR         0x03f1
+#define PERF_CAPABILITIES_MSR   0x0345
+#define PEBS_TRAP_MASK          0x40
+#define PEBS_COUNT              2
+
+size_t vtss_pebs_record_size = 0;
+static unsigned long long vtss_pebs_enable_mask = 0ULL;
+static DEFINE_PER_CPU_SHARED_ALIGNED(vtss_pebs_t*, vtss_pebs_per_cpu);
+
+vtss_pebs_t* vtss_pebs_get(int cpu)
+{
+    vtss_dsa_t* dsa = vtss_dsa_get(cpu);
+
+    if (IS_DSA_64ON32) {
+        if (dsa->v32.pebs_index != dsa->v32.pebs_base) {
+            TRACE("base=0x%p, index=0x%p", dsa->v32.pebs_base, dsa->v32.pebs_index);
+            return (vtss_pebs_t*)dsa->v32.pebs_base;
+        }
+    } else {
+        if (dsa->v64.pebs_index != dsa->v64.pebs_base) {
+            TRACE("base=0x%p, index=0x%p", dsa->v64.pebs_base, dsa->v64.pebs_index);
+            return (vtss_pebs_t*)dsa->v64.pebs_base;
+        }
+    }
+    return NULL;
+}
+
+int vtss_pebs_is_trap(void)
+{
+    unsigned long long msr_val = 0ULL;
+
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+        rdmsrl(PERF_CAPABILITIES_MSR, msr_val);
+    }
+    return (msr_val & PEBS_TRAP_MASK);
+}
+
+#ifdef VTSS_CONFIG_KPTI
+#include <asm/cpu_entry_area.h>
+
+#define PEBS_BUFFER_SIZE (PAGE_SIZE << 4)
+
+static DEFINE_PER_CPU(void*, vtss_pebs_vaddr);
+
+static int vtss_pebs_alloc_buffer(int cpu)
+{
+    void *cea;
+    void *buffer;
+
+    per_cpu(vtss_pebs_vaddr, cpu) = NULL;
+    per_cpu(vtss_pebs_per_cpu, cpu) = NULL;
+    cea = &get_cpu_entry_area(cpu)->cpu_debug_buffers.pebs_buffer;
+    buffer = vtss_cea_alloc_pages(PEBS_BUFFER_SIZE, GFP_KERNEL, cpu);
+    if (unlikely(!buffer)) {
+        ERROR("Cannot allocate PEBS buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+    per_cpu(vtss_pebs_vaddr, cpu) = buffer;
+    vtss_cea_update(cea, buffer, PEBS_BUFFER_SIZE, PAGE_KERNEL);
+    per_cpu(vtss_pebs_per_cpu, cpu) = (vtss_pebs_t*)cea;
+    TRACE("allocated buffer for %d cpu cea=%p, vaddr=%p", cpu, cea, buffer);
+    return 0;
+}
+
+static void vtss_pebs_release_buffer(int cpu)
+{
+    void *cea;
+    void *buffer;
+
+    cea = per_cpu(vtss_pebs_per_cpu, cpu);
+    vtss_cea_clear(cea, PEBS_BUFFER_SIZE);
+    buffer = per_cpu(vtss_pebs_vaddr, cpu);
+    vtss_cea_free_pages(buffer, PEBS_BUFFER_SIZE);
+    TRACE("released buffer for %d cpu cea=%p, vaddr=%p", cpu, cea, buffer);
+}
+
+#elif defined(VTSS_CONFIG_KAISER)
+
+static int vtss_pebs_alloc_buffer(int cpu)
+{
+    void *buffer;
+
+    per_cpu(vtss_pebs_per_cpu, cpu) = NULL;
+    buffer = vtss_kaiser_alloc_pages(PEBS_COUNT*sizeof(vtss_pebs_t), GFP_KERNEL, cpu);
+    if (unlikely(!buffer)) {
+        ERROR("Cannot allocate PEBS buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+    per_cpu(vtss_pebs_per_cpu, cpu) = buffer;
+    TRACE("allocated buffer for %d cpu, buffer=%p", cpu, buffer);
+    return 0;
+}
+
+static void vtss_pebs_release_buffer(int cpu)
+{
+    void *buffer;
+
+    buffer = per_cpu(vtss_pebs_per_cpu, cpu);
+    vtss_kaiser_free_pages(buffer, PEBS_COUNT*sizeof(vtss_pebs_t));
+    TRACE("released buffer for %d cpu, buffer=%p", cpu, buffer);
+}
+
+#else
+
+static int vtss_pebs_alloc_buffer(int cpu)
+{
+    per_cpu(vtss_pebs_per_cpu, cpu) = NULL;
+    if ((per_cpu(vtss_pebs_per_cpu, cpu) = (vtss_pebs_t*)kmalloc_node(
+            PEBS_COUNT*sizeof(vtss_pebs_t), (GFP_KERNEL | __GFP_ZERO), cpu_to_node(cpu))) == NULL)
+    {
+        ERROR("Cannot allocate PEBS buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+    return 0;
+}
+
+static void vtss_pebs_release_buffer(int cpu)
+{
+    if (per_cpu(vtss_pebs_per_cpu, cpu) != NULL)
+        kfree(per_cpu(vtss_pebs_per_cpu, cpu));
+}
+#endif
+
+/* initialize PEBS in DSA for the processor */
+void vtss_pebs_init_dsa(void)
+{
+    int cpu;
+    vtss_dsa_t* dsa;
+    vtss_pebs_t* pebs;
+
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+    dsa = vtss_dsa_get(cpu);
+    pebs = per_cpu(vtss_pebs_per_cpu, cpu);
+
+    if (IS_DSA_64ON32) {
+        dsa->v32.pebs_base   = (void*)pebs;
+        dsa->v32.pebs_pad0   = NULL;
+        dsa->v32.pebs_index  = (void*)pebs;
+        dsa->v32.pebs_pad1   = NULL;
+        dsa->v32.pebs_absmax = (void*)((size_t)pebs + PEBS_COUNT*vtss_pebs_record_size);
+        dsa->v32.pebs_pad2   = NULL;
+        if (vtss_pebs_enable_mask == PEBS_ENABLE_MASK_NHM) {
+            dsa->v32.pebs_threshold = (void*)((size_t)pebs + vtss_pebs_record_size);
+        } else {
+            dsa->v32.pebs_threshold = (void*)pebs;
+        }
+        dsa->v32.pebs_pad3 = NULL;
+        dsa->v32.pebs_reset[0] = dsa->v32.pebs_reset[1] = NULL;
+        dsa->v32.pebs_reset[2] = dsa->v32.pebs_reset[3] = NULL;
+    } else {
+        dsa->v64.pebs_base   = (void*)pebs;
+        dsa->v64.pebs_index  = (void*)pebs;
+        dsa->v64.pebs_absmax = (void*)((size_t)pebs + PEBS_COUNT*vtss_pebs_record_size);
+        if (vtss_pebs_enable_mask == PEBS_ENABLE_MASK_NHM) {
+            dsa->v64.pebs_threshold = (void*)((size_t)pebs + vtss_pebs_record_size);
+        } else {
+            dsa->v64.pebs_threshold = (void*)pebs;
+        }
+        dsa->v64.pebs_reset[0] = dsa->v64.pebs_reset[1] = NULL;
+    }
+    /* invalidate the first PEBS record */
+    pebs->v1.ip = 0ULL;
+}
+
+void vtss_pebs_enable(void)
+{
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f && vtss_pebs_record_size) {
+        wrmsrl(PEBS_ENABLE_MSR, vtss_pebs_enable_mask);
+    }
+}
+
+void vtss_pebs_disable(void)
+{
+/**
+ * NOTE: Disabled as there're CPUs which reboot if
+ * a PEBS-PMI is encountered when PEBS is disabled.
+ * PEBS is effectively disabled when disabling BTS and PMU counters.
+ */
+#if 0
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+        wrmsrl(PEBS_ENABLE_MSR, 0ULL);
+    }
+#endif
+}
+
+static void vtss_pebs_on_each_cpu_func(void* ctx)
+{
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+        wrmsrl(PEBS_ENABLE_MSR, 0ULL);
+    }
+}
+
+int vtss_pebs_init(void)
+{
+    int cpu;
+    vtss_pebs_t pebs;
+
+    if (hardcfg.family == 0x06 && hardcfg.model >= 0x0f) {
+        switch (hardcfg.model) {
+            /// SLM(KNL)
+            case VTSS_CPU_KNL:
+                vtss_pebs_enable_mask = PEBS_ENABLE_MASK_MRM;
+                vtss_pebs_record_size = sizeof(pebs.v3);
+                break;
+            /* HSW/SLK/BDW/KBL */
+            case VTSS_CPU_HSW:
+            case VTSS_CPU_HSW_X:
+            case VTSS_CPU_HSW_ULT:
+            case VTSS_CPU_HSW_GT3:
+            case VTSS_CPU_BDW:
+            case VTSS_CPU_BDW_GT3:
+            case VTSS_CPU_BDW_X:
+            case VTSS_CPU_BDW_XD:
+            case VTSS_CPU_SKL:
+            case VTSS_CPU_SKL_M:
+            case VTSS_CPU_SKL_X:
+            case VTSS_CPU_KBL:
+            case VTSS_CPU_KBL_M:
+            case VTSS_CPU_CNL:
+            case VTSS_CPU_CNL_M:
+                vtss_pebs_record_size = sizeof(pebs.v3);
+                vtss_pebs_enable_mask = PEBS_ENABLE_MASK_NHM;
+                break;
+            /* NHM/SNB/IVB */
+            case VTSS_CPU_NHM:
+            case VTSS_CPU_NHM_G:
+            case VTSS_CPU_NHM_EP:
+            case VTSS_CPU_NHM_EX:
+            case VTSS_CPU_WMR:
+            case VTSS_CPU_WMR_EP:
+            case VTSS_CPU_WMR_EX:
+            case VTSS_CPU_SNB:
+            case VTSS_CPU_SNB_X:
+            case VTSS_CPU_IVB:
+            case VTSS_CPU_IVB_X:
+                vtss_pebs_record_size = sizeof(pebs.v2);
+                vtss_pebs_enable_mask = PEBS_ENABLE_MASK_NHM;
+                break;
+                /* Core2/Atom */
+            default:
+                vtss_pebs_record_size = sizeof(pebs.v1);
+                vtss_pebs_enable_mask = PEBS_ENABLE_MASK_MRM;
+                break;
+        }
+    }
+    for_each_possible_cpu(cpu) {
+        if (vtss_pebs_alloc_buffer(cpu)) goto fail;
+    }
+    on_each_cpu(vtss_pebs_on_each_cpu_func, NULL, SMP_CALL_FUNCTION_ARGS);
+
+    INFO("PEBSv%d: record size: 0x%02lx, mask: 0x%02llx", vtss_pebs_record_size == sizeof(pebs.v3) ? 3 : 1,
+            vtss_pebs_record_size, vtss_pebs_enable_mask);
+
+    return 0;
+fail:
+    for_each_possible_cpu(cpu) {
+        vtss_pebs_release_buffer(cpu);
+    }
+    return VTSS_ERR_NOMEMORY;
+}
+
+void vtss_pebs_fini(void)
+{
+    int cpu;
+    on_each_cpu(vtss_pebs_on_each_cpu_func, NULL, SMP_CALL_FUNCTION_ARGS);
+    for_each_possible_cpu(cpu) {
+        vtss_pebs_release_buffer(cpu);
+    }
+    vtss_pebs_record_size = 0;
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/procfs.c b/drivers/misc/intel/sepdk/vtsspp/procfs.c
new file mode 100644
index 000000000000..6baa741d0ca5
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/procfs.c
@@ -0,0 +1,1115 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "procfs.h"
+#include "globals.h"
+#include "collector.h"
+#include "cpuevents.h"
+#include "nmiwd.h"
+#include "globals.h"
+#include "memory_pool.h"
+
+#include <linux/list.h>         /* for struct list_head */
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/poll.h>
+#include <linux/fs.h>           /* for struct file_operations */
+#include <linux/namei.h>        /* for struct nameidata       */
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/proc_fs.h>
+#include <asm/uaccess.h>
+#include <linux/delay.h>
+
+#define VTSS_PROCFS_CTRL_NAME      ".control"
+#define VTSS_PROCFS_DEBUG_NAME     ".debug"
+#define VTSS_PROCFS_CPUMASK_NAME   ".cpu_mask"
+#define VTSS_PROCFS_DEFSAV_NAME    ".def_sav"
+#define VTSS_PROCFS_TARGETS_NAME   ".targets"
+#define VTSS_PROCFS_TIMESRC_NAME   ".time_source"
+#define VTSS_PROCFS_TIMELIMIT_NAME ".time_limit"
+
+
+#ifndef VTSS_AUTOCONF_CPUMASK_PARSELIST_USER
+#include "cpumask_parselist_user.c"
+#endif
+
+#define DEBUG_PROCFS TRACE
+
+extern int uid;
+extern int gid;
+extern int mode;
+extern unsigned int vtss_client_major_ver;
+extern unsigned int vtss_client_minor_ver;
+        
+struct vtss_procfs_ctrl_data
+{
+    struct list_head list;
+    size_t           size;
+    char             buf[0];
+};
+
+static cpumask_t vtss_procfs_cpumask_ = CPU_MASK_NONE;
+static int       vtss_procfs_defsav_  = 0;
+
+static struct proc_dir_entry *vtss_procfs_root = NULL;
+
+struct proc_dir_entry *vtss_procfs_get_root(void)
+{
+    return vtss_procfs_root;
+}
+
+const char *vtss_procfs_path(void)
+{
+    static char buf[MODULE_NAME_LEN + 7 /* strlen("/proc/") */];
+    snprintf(buf, sizeof(buf)-1, "/proc/%s", THIS_MODULE->name);
+    return buf;
+}
+
+static ssize_t vtss_procfs_ctrl_write(struct file *file, const char __user * buf, size_t count, loff_t * ppos)
+{
+    char chr;
+    size_t buf_size = count;
+    unsigned long flags = 0;
+
+    DEBUG_PROCFS("file=0x%p", file);
+    while (buf_size > 0) {
+        if (get_user(chr, buf))
+            return -EFAULT;
+
+        buf += sizeof(char);
+        buf_size -= sizeof(char);
+        TRACE("chr=%c", chr);
+        switch (chr) {
+        case 'V': { /* VXXXXX.XXXXX client version */
+                int major = 1;
+                vtss_client_major_ver = 0;
+                vtss_client_minor_ver = 0;
+//                return -EINVAL;
+                while (buf_size > 0) {
+                    if (get_user(chr, buf))
+                        return -EFAULT;
+                    if (chr >= '0' && chr <= '9') {
+                        buf += sizeof(char);
+                        buf_size -= sizeof(char);
+                        if (major) vtss_client_major_ver = vtss_client_major_ver * 10 + (chr - '0');
+                        else vtss_client_minor_ver = vtss_client_minor_ver * 10 + (chr - '0');
+                    } else{
+                        if (major && chr == '.'){
+                            major = 0;
+                            buf += sizeof(char);
+                            buf_size -= sizeof(char);
+                        }
+                        else {
+                            break;
+                        }
+                    }
+                }
+//                vtss_client_minor_ver = 0;
+                break;
+        }
+        case 'T': { /* T<pid> - Set target PID */
+                unsigned long pid = 0;
+
+                while (buf_size > 0) {
+                    if (get_user(chr, buf))
+                        return -EFAULT;
+                    if (chr >= '0' && chr <= '9') {
+                        buf += sizeof(char);
+                        buf_size -= sizeof(char);
+                        pid = pid * 10 + (chr - '0');
+                    } else
+                        break;
+                }
+                TRACE("TARGET: pid=%lu", pid);
+                if (pid != 0) {
+                    if (vtss_cmd_set_target((pid_t)pid)) {
+                        ERROR("Unable to find a target pid=%lu", pid);
+                        vtss_procfs_ctrl_wake_up(NULL, 0);
+                    }
+                }
+            }
+            break;
+        case 'I': { /* I<flags> - Initialize */
+                while (buf_size > 0) {
+                    if (get_user(chr, buf))
+                        return -EFAULT;
+                    if (chr >= '0' && chr <= '9') {
+                        buf += sizeof(char);
+                        buf_size -= sizeof(char);
+                        flags = flags * 10 + (chr - '0');
+                    } else
+                        break;
+                }
+                if (flags) {
+                    /* TODO: For compatibility with old implementation !!! */
+                    reqcfg.trace_cfg.trace_flags = flags;
+                }
+                TRACE("INIT: flags=0x%0lX (%lu)", flags, flags);
+                if (vtss_cmd_start() !=0 )
+                {
+                    ERROR("ERROR: Unable to start collection. Initialization failed.");
+                    return VTSS_ERR_INIT_FAILED;
+                }
+            }
+            break;
+        case 'E': { /* E<size>=... - configuration request */
+                unsigned long size = 0;
+                while (buf_size > 0) {
+                    if (get_user(chr, buf))
+                        return -EFAULT;
+                    buf += sizeof(char);
+                    buf_size -= sizeof(char);
+                    if (chr >= '0' && chr <= '9') {
+                        size = size * 10 + (chr - '0');
+                    } else
+                        break;
+                }
+                vtss_collection_cfg_init();
+                TRACE("chr2=%c, size = %d", chr, (int)size);
+                if (chr == '=' && size <= buf_size) {
+                    int namespace_size = 0;
+                    TRACE("BEGIN: size=%lu, buf_size=%zu", size, buf_size);
+                    while (size != 0){
+                        int cfgreq, fake_shift = 0;
+                        trace_cfg_t trace_currreq;
+                        stk_cfg_t stk_req;
+                        cpuevent_cfg_v1_t* cpuevent_currreq = NULL;
+                        if (flags){
+                            /* TODO: For compatibility with old implementation !!! */
+                            fake_shift = sizeof(int);
+                            buf -= fake_shift;
+                            buf_size += fake_shift;
+                            cfgreq = VTSS_CFGREQ_CPUEVENT_V1;
+                        } else {
+                            if (get_user(cfgreq, (const int __user *)buf)) {
+                                ERROR("Error in get_user()");
+                                return -EFAULT;
+                            }
+                        }
+                        TRACE("cfgreq = %lx", (unsigned long)cfgreq);
+                        switch (cfgreq) {
+                        case VTSS_CFGREQ_VOID:
+                            TRACE("VTSS_CFGREQ_VOID");
+                            size = 0;
+                            break;
+                        case VTSS_CFGREQ_CPUEVENT_V1:
+                            TRACE("in reading VTSS_CFGREQ_CPUEVENT_V1, reqcfg.cpuevent_count_v1=%d", (int)reqcfg.cpuevent_count_v1);
+                            if (reqcfg.cpuevent_count_v1 < VTSS_CFG_CHAIN_SIZE){
+                                if (vtss_copy_from_user(&reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1], buf, sizeof(cpuevent_cfg_v1_t))) {
+                                    ERROR("Error in copy_from_user()");
+                                    return -EFAULT;
+                                }
+                                cpuevent_currreq = &reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1];
+                                if (namespace_size + cpuevent_currreq->name_len + cpuevent_currreq->desc_len < VTSS_CFG_SPACE_SIZE * 16) {
+                                    /// copy CPU event name
+                                    if (vtss_copy_from_user(&reqcfg.cpuevent_namespace_v1[namespace_size], &buf[cpuevent_currreq->name_off+fake_shift], cpuevent_currreq->name_len)) {
+                                        ERROR("Error in copy_from_user()");
+                                        return -EFAULT;
+                                    }
+                                    TRACE("Load event[%02d]: '%s'", reqcfg.cpuevent_count_v1, &reqcfg.cpuevent_namespace_v1[namespace_size]);
+                                    /// adjust CPU event record
+                                    cpuevent_currreq->name_off = (int)((size_t)&reqcfg.cpuevent_namespace_v1[namespace_size] - (size_t)&reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1]);
+                                    /// adjust namespace size
+                                    namespace_size += cpuevent_currreq->name_len;
+                                    /// copy event description
+                                    if (vtss_copy_from_user(&reqcfg.cpuevent_namespace_v1[namespace_size], &buf[cpuevent_currreq->desc_off+fake_shift], cpuevent_currreq->desc_len)) {
+                                        ERROR("Error in copy_from_user()");
+                                        return -EFAULT;
+                                    }
+                                    /// adjust CPU event record
+                                    cpuevent_currreq->desc_off = (int)((size_t)&reqcfg.cpuevent_namespace_v1[namespace_size] - (size_t)&reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1]);
+                                    /// adjust namespace size
+                                    namespace_size += cpuevent_currreq->desc_len;
+                                    /// copy CPU event record
+                                    /* TODO: For compatibility with old implementation !!! */
+                                    reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].reqtype = VTSS_CFGREQ_CPUEVENT_V1;
+                                    /// adjust record size (as it may differ from initial request size)
+                                    reqcfg.cpuevent_cfg_v1[reqcfg.cpuevent_count_v1].reqsize = sizeof(cpuevent_cfg_v1_t) + cpuevent_currreq->name_len + cpuevent_currreq->desc_len;
+                                    reqcfg.cpuevent_count_v1++;
+                                }
+                            }
+                            if (!cpuevent_currreq){
+                               ERROR("Error in copy_from_user()");
+                               return -EFAULT;
+                            }
+                            buf += cpuevent_currreq->reqsize+fake_shift;
+                            buf_size -= cpuevent_currreq->reqsize+fake_shift;
+                            size -= cpuevent_currreq->reqsize;
+                            break;
+                        case VTSS_CFGREQ_OSEVENT:
+                            if (reqcfg.osevent_count < VTSS_CFG_CHAIN_SIZE) {
+                                /// copy OS event record
+                                if (vtss_copy_from_user(&reqcfg.osevent_cfg[reqcfg.osevent_count], buf, sizeof(osevent_cfg_t))) {
+                                    ERROR("Error in copy_from_user()");
+                                    return -EFAULT;
+                                }
+                                TRACE("VTSS_CFGREQ_OSEVENT[%d]: event_id=%d", reqcfg.osevent_count, reqcfg.osevent_cfg[reqcfg.osevent_count].event_id);
+                                reqcfg.osevent_count++;
+                            }
+                            buf += sizeof(osevent_cfg_t);
+                            buf_size -= sizeof(osevent_cfg_t);
+                            size -= sizeof(osevent_cfg_t);
+                            break;
+                        case VTSS_CFGREQ_BTS:
+                            if (vtss_copy_from_user(&reqcfg.bts_cfg, buf, sizeof(bts_cfg_t))) {
+                                ERROR("Error in copy_from_user()");
+                                return -EFAULT;
+                            }
+                            TRACE("VTSS_CFGREQ_BTS: brcount=%d, modifier=0x%0X", reqcfg.bts_cfg.brcount, reqcfg.bts_cfg.modifier);
+                            buf += sizeof(bts_cfg_t);
+                            buf_size -= sizeof(bts_cfg_t);
+                            size -= sizeof(bts_cfg_t);
+                            break;
+                        case VTSS_CFGREQ_LBR:
+                            if (vtss_copy_from_user(&reqcfg.lbr_cfg, buf, sizeof(lbr_cfg_t))) {
+                                ERROR("Error in copy_from_user()");
+                                return -EFAULT;
+                            }
+                            TRACE("VTSS_CFGREQ_LBR: brcount=%d, modifier=0x%0X", reqcfg.lbr_cfg.brcount, reqcfg.lbr_cfg.modifier);
+                            buf += sizeof(lbr_cfg_t);
+                            buf_size -= sizeof(lbr_cfg_t);
+                            size -= sizeof(lbr_cfg_t);
+                            break;
+                        case VTSS_CFGREQ_TRACE:
+                            if (vtss_copy_from_user(&trace_currreq, buf, sizeof(trace_cfg_t))) {
+                                ERROR("Error in copy_from_user()");
+                                return -EFAULT;
+                            }
+                            if (trace_currreq.namelen < VTSS_CFG_SPACE_SIZE) {
+                                if (vtss_copy_from_user(&reqcfg.trace_cfg, buf, sizeof(trace_cfg_t)+trace_currreq.namelen)) {
+                                    ERROR("Error in copy_from_user()");
+                                    return -EFAULT;
+                                }
+                            }
+                            TRACE("VTSS_CFGREQ_TRACE: trace_flags=0x%0X, namelen=%d", reqcfg.trace_cfg.trace_flags, trace_currreq.namelen);
+                            buf += sizeof(trace_cfg_t)+(trace_currreq.namelen-1);
+                            buf_size -= sizeof(trace_cfg_t)+(trace_currreq.namelen-1);
+                            size -= sizeof(trace_cfg_t)+(trace_currreq.namelen-1);
+                            break;
+                        case VTSS_CFGREQ_STK:
+                            if (vtss_copy_from_user(&stk_req, buf, sizeof(stk_cfg_t))) {
+                                ERROR("Error in copy_from_user()");
+                                return -EFAULT;
+                            }
+                            if (stk_req.stktype >= vtss_stk_last){
+                                ERROR("The stack settings is not supported in current driver version. Please update the driver.");
+                                break;
+                            }
+                            reqcfg.stk_pg_sz[stk_req.stktype] = (unsigned long)stk_req.stk_pg_sz;
+                            if (reqcfg.stk_pg_sz[stk_req.stktype] == 0) reqcfg.stk_pg_sz[stk_req.stktype] = PAGE_SIZE;
+
+                            reqcfg.stk_sz[stk_req.stktype]=(unsigned long)stk_req.stk_sz;
+                            if (reqcfg.stk_sz[stk_req.stktype] == 0) reqcfg.stk_sz[stk_req.stktype]=(unsigned long)-1;
+                            while (reqcfg.stk_pg_sz[stk_req.stktype] > reqcfg.stk_sz[stk_req.stktype]){
+                                 reqcfg.stk_pg_sz[stk_req.stktype] = (reqcfg.stk_pg_sz[stk_req.stktype] >> 1);
+                            }
+                            TRACE("VTSS_CFGREQ_STK: stk_sz=0x%lx", reqcfg.stk_pg_sz[stk_req.stktype]);
+                            buf += sizeof(stk_cfg_t);
+                            buf_size -= sizeof(stk_cfg_t);
+                            size -= sizeof(stk_cfg_t);
+                            break;
+                        case VTSS_CFGREQ_IPT:
+                            if (vtss_copy_from_user(&reqcfg.ipt_cfg, buf, sizeof(ipt_cfg_t))) {
+                                ERROR("Error in copy_from_user()");
+                                return -EFAULT;
+                            }
+                            buf += sizeof(ipt_cfg_t);
+                            buf_size -= sizeof(ipt_cfg_t);
+                            size -= sizeof(ipt_cfg_t);
+                            TRACE("VTSS_CFGREQ_IPT: reqcfg.ipt_cfg.mode= %X\n", reqcfg.ipt_cfg.mode);
+                            /* calculate ring buffer size in milliseconds  */
+                            reqcfg.ipt_cfg.size = reqcfg.ipt_cfg.size * 1000 + (reqcfg.ipt_cfg.mode>>22);
+                            TRACE("ipt_cfg.mode=%x, ipt_cfg.size=%d", reqcfg.ipt_cfg.mode, reqcfg.ipt_cfg.size);
+                            break;
+                        default:
+                            ERROR("Incorrect config request 0x%X", cfgreq);
+                            return -EFAULT;
+                        }
+                        TRACE("LOOP: size=%lu, buf_size=%zu", size, buf_size);
+                    } /* while (size != 0) */
+                    if ((reqcfg.cpuevent_count_v1 == 0 && !(reqcfg.trace_cfg.trace_flags & (VTSS_CFGTRACE_CTX|VTSS_CFGTRACE_PWRACT|VTSS_CFGTRACE_PWRIDLE)))||
+                        (reqcfg.cpuevent_count_v1 == 0 && hardcfg.family == 0x0b))
+                            vtss_cpuevents_reqcfg_default(0, vtss_procfs_defsav());
+                    vtss_sysevents_reqcfg_append();
+                    if(hardcfg.family != 0x06 || (hardcfg.model != 0x3d /* BDW */ && hardcfg.model != 0x47 /* BDW */ && hardcfg.model != 0x56 /* BDW-DE */ && hardcfg.model != 0x4f /* BDW-DE */ && 
+                                                  hardcfg.model != 0x4e /* SKL */ && hardcfg.model != 0x5e /* SKL */ && hardcfg.model != 0x55 /* SKX */ && 
+                                                  hardcfg.model != 0x9e /* KBL */ && hardcfg.model == 0x8e /* KBL */ &&
+                                                  hardcfg.model != 0x5c /* GLM */ && hardcfg.model != 0x5f /* DNV */ &&
+                                                  hardcfg.model != 0x7a /* GLP */ && hardcfg.model != 0x42 /* CNL */)){
+                        reqcfg.trace_cfg.trace_flags &= ~VTSS_CFGTRACE_IPT;
+                    } else {
+                        /// silently replace BTS with IPT on BDW
+                        if(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_BRANCH)
+                        {
+                            if(hardcfg.family == 0x06 && (hardcfg.model == 0x3d /* BDW */ || hardcfg.model == 0x47 /* BDW */ || hardcfg.model == 0x56 /* BDW-DE */ || hardcfg.model == 0x4f /* BDW-DE */ || 
+                                                          hardcfg.model == 0x4e /* SKL */ || hardcfg.model == 0x5e /* SKL */ || hardcfg.model == 0x55 /* SKX */ || 
+                                                          hardcfg.model == 0x9e /* KBL */ || hardcfg.model == 0x8e /* KBL */ ||
+                                                          hardcfg.model == 0x5c /* GLM */ || hardcfg.model == 0x5f /* DNV */ ||
+                                                          hardcfg.model == 0x7a /* GLP */ || hardcfg.model == 0x42 /* CNL */))
+                            {
+                                reqcfg.trace_cfg.trace_flags |= VTSS_CFGTRACE_IPT;  /// TODO: uncomment when the user-mode part is ready
+                            }
+                        }
+                        /// mutually exclude LBRs and IPT
+                        if(reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT)
+                        {
+                            reqcfg.trace_cfg.trace_flags &= ~(VTSS_CFGTRACE_BRANCH | VTSS_CFGTRACE_LASTBR | VTSS_CFGTRACE_LBRCSTK);
+                            if (reqcfg.ipt_cfg.mode & vtss_iptmode_full)
+                            {
+                                TRACE("VTSS_CFGREQ_IPT: remove stacks, reqcfg.ipt_cfg.mode= %X\n", reqcfg.ipt_cfg.mode);
+                                reqcfg.trace_cfg.trace_flags &= ~VTSS_CFGTRACE_STACKS;
+                            }
+                        }
+                    }
+                } else {
+                    ERROR("Invalid command: E%lu=...", size);
+                    return -EINVAL;
+                }
+            }
+            break;
+        case 'F': /* F - Finish or Stop */
+            DEBUG_PROCFS("STOP");
+            vtss_cmd_stop();
+            break;
+        case 'P': /* P - Pause */
+            DEBUG_PROCFS("PAUSE");
+            vtss_cmd_pause();
+            break;
+        case 'R': /* R - Resume */
+            DEBUG_PROCFS("RESUME");
+            vtss_cmd_resume();
+            break;
+        case 'B': /* B - Stop ring buffer */
+            DEBUG_PROCFS("STOP RB");
+            vtss_cmd_stop_ring_buffer();
+            break;
+        case 'W': /* W - Watchdog */
+            //W0 - disable watchdog
+            //W1 - enable watchdog
+            {
+              int st = 0;
+              DEBUG_PROCFS("Watchdog enable/disable command");
+              if (get_user(chr, buf))
+                return -EFAULT;
+              DEBUG_PROCFS("chr = %c", chr);
+              buf += sizeof(char);
+              buf_size -= sizeof(char);
+              if (chr == '0') {
+              st = vtss_nmi_watchdog_disable(0);
+              } else if (chr == '1') {
+                st = vtss_nmi_watchdog_enable(0);
+              } else if (chr == 'd') { //internal API
+            st = vtss_nmi_watchdog_disable(1); //for tests.this mode will not increment counter
+              } else if (chr == 'e') { //internal API
+                st = vtss_nmi_watchdog_enable(1); //for test. this mode will not decrement counter
+              } else {
+            st = -1;
+                ERROR("Watchdog command is not recognized");
+              }
+          if (st < 0) return -EINVAL;
+          if (st > 0) count = count - 1; //ignore the command
+          DEBUG_PROCFS("Watchdog command finished");
+        }
+            break;
+         case ' ':
+        case '\n':
+            break;
+        default:
+            ERROR("Invalid command: '%c'", chr);
+            return -EINVAL;
+        }
+    }
+    return count;
+}
+
+static DECLARE_WAIT_QUEUE_HEAD(vtss_procfs_ctrl_waitq);
+#ifdef VTSS_CONFIG_REALTIME
+static DEFINE_RAW_SPINLOCK(vtss_procfs_ctrl_list_lock);
+#else
+static DEFINE_SPINLOCK(vtss_procfs_ctrl_list_lock);
+#endif
+static LIST_HEAD(vtss_procfs_ctrl_list);
+static atomic_t vtss_procfs_attached = ATOMIC_INIT(0);
+
+int vtss_procfs_ctrl_wake_up(void *msg, size_t size)
+{
+    unsigned long flags;
+    struct vtss_procfs_ctrl_data *ctld = (struct vtss_procfs_ctrl_data*)vtss_kmalloc(sizeof(struct vtss_procfs_ctrl_data)+size, GFP_ATOMIC);
+
+    if (ctld == NULL) {
+        ERROR("Unable to allocate memory for message");
+        return -ENOMEM;
+    }
+    if (size) {
+        memcpy(ctld->buf, msg, size);
+        TRACE("msg=['%s', %d]", (char*)msg, (int)size);
+    } else {
+        DEBUG_PROCFS("[EOF]");
+    }
+    ctld->size = size;
+    vtss_spin_lock_irqsave(&vtss_procfs_ctrl_list_lock, flags);
+    list_add_tail(&ctld->list, &vtss_procfs_ctrl_list);
+    vtss_spin_unlock_irqrestore(&vtss_procfs_ctrl_list_lock, flags);
+    if (waitqueue_active(&vtss_procfs_ctrl_waitq)){
+        wake_up_interruptible(&vtss_procfs_ctrl_waitq);
+    }
+    return 0;
+}
+
+void vtss_procfs_ctrl_flush(void)
+{
+    unsigned long flags;
+    struct list_head *p, *tmp;
+    struct vtss_procfs_ctrl_data *ctld;
+    vtss_spin_lock_irqsave(&vtss_procfs_ctrl_list_lock, flags);
+    list_for_each_safe(p, tmp, &vtss_procfs_ctrl_list) {
+        ctld = list_entry(p, struct vtss_procfs_ctrl_data, list);
+        list_del_init(p);
+        vtss_kfree(ctld);
+    }
+    vtss_spin_unlock_irqrestore(&vtss_procfs_ctrl_list_lock, flags);
+}
+
+static ssize_t vtss_procfs_ctrl_read(struct file* file, char __user* buf, size_t size, loff_t* ppos)
+{
+    ssize_t rsize = 0;
+    unsigned long flags;
+    struct vtss_procfs_ctrl_data *ctld;
+    int no_ctrl;
+
+    DEBUG_PROCFS("file=0x%p", file);
+    /* wait for nonempty ready queue */
+    vtss_spin_lock_irqsave(&vtss_procfs_ctrl_list_lock, flags);
+    while (list_empty(&vtss_procfs_ctrl_list)) {
+        vtss_spin_unlock_irqrestore(&vtss_procfs_ctrl_list_lock, flags);
+        DEBUG_PROCFS("file=0x%p: WAIT", file);
+        if (file->f_flags & O_NONBLOCK)
+            return -EAGAIN;
+#if defined(VTSS_CONFIG_REALTIME)
+        {
+            unsigned long delay;
+            delay = msecs_to_jiffies(1000);
+            if (wait_event_interruptible_timeout(vtss_procfs_ctrl_waitq,
+                !list_empty(&vtss_procfs_ctrl_list), delay) < 0)
+                return -ERESTARTSYS;
+        }
+#else
+        if (wait_event_interruptible(vtss_procfs_ctrl_waitq,
+                !list_empty(&vtss_procfs_ctrl_list)) < 0)
+            return -ERESTARTSYS;
+#endif
+        vtss_spin_lock_irqsave(&vtss_procfs_ctrl_list_lock, flags);
+    }
+    /* get the first message from list */
+#ifdef list_first_entry
+    ctld = list_first_entry(&vtss_procfs_ctrl_list, struct vtss_procfs_ctrl_data, list);
+#else
+    ctld = list_entry(vtss_procfs_ctrl_list.next, struct vtss_procfs_ctrl_data, list);
+#endif
+    list_del_init(&ctld->list);
+    no_ctrl = list_empty(&vtss_procfs_ctrl_list);
+    vtss_spin_unlock_irqrestore(&vtss_procfs_ctrl_list_lock, flags);
+    /* write it out */
+    rsize = ctld->size;
+    if (rsize == 0) {
+        TRACE("file=0x%p: EOF", file);
+        vtss_kfree(ctld);
+        return 0;
+    }
+    DEBUG_PROCFS("file=0x%p, copy_to_user=['%s', %zu of %zu]", file, ctld->buf, ctld->size, size);
+    if (rsize > size) {
+        ERROR("Not enough buffer size for whole message");
+        vtss_kfree(ctld);
+        return -EINVAL;
+    }
+    *ppos += rsize;
+    if (copy_to_user(buf, ctld->buf, rsize)) {
+        vtss_kfree(ctld);
+        return -EFAULT;
+    }
+    vtss_kfree(ctld);
+    TRACE("rsize = %d", (int)rsize);
+
+    return rsize;
+}
+
+static unsigned int vtss_procfs_ctrl_poll(struct file *file, poll_table * poll_table)
+{
+    unsigned int rc = 0;
+    unsigned long flags;
+
+    if (!atomic_read(&vtss_procfs_attached))
+    {
+       TRACE("not attached");
+       return (POLLIN | POLLRDNORM);
+    }
+    poll_wait(file, &vtss_procfs_ctrl_waitq, poll_table);
+    vtss_spin_lock_irqsave(&vtss_procfs_ctrl_list_lock, flags);
+    if (!list_empty(&vtss_procfs_ctrl_list))
+        rc = (POLLIN | POLLRDNORM);
+    vtss_spin_unlock_irqrestore(&vtss_procfs_ctrl_list_lock, flags);
+    TRACE("file=0x%p: %s", file, (rc ? "READY" : "-----"));
+    return rc;
+}
+
+
+static int vtss_procfs_ctrl_open(struct inode *inode, struct file *file)
+{
+    /* Increase the priority for trace reader to avoid lost events */
+    set_user_nice(current, -19);
+    atomic_inc(&vtss_procfs_attached);
+    vtss_cmd_open();
+    return 0;
+}
+
+static int vtss_procfs_ctrl_close(struct inode *inode, struct file *file)
+{
+    if (atomic_dec_and_test(&vtss_procfs_attached)) {
+        TRACE("Nobody is attached");
+        vtss_procfs_ctrl_flush();
+        vtss_cmd_stop_async();
+        /* set defaults for next session */
+        cpumask_copy(&vtss_procfs_cpumask_, cpu_present_mask);
+        vtss_procfs_defsav_ = 0;
+    }
+    vtss_cmd_close();
+    /* Restore default priority for trace reader */
+    set_user_nice(current, 0);
+    return 0;
+}
+
+static const struct file_operations vtss_procfs_ctrl_fops = {
+    .owner   = THIS_MODULE,
+    .read    = vtss_procfs_ctrl_read,
+    .write   = vtss_procfs_ctrl_write,
+    .open    = vtss_procfs_ctrl_open,
+    .release = vtss_procfs_ctrl_close,
+    .poll    = vtss_procfs_ctrl_poll,
+};
+
+/* ************************************************************************* */
+
+static void *debug_info = NULL;
+
+static int vtss_procfs_debug_show(struct seq_file *s, void *v)
+{
+    return vtss_debug_info(s);
+}
+
+static void *vtss_procfs_debug_start(struct seq_file *s, loff_t *pos)
+{
+    return (*pos) ? NULL : &debug_info;
+}
+
+static void *vtss_procfs_debug_next(struct seq_file *s, void *v, loff_t *pos)
+{
+    return NULL;
+}
+
+static void vtss_procfs_debug_stop(struct seq_file *s, void *v)
+{
+}
+
+static const struct seq_operations vtss_procfs_debug_sops = {
+    .start = vtss_procfs_debug_start,
+    .next  = vtss_procfs_debug_next,
+    .stop  = vtss_procfs_debug_stop,
+    .show  = vtss_procfs_debug_show,
+};
+
+static int vtss_procfs_debug_open(struct inode *inode, struct file *file)
+{
+    return seq_open(file, &vtss_procfs_debug_sops);
+}
+
+static const struct file_operations vtss_procfs_debug_fops = {
+    .owner   = THIS_MODULE,
+    .open    = vtss_procfs_debug_open,
+    .read    = seq_read,
+    .llseek  = seq_lseek,
+    .release = seq_release,
+};
+
+/* ************************************************************************* */
+
+const struct cpumask* vtss_procfs_cpumask(void)
+{
+    return &vtss_procfs_cpumask_;
+}
+
+static ssize_t vtss_procfs_cpumask_read(struct file* file, char __user* buf, size_t size, loff_t* ppos)
+{
+    ssize_t rc = 0;
+
+    if (*ppos == 0) {
+        char *page = (char*)vtss_get_free_page(GFP_KERNEL);
+
+        /* buf currently PAGE_SIZE, need 9 chars per 32 bits. */
+        BUILD_BUG_ON((NR_CPUS/32 * 9) > (PAGE_SIZE-1));
+
+        if (page == NULL)
+            return -ENOMEM;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,0,0)
+        rc = cpulist_scnprintf(page, PAGE_SIZE-2, &vtss_procfs_cpumask_);
+#else
+        rc = scnprintf(page, PAGE_SIZE-2, "%*pbl", cpumask_pr_args(&vtss_procfs_cpumask_));
+#endif
+        page[rc++] = '\n';
+        page[rc]   = '\0';
+        *ppos += rc;
+        if (rc <= size) {
+            if (copy_to_user(buf, page, rc)) {
+                rc = -EFAULT;
+            }
+        } else {
+            rc = -EINVAL;
+        }
+        vtss_free_page((unsigned long)page);
+    }
+    return rc;
+}
+
+static ssize_t vtss_procfs_cpumask_write(struct file *file, const char __user * buf, size_t count, loff_t * ppos)
+{
+    ssize_t rc = -EINVAL;
+    cpumask_var_t new_value;
+
+    if (!alloc_cpumask_var(&new_value, (GFP_KERNEL | __GFP_NOWARN)))
+        return -ENOMEM;
+    if (!cpumask_parselist_user(buf, count, new_value)) {
+        cpumask_and(&vtss_procfs_cpumask_, new_value, cpu_present_mask);
+        rc = count;
+    }
+    free_cpumask_var(new_value);
+    return rc;
+}
+
+static int vtss_procfs_cpumask_open(struct inode *inode, struct file *file)
+{
+    /* TODO: ... */
+    return 0;
+}
+
+static int vtss_procfs_cpumask_close(struct inode *inode, struct file *file)
+{
+    /* TODO: ... */
+    return 0;
+}
+
+static const struct file_operations vtss_procfs_cpumask_fops = {
+    .owner   = THIS_MODULE,
+    .read    = vtss_procfs_cpumask_read,
+    .write   = vtss_procfs_cpumask_write,
+    .open    = vtss_procfs_cpumask_open,
+    .release = vtss_procfs_cpumask_close,
+};
+
+/* ************************************************************************* */
+
+int vtss_procfs_defsav(void)
+{
+    return vtss_procfs_defsav_;
+}
+
+static ssize_t vtss_procfs_defsav_read(struct file* file, char __user* buf, size_t size, loff_t* ppos)
+{
+    ssize_t rc = 0;
+
+    if (*ppos == 0) {
+        char buff[32]; /* enough for <int> */
+        rc = snprintf(buff, sizeof(buff)-2, "%d", vtss_procfs_defsav_);
+        rc = (rc < 0) ? 0 : rc;
+        buff[rc++] = '\n';
+        buff[rc]   = '\0';
+        *ppos += rc;
+        if (rc <= size) {
+            if (copy_to_user(buf, buff, rc)) {
+                rc = -EFAULT;
+            }
+        } else {
+            rc = -EINVAL;
+        }
+    }
+    return rc;
+}
+
+static ssize_t vtss_procfs_defsav_write(struct file *file, const char __user * buf, size_t count, loff_t * ppos)
+{
+    char chr;
+    size_t i;
+    unsigned long val = 0;
+
+    for (i = 0; i < count; i++, buf += sizeof(char)) {
+        if (get_user(chr, buf))
+            return -EFAULT;
+        if (chr >= '0' && chr <= '9') {
+            val = val * 10 + (chr - '0');
+        } else
+            break;
+    }
+    vtss_procfs_defsav_ = (int)(val < 1000000)    ? 1000000    : val;
+    vtss_procfs_defsav_ = (int)(val > 2000000000) ? 2000000000 : val;
+    return count;
+}
+
+static int vtss_procfs_defsav_open(struct inode *inode, struct file *file)
+{
+    /* TODO: ... */
+    return 0;
+}
+
+static int vtss_procfs_defsav_close(struct inode *inode, struct file *file)
+{
+    /* TODO: ... */
+    return 0;
+}
+
+static const struct file_operations vtss_procfs_defsav_fops = {
+    .owner   = THIS_MODULE,
+    .read    = vtss_procfs_defsav_read,
+    .write   = vtss_procfs_defsav_write,
+    .open    = vtss_procfs_defsav_open,
+    .release = vtss_procfs_defsav_close,
+};
+
+/* ************************************************************************* */
+
+static void *targets_info = NULL;
+
+static int vtss_procfs_targets_show(struct seq_file *s, void *v)
+{
+    return vtss_target_pids(s);
+}
+
+static void *vtss_procfs_targets_start(struct seq_file *s, loff_t *pos)
+{
+    return (*pos) ? NULL : &targets_info;
+}
+
+static void *vtss_procfs_targets_next(struct seq_file *s, void *v, loff_t *pos)
+{
+    return NULL;
+}
+
+static void vtss_procfs_targets_stop(struct seq_file *s, void *v)
+{
+}
+
+static const struct seq_operations vtss_procfs_targets_sops = {
+    .start = vtss_procfs_targets_start,
+    .next  = vtss_procfs_targets_next,
+    .stop  = vtss_procfs_targets_stop,
+    .show  = vtss_procfs_targets_show,
+};
+
+static int vtss_procfs_targets_open(struct inode *inode, struct file *file)
+{
+    return seq_open(file, &vtss_procfs_targets_sops);
+}
+
+static const struct file_operations vtss_procfs_targets_fops = {
+    .owner   = THIS_MODULE,
+    .open    = vtss_procfs_targets_open,
+    .read    = seq_read,
+    .llseek  = seq_lseek,
+    .release = seq_release,
+};
+
+/* ************************************************************************* */
+
+static ssize_t vtss_procfs_timesrc_read(struct file* file, char __user* buf, size_t size, loff_t* ppos)
+{
+    ssize_t rc = 0;
+
+    if (*ppos == 0) {
+        char buff[8]; /* enough for "tsc" or "sys" string */
+        rc = snprintf(buff, sizeof(buff)-2, "%s", vtss_time_source ? "tsc" : "sys");
+        rc = (rc < 0) ? 0 : rc;
+        buff[rc++] = '\n';
+        buff[rc]   = '\0';
+        *ppos += rc;
+        if (rc <= size) {
+            if (copy_to_user(buf, buff, rc)) {
+                rc = -EFAULT;
+            }
+        } else {
+            rc = -EINVAL;
+        }
+    }
+    return rc;
+}
+
+static ssize_t vtss_procfs_timesrc_write(struct file *file, const char __user * buf, size_t count, loff_t * ppos)
+{
+    char val[8];
+
+    if (count < 3 || vtss_copy_from_user(val, buf, 3)) {
+        ERROR("Error in copy_from_user()");
+        return -EFAULT;
+    }
+    val[3] = '\0';
+    if (!strncmp(val, "tsc", 3)) {
+        if (check_tsc_unstable()){
+            ERROR("TSC timesource is unstable. Switching to system time...");
+            //TODO: It's better to return error for the case.This change require testing on systems with TSC reliable and not reliable.
+        } else {
+            vtss_time_source = 1;
+        }
+    }
+    if (!strncmp(val, "sys", 3))
+        vtss_time_source = 0;
+    TRACE("time source=%s", vtss_time_source ? "tsc" : "sys");
+    return count;
+}
+
+static int vtss_procfs_timesrc_open(struct inode *inode, struct file *file)
+{
+    /* TODO: ... */
+    return 0;
+}
+
+static int vtss_procfs_timesrc_close(struct inode *inode, struct file *file)
+{
+    /* TODO: ... */
+    return 0;
+}
+
+static const struct file_operations vtss_procfs_timesrc_fops = {
+    .owner   = THIS_MODULE,
+    .read    = vtss_procfs_timesrc_read,
+    .write   = vtss_procfs_timesrc_write,
+    .open    = vtss_procfs_timesrc_open,
+    .release = vtss_procfs_timesrc_close,
+};
+
+/* ************************************************************************* */
+
+static ssize_t vtss_procfs_timelimit_read(struct file* file, char __user* buf, size_t size, loff_t* ppos)
+{
+    ssize_t rc = 0;
+
+    if (*ppos == 0) {
+        char buff[32]; /* enough for <unsigned long long> */
+        rc = snprintf(buff, sizeof(buff)-2, "%llu", vtss_time_limit);
+        rc = (rc < 0) ? 0 : rc;
+        buff[rc++] = '\n';
+        buff[rc]   = '\0';
+        *ppos += rc;
+        if (rc <= size) {
+            if (copy_to_user(buf, buff, rc)) {
+                rc = -EFAULT;
+            }
+        } else {
+            rc = -EINVAL;
+        }
+    }
+    return rc;
+}
+
+static ssize_t vtss_procfs_timelimit_write(struct file *file, const char __user * buf, size_t count, loff_t * ppos)
+{
+    char chr;
+    size_t i;
+    unsigned long long val = 0;
+
+    for (i = 0; i < count; i++, buf += sizeof(char)) {
+        if (get_user(chr, buf))
+            return -EFAULT;
+        if (chr >= '0' && chr <= '9') {
+            val = val * 10ULL + (chr - '0');
+        } else
+            break;
+    }
+    vtss_time_limit = (cycles_t)val;
+    TRACE("vtss_time_limit=%llu", vtss_time_limit);
+    return count;
+}
+
+static int vtss_procfs_timelimit_open(struct inode *inode, struct file *file)
+{
+    /* TODO: ... */
+    return 0;
+}
+
+static int vtss_procfs_timelimit_close(struct inode *inode, struct file *file)
+{
+    /* TODO: ... */
+    return 0;
+}
+
+static const struct file_operations vtss_procfs_timelimit_fops = {
+    .owner   = THIS_MODULE,
+    .read    = vtss_procfs_timelimit_read,
+    .write   = vtss_procfs_timelimit_write,
+    .open    = vtss_procfs_timelimit_open,
+    .release = vtss_procfs_timelimit_close,
+};
+
+/* ************************************************************************* */
+
+static void vtss_procfs_rmdir(void)
+{
+    if (vtss_procfs_root != NULL) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+        if (atomic_read(&vtss_procfs_root->count) == 1) {
+            remove_proc_entry(THIS_MODULE->name, NULL);
+#else
+        remove_proc_subtree(THIS_MODULE->name, NULL);
+#endif
+            vtss_procfs_root = NULL;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+        } else {
+            ERROR("Entry '%s' is busy", vtss_procfs_path());
+        }
+#endif
+    }
+}
+
+static int vtss_procfs_mkdir(void)
+{
+    struct path path;
+
+    if (vtss_procfs_root == NULL) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+        if (kern_path(vtss_procfs_path(), 0, &path)) {
+            /* doesn't exist, so create it */
+            vtss_procfs_root = proc_mkdir(THIS_MODULE->name, NULL);
+        } else {
+            /* if exist, attach to it */
+            vtss_procfs_root = PDE(path.dentry->d_inode);
+            path_put(&path);
+        }
+#else
+        if (kern_path(vtss_procfs_path(), 0, &path) == 0) {
+            /* if exist, remove it */
+            remove_proc_subtree(THIS_MODULE->name, NULL);
+         }
+        /* doesn't exist, so create it */
+        vtss_procfs_root = proc_mkdir(THIS_MODULE->name, NULL);
+#endif
+        if (vtss_procfs_root != NULL) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+#ifdef VTSS_AUTOCONF_PROCFS_OWNER
+            vtss_procfs_root->owner = THIS_MODULE;
+#endif
+#endif
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+        vtss_procfs_root->uid = uid;
+        vtss_procfs_root->gid = gid;
+#else
+#if defined CONFIG_UIDGID_STRICT_TYPE_CHECKS || (LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0))
+       {
+          kuid_t kuid = KUIDT_INIT(uid);
+          kgid_t kgid = KGIDT_INIT(gid);
+          proc_set_user(vtss_procfs_root, kuid, kgid);
+       }
+#else
+          proc_set_user(vtss_procfs_root, uid, gid);
+#endif
+#endif
+        }
+    }
+    return (vtss_procfs_root != NULL) ? 0 : -ENOENT;
+}
+
+void vtss_procfs_fini(void)
+{
+    if (atomic_read(&vtss_procfs_attached)) {
+        ERROR("procfs is still attached");
+    }
+    vtss_procfs_ctrl_flush();
+    if (vtss_procfs_root != NULL) {
+        remove_proc_entry(VTSS_PROCFS_CTRL_NAME,      vtss_procfs_root);
+        remove_proc_entry(VTSS_PROCFS_DEBUG_NAME,     vtss_procfs_root);
+        remove_proc_entry(VTSS_PROCFS_CPUMASK_NAME,   vtss_procfs_root);
+        remove_proc_entry(VTSS_PROCFS_DEFSAV_NAME,    vtss_procfs_root);
+        remove_proc_entry(VTSS_PROCFS_TARGETS_NAME,   vtss_procfs_root);
+        remove_proc_entry(VTSS_PROCFS_TIMESRC_NAME,   vtss_procfs_root);
+        remove_proc_entry(VTSS_PROCFS_TIMELIMIT_NAME, vtss_procfs_root);
+        vtss_procfs_rmdir();
+    }
+}
+
+static int vtss_procfs_create_entry(const char* name, const struct file_operations* fops)
+{
+    struct proc_dir_entry *pde = proc_create(name, (mode_t)(mode ? (mode & 0666) : 0660), vtss_procfs_root, fops);
+    if (pde == NULL) {
+        ERROR("Could not create '%s/%s'", vtss_procfs_path(), name);
+        vtss_procfs_fini();
+        return -ENOENT;
+    }
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+// 3 lines below not supported anymore
+#ifdef VTSS_AUTOCONF_PROCFS_OWNER
+    pde->owner = THIS_MODULE;
+#endif
+#endif
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+    pde->uid = uid;
+    pde->gid = gid;
+#else
+#if defined CONFIG_UIDGID_STRICT_TYPE_CHECKS || (LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0))
+{
+      kuid_t kuid = KUIDT_INIT(uid);
+      kgid_t kgid = KGIDT_INIT(gid);
+      proc_set_user(pde, kuid, kgid);
+}
+#else
+          proc_set_user(pde, uid, gid);
+#endif
+#endif
+    return 0;
+}
+
+int vtss_procfs_init(void)
+{
+    int rc = 0;
+    unsigned long flags;
+
+    atomic_set(&vtss_procfs_attached, 0);
+    vtss_spin_lock_irqsave(&vtss_procfs_ctrl_list_lock, flags);
+    INIT_LIST_HEAD(&vtss_procfs_ctrl_list);
+    vtss_spin_unlock_irqrestore(&vtss_procfs_ctrl_list_lock, flags);
+    cpumask_copy(&vtss_procfs_cpumask_, cpu_present_mask);
+    vtss_procfs_defsav_ = 0;
+
+    if (vtss_procfs_mkdir()) {
+        ERROR("Could not create or find root directory '%s'", vtss_procfs_path());
+        return -ENOENT;
+    }
+    rc |= vtss_procfs_create_entry(VTSS_PROCFS_CTRL_NAME,      &vtss_procfs_ctrl_fops);
+    rc |= vtss_procfs_create_entry(VTSS_PROCFS_DEBUG_NAME,     &vtss_procfs_debug_fops);
+    rc |= vtss_procfs_create_entry(VTSS_PROCFS_CPUMASK_NAME,   &vtss_procfs_cpumask_fops);
+    rc |= vtss_procfs_create_entry(VTSS_PROCFS_DEFSAV_NAME,    &vtss_procfs_defsav_fops);
+    rc |= vtss_procfs_create_entry(VTSS_PROCFS_TARGETS_NAME,   &vtss_procfs_targets_fops);
+    rc |= vtss_procfs_create_entry(VTSS_PROCFS_TIMESRC_NAME,   &vtss_procfs_timesrc_fops);
+    rc |= vtss_procfs_create_entry(VTSS_PROCFS_TIMELIMIT_NAME, &vtss_procfs_timelimit_fops);
+    return rc;
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/record.c b/drivers/misc/intel/sepdk/vtsspp/record.c
new file mode 100644
index 000000000000..a71b0e8e8bc9
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/record.c
@@ -0,0 +1,834 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "record.h"
+#include "globals.h"
+#include "time.h"
+#include "cpuevents.h"
+
+#include <linux/sched.h>
+#include <linux/math64.h>
+#include <linux/delay.h> // for msleep_interruptible()
+
+#define VTSS_PROCESS_NAME_LEN   0x10
+
+int vtss_record_magic(struct vtss_transport_data* trnd, int is_safe)
+{
+#ifdef VTSS_USE_UEC
+    static unsigned int marker[2] = { UEC_MAGIC, UEC_MAGICVALUE };
+    return vtss_transport_record_write(trnd, marker, sizeof(marker), NULL, 0, is_safe);
+#else
+    int rc = -EFAULT;
+    void* entry;
+    unsigned int* p = (unsigned int*)vtss_transport_record_reserve(trnd, &entry, 2*sizeof(unsigned int));
+    if (likely(p)) {
+        *p++ = UEC_MAGIC;
+        *p++ = UEC_MAGICVALUE;
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+#endif
+}
+
+int vtss_record_debug_info(struct vtss_transport_data* trnd, const char* message, int is_safe)
+{
+    int rc = 0;
+    if (likely(message != NULL)) {
+        size_t msglen = strlen(message) + 1;
+#ifdef VTSS_USE_UEC
+        debug_info_record_t dbgrec;
+        dbgrec.flagword = UEC_LEAF1 | UECL1_USERTRACE;
+        dbgrec.size = msglen + sizeof(dbgrec.size) + sizeof(dbgrec.type);
+        dbgrec.type = UECSYSTRACE_DEBUG;
+        rc = vtss_transport_record_write(trnd, &dbgrec, sizeof(dbgrec), (void*)message, msglen, is_safe);
+#else
+        void* entry;
+        debug_info_record_t* dbgrec = (debug_info_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(debug_info_record_t) + msglen);
+        if (likely(dbgrec)) {
+            dbgrec->flagword = UEC_LEAF1 | UECL1_USERTRACE;
+            dbgrec->size = msglen + sizeof(dbgrec->size) + sizeof(dbgrec->type);
+            dbgrec->type = UECSYSTRACE_DEBUG;
+            memcpy(++dbgrec, message, msglen);
+            rc = vtss_transport_record_commit(trnd, entry, is_safe);
+        }
+#endif
+    }
+    return rc;
+}
+
+int vtss_record_process_exec(struct vtss_transport_data* trnd, pid_t tid, pid_t pid, int cpu, const char *filename, int is_safe)
+{
+    size_t namelen = filename ? strlen(filename) + 1 : 0;
+#ifdef VTSS_USE_UEC
+    prc_trace_record_t procrec;
+    procrec.flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_CPUIDX | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC | UECL1_SYSTRACE;
+    procrec.activity = UECACT_NEWTASK;
+    procrec.cpuidx   = cpu;
+    procrec.pid      = pid;
+    procrec.tid      = tid;
+    vtss_time_get_sync(&procrec.cputsc, &procrec.realtsc);
+    procrec.size     = namelen + sizeof(procrec.size) + sizeof(procrec.type);
+    procrec.type     = UECSYSTRACE_PROCESS_NAME;
+    return vtss_transport_record_write(trnd, &procrec, sizeof(procrec), (void*)filename, namelen, is_safe);
+#else
+    int rc = -EFAULT;
+    void* entry;
+    prc_trace_record_t* procrec = (prc_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(prc_trace_record_t) + namelen);
+    if (likely(procrec)) {
+        procrec->flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_CPUIDX | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC | UECL1_SYSTRACE;
+        procrec->activity = UECACT_NEWTASK;
+        procrec->cpuidx   = cpu;
+        procrec->pid      = pid;
+        procrec->tid      = tid;
+        vtss_time_get_sync(&procrec->cputsc, &procrec->realtsc);
+        procrec->size     = namelen + sizeof(procrec->size) + sizeof(procrec->type);
+        procrec->type     = UECSYSTRACE_PROCESS_NAME;
+        if (likely(namelen))
+            memcpy(++procrec, filename, namelen);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+#endif
+}
+
+int vtss_record_process_exit(struct vtss_transport_data* trnd, pid_t tid, pid_t pid, int cpu, const char *filename, int is_safe)
+{
+    size_t namelen = filename ? strlen(filename) + 1 : 0;
+
+#ifdef VTSS_USE_UEC
+    prc_trace_record_t procrec;
+    procrec.flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_CPUIDX | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC | UECL1_SYSTRACE;
+    procrec.activity = UECACT_OLDTASK;
+    procrec.cpuidx   = cpu;
+    procrec.pid      = pid;
+    procrec.tid      = tid;
+    vtss_time_get_sync(&procrec.cputsc, &procrec.realtsc);
+    procrec.size     = namelen + sizeof(procrec.size) + sizeof(procrec.type);
+    procrec.type     = UECSYSTRACE_PROCESS_NAME;
+    return vtss_transport_record_write(trnd, &procrec, sizeof(procrec), (void*)filename, namelen, is_safe);
+#else
+    int rc = -EFAULT;
+    void* entry;
+    prc_trace_record_t* procrec = (prc_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(prc_trace_record_t) + namelen);
+    if (likely(procrec)) {
+        procrec->flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_CPUIDX | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC | UECL1_SYSTRACE;
+        procrec->activity = UECACT_OLDTASK;
+        procrec->cpuidx   = cpu;
+        procrec->pid      = pid;
+        procrec->tid      = tid;
+        vtss_time_get_sync(&procrec->cputsc, &procrec->realtsc);
+        procrec->size     = namelen + sizeof(procrec->size) + sizeof(procrec->type);
+        procrec->type     = UECSYSTRACE_PROCESS_NAME;
+        if (likely(namelen))
+            memcpy(++procrec, filename, namelen);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+#endif
+}
+
+int vtss_record_thread_create(struct vtss_transport_data* trnd, pid_t tid, pid_t pid, int cpu, int is_safe)
+{
+#ifdef VTSS_USE_UEC
+    nth_trace_record_t threc;
+    threc.flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC;
+    threc.activity = UECACT_NEWTASK;
+    threc.residx   = tid;
+    threc.cpuidx   = cpu;
+    threc.pid      = pid;
+    threc.tid      = tid;
+    vtss_time_get_sync(&threc.cputsc, &threc.realtsc);
+    return vtss_transport_record_write(trnd, &threc, sizeof(threc), NULL, 0, is_safe);
+#else
+    int rc = -EFAULT;
+    void* entry;
+    nth_trace_record_t* threc = (nth_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(nth_trace_record_t));
+    if (likely(threc)) {
+        threc->flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC;
+        threc->activity = UECACT_NEWTASK;
+        threc->residx   = tid;
+        threc->cpuidx   = cpu;
+        threc->pid      = pid;
+        threc->tid      = tid;
+        vtss_time_get_sync(&threc->cputsc, &threc->realtsc);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+#endif
+}
+
+int vtss_record_thread_stop(struct vtss_transport_data* trnd, pid_t tid, pid_t pid, int cpu, int is_safe)
+{
+#ifdef VTSS_USE_UEC
+    nth_trace_record_t threc;
+    threc.flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC;
+    threc.activity = UECACT_OLDTASK;
+    threc.residx   = tid;
+    threc.cpuidx   = cpu;
+    threc.pid      = pid;
+    threc.tid      = tid;
+    vtss_time_get_sync(&threc.cputsc, &threc.realtsc);
+    return vtss_transport_record_write(trnd, &threc, sizeof(threc), NULL, 0, is_safe);
+#else
+    int rc = -EFAULT;
+    void* entry;
+    nth_trace_record_t* threc = (nth_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(nth_trace_record_t));
+    if (likely(threc)) {
+        threc->flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC;
+        threc->activity = UECACT_OLDTASK;
+        threc->residx   = tid;
+        threc->cpuidx   = cpu;
+        threc->pid      = pid;
+        threc->tid      = tid;
+        vtss_time_get_sync(&threc->cputsc, &threc->realtsc);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+#endif
+}
+
+int vtss_record_switch_from(struct vtss_transport_data* trnd, int cpu, int is_preempt, int is_safe, unsigned long* chunk_id)
+{
+#ifdef VTSS_USE_UEC
+    cto_trace_record_t ctxrec;
+    ctxrec.sysout.flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_REALTSC;
+    ctxrec.sysout.activity = (is_preempt ? 0 : UECACT_SYNCHRO) | UECACT_SWITCHFROM;
+    ctxrec.sysout.cpuidx   = cpu;
+    vtss_time_get_sync(&ctxrec.sysout.cputsc, &ctxrec.sysout.realtsc);
+    return vtss_transport_record_write(trnd, &ctxrec, sizeof(ctxrec.sysout), NULL, 0, is_safe);
+#else
+    int rc = -EFAULT;
+    void* entry;
+    cto_trace_record_t* ctxrec = (cto_trace_record_t*)vtss_transport_activity_record_reserve(trnd, &entry, sizeof(ctxrec->sysout), chunk_id);
+    if (likely(ctxrec)) {
+        ctxrec->sysout.flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_REALTSC;
+        ctxrec->sysout.activity = (is_preempt ? 0 : UECACT_SYNCHRO) | UECACT_SWITCHFROM;
+        ctxrec->sysout.cpuidx   = cpu;
+        vtss_time_get_sync(&ctxrec->sysout.cputsc, &ctxrec->sysout.realtsc);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+#endif
+}
+
+int vtss_record_switch_to(struct vtss_transport_data* trnd, pid_t tid, int cpu, void* ip, int is_safe, unsigned long* chunk_id)
+{
+#ifdef VTSS_USE_UEC
+    cti_trace_record_t ctxrec;
+    ctxrec.procina.flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_REALTSC | UECL1_EXECADDR;
+    ctxrec.procina.activity = UECACT_SWITCHTO;
+    ctxrec.procina.residx   = tid;
+    ctxrec.procina.cpuidx   = cpu;
+    vtss_time_get_sync(&ctxrec.procina.cputsc, &ctxrec.procina.realtsc);
+    ctxrec.procina.execaddr = (unsigned long long)(size_t)ip;
+    return vtss_transport_record_write(trnd, &ctxrec, sizeof(ctxrec.procina), NULL, 0, is_safe);
+#else
+    int rc = -EFAULT;
+    void* entry;
+    cti_trace_record_t* ctxrec = (cti_trace_record_t*)vtss_transport_activity_record_reserve(trnd, &entry, sizeof(ctxrec->procina), chunk_id);
+    if (likely(ctxrec)) {
+        ctxrec->procina.flagword = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_REALTSC | UECL1_EXECADDR;
+        ctxrec->procina.activity = UECACT_SWITCHTO;
+        ctxrec->procina.residx   = tid;
+        ctxrec->procina.cpuidx   = cpu;
+        vtss_time_get_sync(&ctxrec->procina.cputsc, &ctxrec->procina.realtsc);
+        ctxrec->procina.execaddr = (unsigned long long)(size_t)ip;
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+#endif
+}
+
+#define VTSS_MAX_ACTIVE_CPUEVENTS VTSS_CFG_CHAIN_SIZE/10
+
+int vtss_record_sample(struct vtss_transport_data* trnd, pid_t tid, int cpu, cpuevent_t* cpuevent_chain, void* ip, int is_safe, unsigned long* chunk_id)
+{
+    int i, j, rc = -EFAULT;
+    event_trace_record_t eventrec;
+
+#ifdef VTSS_USE_UEC
+    unsigned long flags;
+    unsigned long long* scratch;
+
+    local_irq_save(flags);
+    scratch = (unsigned long long*)pcb_cpu.scratch_ptr;
+    for (i = 0, j = 0; i < VTSS_CFG_CHAIN_SIZE; i++) {
+        if (!cpuevent_chain[i].valid) {
+            break;
+        }
+        if (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx) {
+            continue;
+        }
+        scratch[j++] = cpuevent_chain[i].count;
+        if (j >= VTSS_MAX_ACTIVE_CPUEVENTS) {
+            ERROR("MAX active cpuevents is reached");
+            break;
+        }
+    }
+    scratch[j] = (unsigned long long)(size_t)ip;
+    if (ip != NULL) {
+        eventrec.sperec.flagword = UEC_VECTORED | UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX |
+            UECL1_CPUIDX | UECL1_CPUTSC | UECL1_MUXGROUP | UECL1_CPUEVENT | UECL1_EXECADDR;
+        eventrec.sperec.vectored = UECL1_CPUEVENT;
+        eventrec.sperec.activity = UECACT_SAMPLED;
+        eventrec.sperec.residx   = tid;
+        eventrec.sperec.cpuidx   = cpu;
+        eventrec.sperec.cputsc   = vtss_time_cpu();
+        eventrec.sperec.muxgroup = cpuevent_chain[0].mux_idx;
+        eventrec.sperec.event_no = j;
+        rc = vtss_transport_record_write(trnd, &eventrec.sperec, sizeof(eventrec.sperec), scratch, (j+1)*sizeof(unsigned long long), is_safe);
+    } else {
+        eventrec.gperec.flagword = UEC_VECTORED | UEC_LEAF1 | UECL1_VRESIDX |
+            UECL1_CPUIDX | UECL1_CPUTSC | UECL1_MUXGROUP | UECL1_CPUEVENT;
+        eventrec.gperec.vectored = UECL1_CPUEVENT;
+        eventrec.gperec.residx   = tid;
+        eventrec.gperec.cpuidx   = cpu;
+        eventrec.gperec.cputsc   = vtss_time_cpu();
+        eventrec.gperec.muxgroup = cpuevent_chain[0].mux_idx;
+        eventrec.gperec.event_no = j;
+        rc = vtss_transport_record_write(trnd, &eventrec.gperec, sizeof(eventrec.gperec), scratch, j*sizeof(unsigned long long), is_safe);
+    }
+    local_irq_restore(flags);
+#else
+    int n;
+    unsigned long long* counters;
+
+    for (i = 0, n = 0; i < VTSS_CFG_CHAIN_SIZE; i++) {
+        if (!cpuevent_chain[i].valid) {
+            break;
+        }
+        if (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx) {
+            continue;
+        }
+        if (++n >= VTSS_MAX_ACTIVE_CPUEVENTS) {
+            ERROR("MAX active cpuevents is reached");
+            break;
+        }
+    }
+    if (ip != NULL) {
+        void* entry;
+        event_trace_record_t* eventrec = (event_trace_record_t*)vtss_transport_activity_record_reserve(trnd, &entry, sizeof(eventrec->sperec) + (n+1)*sizeof(unsigned long long), chunk_id);
+        if (likely(eventrec)) {
+            eventrec->sperec.flagword = UEC_VECTORED | UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX |
+                UECL1_CPUIDX | UECL1_CPUTSC | UECL1_MUXGROUP | UECL1_CPUEVENT | UECL1_EXECADDR;
+            eventrec->sperec.vectored = UECL1_CPUEVENT;
+            eventrec->sperec.activity = UECACT_SAMPLED;
+            eventrec->sperec.residx   = tid;
+            eventrec->sperec.cpuidx   = cpu;
+            eventrec->sperec.cputsc   = vtss_time_cpu();
+            eventrec->sperec.muxgroup = cpuevent_chain[0].mux_idx;
+            eventrec->sperec.event_no = n;
+
+            counters = (unsigned long long*)((char*)eventrec+sizeof(eventrec->sperec));
+            for (i = 0, j = 0; i < VTSS_CFG_CHAIN_SIZE; i++) {
+                if (!cpuevent_chain[i].valid) {
+                    break;
+                }
+                if (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx) {
+                    continue;
+                }
+                counters[j++] = cpuevent_chain[i].count;
+                if (j >= VTSS_MAX_ACTIVE_CPUEVENTS) {
+                    break;
+                }
+            }
+            counters[j] = (unsigned long long)(size_t)ip;
+            rc = vtss_transport_record_commit(trnd, entry, is_safe);
+        }
+    } else {
+        void* entry;
+        event_trace_record_t* eventrec = (event_trace_record_t*)vtss_transport_activity_record_reserve(trnd, &entry, sizeof(eventrec->gperec) + n*sizeof(unsigned long long), chunk_id);
+        if (likely(eventrec)) {
+            eventrec->gperec.flagword = UEC_VECTORED | UEC_LEAF1 | UECL1_VRESIDX |
+                UECL1_CPUIDX | UECL1_CPUTSC | UECL1_MUXGROUP | UECL1_CPUEVENT;
+            eventrec->gperec.vectored = UECL1_CPUEVENT;
+            eventrec->gperec.residx   = tid;
+            eventrec->gperec.cpuidx   = cpu;
+            eventrec->gperec.cputsc   = vtss_time_cpu();
+            eventrec->gperec.muxgroup = cpuevent_chain[0].mux_idx;
+            eventrec->gperec.event_no = n;
+
+            counters = (unsigned long long*)((char*)eventrec+sizeof(eventrec->gperec));
+            for (i = 0, j = 0; i < VTSS_CFG_CHAIN_SIZE; i++) {
+                if (!cpuevent_chain[i].valid) {
+                    break;
+                }
+                if (cpuevent_chain[i].mux_grp != cpuevent_chain[i].mux_idx) {
+                    continue;
+                }
+                counters[j++] = cpuevent_chain[i].count;
+                if (j >= VTSS_MAX_ACTIVE_CPUEVENTS) {
+                    break;
+                }
+            }
+            rc = vtss_transport_record_commit(trnd, entry, is_safe);
+        }
+    }
+#endif
+    return rc;
+}
+
+int vtss_record_bts(struct vtss_transport_data* trnd, pid_t tid, int cpu, void* bts_buff, size_t bts_size, int is_safe)
+{
+#ifdef VTSS_USE_UEC
+    bts_trace_record_t btsrec;
+
+    if (bts_size >= ((unsigned short)~0)-4)
+        return -1;
+    /// generate branch trace record
+    /// [flagword][residx][cpuidx][tsc][systrace(bts)]
+    btsrec.flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_SYSTRACE;
+    btsrec.residx   = tid;
+    btsrec.cpuidx   = cpu;
+    btsrec.cputsc   = vtss_time_cpu();
+    btsrec.size     = (unsigned short)(bts_size + sizeof(btsrec.size) + sizeof(btsrec.type));
+    btsrec.type     = UECSYSTRACE_BRANCH_V0;
+    return vtss_transport_record_write(trnd, &btsrec, sizeof(bts_trace_record_t), bts_buff, bts_size, is_safe);
+#else
+    int rc = -EFAULT;
+    void* entry;
+    bts_trace_record_t* btsrec;
+
+    if (bts_size >= ((unsigned short)~0)-4)
+        return rc;
+    btsrec = (bts_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(bts_trace_record_t) + bts_size);
+    if (likely(btsrec)) {
+        /// generate branch trace record
+        /// [flagword][residx][cpuidx][tsc][systrace(bts)]
+        btsrec->flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_SYSTRACE;
+        btsrec->residx   = tid;
+        btsrec->cpuidx   = cpu;
+        btsrec->cputsc   = vtss_time_cpu();
+        btsrec->size     = (unsigned short)(bts_size + sizeof(btsrec->size) + sizeof(btsrec->type));
+        btsrec->type     = UECSYSTRACE_BRANCH_V0;
+        memcpy(++btsrec, bts_buff, bts_size);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+#endif
+}
+
+int vtss_record_module(struct vtss_transport_data* trnd, int m32, unsigned long addr, unsigned long size, const char *pname, unsigned long pgoff, long long cputsc, long long realtsc, int is_safe)
+{
+#ifdef VTSS_USE_UEC
+    dlm_trace_record_t modrec;
+
+    modrec.flagword = UEC_LEAF1 | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC | UECL1_SYSTRACE;
+    modrec.pid      = 0;
+    modrec.tid      = 0;
+    //vtss_time_get_sync(&modrec.cputsc, &modrec.realtsc);
+    modrec.cputsc = cputsc;
+    modrec.realtsc = realtsc;
+    modrec.start  = addr;
+    modrec.end    = addr + size;
+    modrec.offset = pgoff << PAGE_SHIFT;
+    modrec.bin    = (unsigned char)MODTYPE_ELF;
+    modrec.len    = strlen(pname) + 1;
+    modrec.size   = (unsigned short)(sizeof(modrec) - (size_t)((char*)&modrec.size - (char*)&modrec.flagword) + modrec.len);
+
+#ifdef CONFIG_X86_64
+    modrec.type = m32 ? UECSYSTRACE_MODULE_MAP32 : UECSYSTRACE_MODULE_MAP64;
+    /* convert the structure from 64 to 32 bits */
+    if (modrec.type == UECSYSTRACE_MODULE_MAP32) {
+        dlm_trace_record_32_t modrec32;
+
+        modrec32.flagword= modrec.flagword;
+        modrec32.pid     = modrec.pid;
+        modrec32.tid     = modrec.tid;
+        modrec32.cputsc  = modrec.cputsc;
+        modrec32.realtsc = modrec.realtsc;
+        modrec32.type    = modrec.type;
+        modrec32.start   = (unsigned int)modrec.start;
+        modrec32.end     = (unsigned int)modrec.end;
+        modrec32.offset  = (unsigned int)modrec.offset;
+        modrec32.bin     = modrec.bin;
+        modrec32.len     = modrec.len;
+        modrec32.size    = (unsigned short)(sizeof(modrec32) - (size_t)((char*)&modrec32.size - (char*)&modrec32.flagword) + modrec32.len);
+        return vtss_transport_record_write(trnd, &modrec32, sizeof(modrec32), (void*)pname, modrec32.len, is_safe);
+    }
+#else  /* CONFIG_X86_64 */
+    modrec.type = UECSYSTRACE_MODULE_MAP32;
+#endif /* CONFIG_X86_64 */
+    return vtss_transport_record_write(trnd, &modrec, sizeof(modrec), (void*)pname, modrec.len, is_safe);
+#else  /* VTSS_USE_UEC */
+    int rc = -EFAULT;
+    void* entry;
+    size_t namelen = pname ? strlen(pname) + 1 : 0;
+
+#ifdef CONFIG_X86_64
+    if (m32) {
+        dlm_trace_record_32_t* modrec = (dlm_trace_record_32_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(dlm_trace_record_32_t) + namelen);
+        if (unlikely(!modrec)){
+            //If transport is not ready we are in "cmd_start => target_new".
+            //During attach the transport will not attach till client get response from driver,
+            //so, it's no sense to wait for the case
+            if (vtss_transport_is_ready(trnd) && (!irqs_disabled())){
+                //try again
+                int cnt = 20;
+                while ((!modrec) && cnt > 0){
+                    msleep_interruptible(100);
+                    cnt--;
+                    modrec = (dlm_trace_record_32_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(dlm_trace_record_32_t) + namelen);
+                }
+            }
+        }
+        if (likely(modrec)) {
+            modrec->flagword = UEC_LEAF1 | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC | UECL1_SYSTRACE;
+            modrec->pid      = 0;
+            modrec->tid      = 0;
+            //vtss_time_get_sync(&modrec->cputsc, &modrec->realtsc);
+            modrec->cputsc = cputsc;
+            modrec->realtsc = realtsc;
+
+            modrec->start  = (unsigned int)addr;
+            modrec->end    = (unsigned int)(addr + size);
+            modrec->offset = (unsigned int)(pgoff << PAGE_SHIFT);
+            modrec->bin    = (unsigned char)MODTYPE_ELF;
+            modrec->len    = namelen;
+            modrec->size   = (unsigned short)(sizeof(dlm_trace_record_32_t) - (size_t)((char*)&modrec->size - (char*)&modrec->flagword) + namelen);
+            modrec->type   = UECSYSTRACE_MODULE_MAP32;
+
+            if (namelen)
+                memcpy(++modrec, pname, namelen);
+            rc = vtss_transport_record_commit(trnd, entry, is_safe);
+        }
+    } else {
+        dlm_trace_record_t* modrec = (dlm_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(dlm_trace_record_t) + namelen);
+        if (unlikely(!modrec)){
+            if (vtss_transport_is_ready(trnd) && (!irqs_disabled())){
+                //try again
+                int cnt = 20;
+                while ((!modrec) && cnt > 0){
+                    TRACE("record awaiting transport ready");
+                    msleep_interruptible(100);
+                    cnt--;
+                    modrec = (dlm_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(dlm_trace_record_t) + namelen);
+                }
+            }
+        }
+        if (likely(modrec)) {
+            modrec->flagword = UEC_LEAF1 | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC | UECL1_SYSTRACE;
+            modrec->pid      = 0;
+            modrec->tid      = 0;
+//            vtss_time_get_sync(&modrec->cputsc, &modrec->realtsc);
+
+            modrec->cputsc = cputsc;
+            modrec->realtsc = realtsc;
+
+            modrec->start  = addr;
+            modrec->end    = addr + size;
+            modrec->offset = pgoff << PAGE_SHIFT;
+            modrec->bin    = (unsigned char)MODTYPE_ELF;
+            modrec->len    = namelen;
+            modrec->size   = (unsigned short)(sizeof(dlm_trace_record_t) - (size_t)((char*)&modrec->size - (char*)&modrec->flagword) + namelen);
+            modrec->type   = UECSYSTRACE_MODULE_MAP64;
+
+            if (namelen)
+                memcpy(++modrec, pname, namelen);
+            rc = vtss_transport_record_commit(trnd, entry, is_safe);
+        }
+    }
+#else  /* CONFIG_X86_64 */
+    dlm_trace_record_t* modrec = (dlm_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(dlm_trace_record_t) + namelen);
+    if (likely(modrec)) {
+        modrec->flagword = UEC_LEAF1 | UECL1_USRLVLID | UECL1_CPUTSC | UECL1_REALTSC | UECL1_SYSTRACE;
+        modrec->pid      = 0;
+        modrec->tid      = 0;
+  //      vtss_time_get_sync(&modrec->cputsc, &modrec->realtsc);
+        modrec->cputsc = cputsc;
+        modrec->realtsc = realtsc;
+
+
+        modrec->start  = addr;
+        modrec->end    = addr + size;
+        modrec->offset = pgoff << PAGE_SHIFT;
+        modrec->bin    = (unsigned char)MODTYPE_ELF;
+        modrec->len    = namelen;
+        modrec->size   = (unsigned short)(sizeof(dlm_trace_record_t) - (size_t)((char*)&modrec->size - (char*)&modrec->flagword) + namelen);
+        modrec->type   = UECSYSTRACE_MODULE_MAP32;
+
+        if (namelen)
+            memcpy(++modrec, pname, namelen);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+#endif /* CONFIG_X86_64 */
+    return rc;
+#endif /* VTSS_USE_UEC */
+}
+
+int vtss_record_configs(struct vtss_transport_data* trnd, int m32, int is_safe)
+{
+    int rc = 0;
+    unsigned short size;
+
+    /* Fixup maxusr_address */
+#ifdef CONFIG_X86_64
+    hardcfg.maxusr_address = m32 ? IA32_PAGE_OFFSET : PAGE_OFFSET;
+#else
+    hardcfg.maxusr_address = PAGE_OFFSET;
+#endif
+    TRACE("hardcfg.maxusr_address=0x%llx", hardcfg.maxusr_address);
+
+    /* generate forward compatibility format record (always) */
+    {
+        fcf_trace_record_t fcfrec;
+        /// [flagword][systrace(fmtcfg)]
+        fcfrec.flagword = UEC_LEAF1 | UECL1_SYSTRACE;
+        fcfrec.size     = sizeof(fmtcfg) + sizeof(fcfrec.size) + sizeof(fcfrec.type);
+        fcfrec.type     = UECSYSTRACE_FMTCFG;
+        rc |= vtss_transport_record_write(trnd, &fcfrec, sizeof(fcfrec), (void*)fmtcfg, sizeof(fmtcfg), is_safe);
+    }
+
+    /* generate collector configuration record */
+    {
+        static const char colname[] = "vtss++ (" VTSS_TO_STR(VTSS_VERSION_STRING) ")";
+        col_trace_record_t colrec;
+        /// [flagword][systrace(colcfg)]
+        colrec.flagword = UEC_LEAF1 | UECL1_SYSTRACE;
+        colrec.size     = sizeof(col_trace_record_t) - sizeof(colrec.flagword) + sizeof(colname);
+        colrec.type     = UECSYSTRACE_COLCFG;
+
+        colrec.version  = 1;
+        colrec.major    = VTSS_VERSION_MAJOR;
+        colrec.minor    = VTSS_VERSION_MINOR;
+        colrec.revision = VTSS_VERSION_REVISION;
+        colrec.features =
+            VTSS_CFGTRACE_CPUEV  | VTSS_CFGTRACE_SWCFG  |
+            VTSS_CFGTRACE_HWCFG  | VTSS_CFGTRACE_SAMPLE  | VTSS_CFGTRACE_TP     |
+            VTSS_CFGTRACE_MODULE | VTSS_CFGTRACE_PROCTHR |
+            VTSS_CFGTRACE_BRANCH | VTSS_CFGTRACE_EXECTX  | VTSS_CFGTRACE_TBS    |
+            VTSS_CFGTRACE_LASTBR | VTSS_CFGTRACE_TREE    | VTSS_CFGTRACE_SYNCARG;
+        colrec.features |= reqcfg.trace_cfg.trace_flags;
+        colrec.len = (unsigned char)sizeof(colname);
+        rc |= vtss_transport_record_write(trnd, &colrec, sizeof(colrec), (void*)colname, sizeof(colname), is_safe);
+    }
+
+    /* generate system configuration record */
+    {
+        sys_trace_record_t sysrec;
+        size = syscfg.record_size;
+        /// [flagword][systrace(sysinfo)]
+        sysrec.flagword = UEC_LEAF1 | UECL1_SYSTRACE;
+        sysrec.size     = size + sizeof(sysrec.size) + sizeof(sysrec.type);
+        sysrec.type     = UECSYSTRACE_SYSINFO;
+        rc |= vtss_transport_record_write(trnd, &sysrec, sizeof(sysrec), (void*)&syscfg, size, is_safe);
+    }
+
+    /* generate hardware configuration record */
+    {
+        hcf_trace_record_t hcfrec;
+        size = (unsigned short)(sizeof(hardcfg) - (NR_CPUS - hardcfg.cpu_no) * sizeof(hardcfg.cpu_map[0]));
+        /// [flagword][systrace(hwcfg)]
+        hcfrec.flagword = UEC_LEAF1 | UECL1_SYSTRACE;
+        hcfrec.size     = size + sizeof(hcfrec.size) + sizeof(hcfrec.type);
+        hcfrec.type     = UECSYSTRACE_HWCFG;
+        hardcfg.timer_freq = vtss_freq_real(); /* update real timer freq */
+        rc |= vtss_transport_record_write(trnd, &hcfrec, sizeof(hcfrec), (void*)&hardcfg, size, is_safe);
+    }
+
+    /* generate Intel PT hardware configuration record */
+    {
+        ptc_trace_record_t ptcrec;
+        /// [flagword][systrace(iptcfg)]
+        ptcrec.flagword = UEC_LEAF1 | UECL1_USERTRACE;
+        ptcrec.size     = sizeof(iptcfg) + sizeof(ptcrec.size) + sizeof(ptcrec.type);
+        ptcrec.type = UECSYSTRACE_IPTCFG;
+        rc |= vtss_transport_record_write(trnd, &ptcrec, sizeof(ptcrec), (void*)&iptcfg, sizeof(iptcfg), is_safe);
+    }
+    /* generate time marker record */
+    {
+        struct timespec now;
+        time_marker_record_t timark;
+        timark.flagword = UEC_LEAF1 | UEC_VECTORED | UECL1_REALTSC;
+        timark.vectored = UECL1_REALTSC;
+        timark.vec_no   = 2;
+        timark.tsc      = vtss_time_real();
+        getnstimeofday(&now);
+        /* convert global time to 100ns units */
+        timark.utc      = div64_u64((u64)timespec_to_ns(&now), 100ULL);
+        rc |= vtss_transport_record_write(trnd, &timark, sizeof(timark), NULL, 0, is_safe);
+    }
+
+    return rc;
+}
+
+int vtss_record_softcfg(struct vtss_transport_data* trnd, pid_t tid, int is_safe)
+{
+    int i, rc = 0;
+    int evtsize;
+    vtss_softcfg_t* softcfg;
+    unsigned short size;
+    unsigned long flags;
+    scf_trace_record_t scfrec;
+
+    if (reqcfg.cpuevent_count_v1 > 0) {
+        local_irq_save(flags);
+        softcfg = (vtss_softcfg_t*)pcb_cpu.scratch_ptr;
+
+        softcfg->version = 2;
+        size = sizeof(int) + sizeof(short); // version + cpu_chain_len
+
+        for (i = 0; i < reqcfg.cpuevent_count_v1; i++) {
+            evtsize = reqcfg.cpuevent_cfg_v1[i].reqsize;
+
+            if (size + evtsize >= VTSS_DYNSIZE_SCRATCH) {
+                break;
+            }
+            /// copy event configuration
+            memcpy((char*)softcfg + size, &reqcfg.cpuevent_cfg_v1[i], sizeof(cpuevent_cfg_v1_t));
+            /// adjust event name and description offsets
+            ((cpuevent_cfg_v1_t*)((char*)softcfg + size))->name_off =
+                sizeof(cpuevent_cfg_v1_t);
+            ((cpuevent_cfg_v1_t*)((char*)softcfg + size))->desc_off =
+                sizeof(cpuevent_cfg_v1_t) + reqcfg.cpuevent_cfg_v1[i].name_len;
+            /// copy event name and description
+            size += sizeof(cpuevent_cfg_v1_t);
+            memcpy((char*)softcfg + size,
+                   (char*)&reqcfg.cpuevent_cfg_v1[i] + reqcfg.cpuevent_cfg_v1[i].name_off,
+                   reqcfg.cpuevent_cfg_v1[i].name_len);
+            size += reqcfg.cpuevent_cfg_v1[i].name_len;
+            memcpy((char*)softcfg + size,
+                   (char*)&reqcfg.cpuevent_cfg_v1[i] + reqcfg.cpuevent_cfg_v1[i].desc_off,
+                   reqcfg.cpuevent_cfg_v1[i].desc_len);
+            size += reqcfg.cpuevent_cfg_v1[i].desc_len;
+        }
+        softcfg->cpu_chain_len = i;
+        /// generate software configuration record
+        /// [flagword][systrace(swcfg)]
+        scfrec.flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_SYSTRACE;
+        scfrec.vresidx = (unsigned)tid;
+        scfrec.size = size + sizeof(scfrec.size) + sizeof(scfrec.type);
+        scfrec.type = UECSYSTRACE_SWCFG;
+        rc |= vtss_transport_record_write(trnd, &scfrec, sizeof(scfrec), (void*)softcfg, size, is_safe);
+        local_irq_restore(flags);
+    }
+    return rc;
+}
+
+int vtss_record_probe(struct vtss_transport_data* trnd, int cpu, int fid, int is_safe)
+{
+    prb_trace_record_t proberec;
+
+    proberec.flagword  = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_USERTRACE;
+    proberec.activity  = UECACT_PROBED;
+    /* it's a global probe and TID isn't important here */
+    proberec.residx    = 0;
+    proberec.cpuidx    = cpu;
+    /* NOTE: it's real TSC to be consistent with TPSS */
+    proberec.cputsc    = vtss_time_real();
+    proberec.size      = sizeof(prb_trace_record_t) - offsetof(prb_trace_record_t, size);
+    /* arch isn't important here but make it as native arch */
+#ifdef CONFIG_X86_64
+    proberec.type      = URT_APIWRAP64_V1;
+#else
+    proberec.type      = URT_APIWRAP32_V1;
+#endif
+    proberec.entry_tsc = proberec.cputsc;
+    proberec.entry_cpu = proberec.cpuidx;
+    proberec.fid       = fid;
+    return vtss_transport_record_write(trnd, &proberec, sizeof(proberec), NULL, 0, is_safe);
+}
+
+int vtss_record_probe_all(int cpu, int fid, int is_safe)
+{
+    prb_trace_record_t proberec;
+
+    proberec.flagword  = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_USERTRACE;
+    proberec.activity  = UECACT_PROBED;
+    /* it's a global probe and TID isn't important here */
+    proberec.residx    = 0;
+    proberec.cpuidx    = cpu;
+    /* NOTE: it's real TSC to be consistent with TPSS */
+    proberec.cputsc    = vtss_time_real();
+    proberec.size      = sizeof(prb_trace_record_t) - offsetof(prb_trace_record_t, size);
+    /* arch isn't important here but make it as native arch */
+#ifdef CONFIG_X86_64
+    proberec.type      = URT_APIWRAP64_V1;
+#else
+    proberec.type      = URT_APIWRAP32_V1;
+#endif
+    proberec.entry_tsc = proberec.cputsc;
+    proberec.entry_cpu = proberec.cpuidx;
+    proberec.fid       = fid;
+    return vtss_transport_record_write_all(&proberec, sizeof(proberec), NULL, 0, is_safe);
+}
+
+int vtss_record_thread_name(struct vtss_transport_data* trnd, pid_t tid, const char *taskname, int is_safe)
+{
+    size_t namelen = taskname ? strlen(taskname) + 1 : 0;
+#ifdef VTSS_USE_UEC
+    thname_trace_record_t namerec;
+    namerec.probe.flagword  = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_USERTRACE;
+    namerec.probe.activity  = UECACT_PROBED;
+    namerec.probe.residx    = tid;
+    namerec.probe.cpuidx    = 0;
+    namerec.probe.cputsc    = vtss_time_real();
+
+    namerec.probe.size      = sizeof(thname_trace_record_t) - offsetof(prb_trace_record_t, size) + namelen;
+#ifdef CONFIG_X86_64
+    namerec.probe.type      = URT_APIWRAP64_V1;
+#else
+    namerec.probe.type      = URT_APIWRAP32_V1;
+#endif
+    namerec.probe.entry_tsc = namerec.probe.cputsc;
+    namerec.probe.entry_cpu = namerec.probe.cpuidx;
+    namerec.probe.fid       = FID_THREAD_NAME;
+
+    namerec.version  = 1;
+    namerec.length   = namelen;
+
+    return vtss_transport_record_write(trnd, &namerec, sizeof(namerec), (void*)taskname, namelen, is_safe);
+#else
+    int rc = -EFAULT;
+    void* entry;
+    thname_trace_record_t* namerec = (thname_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(thname_trace_record_t) + namelen);
+    if (likely(namerec)) {
+        namerec->probe.flagword  = UEC_LEAF1 | UECL1_ACTIVITY | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_USERTRACE;
+        namerec->probe.activity  = UECACT_PROBED;
+        namerec->probe.residx    = tid;
+        namerec->probe.cpuidx    = 0;
+        namerec->probe.cputsc    = vtss_time_real();
+
+        namerec->probe.size      = sizeof(thname_trace_record_t) - offsetof(prb_trace_record_t, size) + namelen;
+#ifdef CONFIG_X86_64
+        namerec->probe.type      = URT_APIWRAP64_V1;
+#else
+        namerec->probe.type      = URT_APIWRAP32_V1;
+#endif
+        namerec->probe.entry_tsc = namerec->probe.cputsc;
+        namerec->probe.entry_cpu = namerec->probe.cpuidx;
+        namerec->probe.fid       = FID_THREAD_NAME;
+        
+        namerec->version  = 1;
+        namerec->length   = namelen;
+        memcpy(++namerec, taskname, namelen);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+#endif
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/stack.c b/drivers/misc/intel/sepdk/vtsspp/stack.c
new file mode 100644
index 000000000000..43afee4e47aa
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/stack.c
@@ -0,0 +1,899 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "stack.h"
+#include "regs.h"
+#include "globals.h"
+#include "record.h"
+#include "user_vm.h"
+#include "time.h"
+#include "lbr.h"
+
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>      /* for kmap()/kunmap() */
+#include <linux/pagemap.h>      /* for page_cache_release() */
+#include <asm/page.h>
+#include <asm/processor.h>
+#include <linux/nmi.h>
+#include <linux/module.h>
+#include "unwind.c"
+
+#define VTSS_STK_LOG(fmt, ...) do {\
+    int nb = snprintf(stk->dbgmsg, sizeof(stk->dbgmsg)-1, "%s: " fmt, __FUNCTION__, ##__VA_ARGS__);\
+    if (nb > 0 && nb < sizeof(stk->dbgmsg)-1) {\
+        stk->dbgmsg[nb] = '\0';\
+        vtss_record_debug_info(trnd, stk->dbgmsg, 0);\
+    }\
+} while(0)
+
+static int vtss_stack_store_ip(unsigned long addr, unsigned long *prev_addr, char *callchain, int *callchain_pos, int callchain_size)
+{
+    unsigned long addr_diff;
+    int sign;
+    char prefix = 0;
+    int j;
+
+    addr_diff = addr - (*prev_addr);
+    sign = (addr_diff & (((size_t)1) << ((sizeof(size_t) << 3) - 1))) ? 0xff : 0;
+    for (j = sizeof(void*) - 1; j >= 0; j--)
+    {
+        if(((addr_diff >> (j << 3)) & 0xff) != sign)
+        {
+           break;
+        }
+    }
+    prefix |= sign ? 0x40 : 0;
+    prefix |= j + 1;
+
+    if (callchain_size <= (*callchain_pos)+1+j+1) {
+        return -1;
+    }
+
+    callchain[*callchain_pos] = prefix;
+    (*callchain_pos)++;
+
+    *(unsigned long*)&(callchain[*callchain_pos]) = addr_diff;
+    (*callchain_pos) += j + 1;
+    *prev_addr = addr;
+    return 0;
+}
+
+#ifdef VTSS_AUTOCONF_STACKTRACE_OPS
+
+#include <asm/stacktrace.h>
+
+#ifdef VTSS_AUTOCONF_STACKTRACE_OPS_WARNING
+static void vtss_warning(void *data, char *msg)
+{
+}
+
+static void vtss_warning_symbol(void *data, char *msg, unsigned long symbol)
+{
+}
+#endif
+
+typedef struct kernel_stack_control_t
+{
+    unsigned long bp;
+    unsigned char* kernel_callchain;
+    int* kernel_callchain_size;
+    int* kernel_callchain_pos;
+    unsigned long prev_addr;
+    int done;
+} kernel_stack_control_t;
+
+static int vtss_stack_stack(void *data, char *name)
+{
+    kernel_stack_control_t* stk = (kernel_stack_control_t*)data;
+    if (!stk) return -1;
+    if (stk->done){
+        ERROR("Error happens during stack processing");
+        return -1;
+    }
+    return 0;
+}
+
+static void vtss_stack_address(void *data, unsigned long addr, int reliable)
+{
+    kernel_stack_control_t* stk = (kernel_stack_control_t*)data;
+    TRACE("%s%pB %d", reliable ? "" : "? ", (void*)addr, *stk->kernel_callchain_pos);
+    touch_nmi_watchdog();
+    if (!reliable){
+        return;
+    }
+    if (!stk || !stk->kernel_callchain_size || !stk->kernel_callchain_pos){
+        return;
+    }
+    if ((*stk->kernel_callchain_size) <= (*stk->kernel_callchain_pos)) {
+        return;
+    }
+#ifndef CONFIG_FRAME_POINTER
+    if (addr < VTSS_KOFFSET) return;
+#endif
+    if (stk->done) return;
+    
+    if(vtss_stack_store_ip(addr, &stk->prev_addr, stk->kernel_callchain,
+        stk->kernel_callchain_pos, *stk->kernel_callchain_size) == -1) {
+        stk->done = 1;
+    }
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,6,0)
+static int vtss_stack_address_int(void *data, unsigned long addr, int reliable)
+{
+    vtss_stack_address(data, addr, reliable);
+    return 0;
+}
+#endif
+
+#if defined(VTSS_AUTOCONF_STACKTRACE_OPS_WALK_STACK)
+static unsigned long vtss_stack_walk(
+#if defined(VTSS_AUTOCONF_WALK_STACK_TASK_ARG)
+    struct task_struct *t,
+#else
+    struct thread_info *t,
+#endif
+    unsigned long *stack,
+    unsigned long bp,
+    const struct stacktrace_ops *ops,
+    void *data,
+    unsigned long *end,
+    int *graph)
+{
+    kernel_stack_control_t* stk = (kernel_stack_control_t*)data;
+    if (!stk){
+        ERROR("Internal error!");
+        return bp;
+    }
+    if (!stack){
+        ERROR("Broken stack pointer!");
+        stk->done=1;
+        return bp;
+    }
+    if (stack <= (unsigned long*)VTSS_MAX_USER_SPACE){
+        TRACE("Stack pointer belongs user space. We will not process it. stack_ptr=%p", stack);
+        if (stk->kernel_callchain_pos && *stk->kernel_callchain_pos == 0){
+            TRACE("Most probably stack pointer intitialization is wrong. No one stack address is resolved.");
+        }
+        stk->done=1;
+        return bp;
+    }
+/*    if ((bp <= vtss_max_user_space) && (bp != 0)){
+        TRACE("Frame pointer belongs user space. We will not process it. bp=%lx", bp);
+        stk->done=1;
+        return bp;
+    }
+*/
+    if (stk->bp==0){
+        TRACE("bp=0x%p, stack=0x%p, end=0x%p", (void*)stk->bp, stack, end);
+        stk->bp = bp;
+    }
+    if (stk->done){
+        return bp;
+    }
+    bp = print_context_stack(t, stack, stk->bp, ops, data, end, graph);
+    if (stk != NULL && bp < VTSS_KSTART) {
+        TRACE("user bp=0x%p", (void*)bp);
+        stk->bp = bp;
+    }
+#if (!defined(CONFIG_X86_64)) && (LINUX_VERSION_CODE < KERNEL_VERSION(3,15,0))
+    {
+        struct thread_info* context = (struct thread_info *)((unsigned long)stack & (~(THREAD_SIZE - 1)));
+        unsigned long* next_stack = NULL;
+        if (context && (unsigned long)context > VTSS_MAX_USER_SPACE) next_stack = (unsigned long *)context->previous_esp;
+        if ((!next_stack) || (!((void*)next_stack >(void*)context && (void*)next_stack < (void*)context + THREAD_SIZE - sizeof(unsigned long)))) stk->done=1;
+    }
+#endif
+    return bp;
+}
+#endif
+
+static const struct stacktrace_ops vtss_stack_ops = {
+#ifdef VTSS_AUTOCONF_STACKTRACE_OPS_WARNING
+    .warning        = vtss_warning,
+    .warning_symbol = vtss_warning_symbol,
+#endif
+    .stack          = vtss_stack_stack,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,6,0)
+    .address        = vtss_stack_address,
+#else
+    .address        = vtss_stack_address_int,
+#endif
+#if defined(VTSS_AUTOCONF_STACKTRACE_OPS_WALK_STACK)
+    .walk_stack     = vtss_stack_walk,
+#endif
+};
+
+static void *vtss_stack_unwind_kernel(struct vtss_transport_data* trnd, stack_control_t* stk, struct task_struct* task, struct pt_regs* regs_in, void *reg_fp)
+{
+    kernel_stack_control_t k_stk;
+
+    k_stk.bp = (unsigned long)reg_fp;
+    k_stk.kernel_callchain = stk->kernel_callchain;
+    k_stk.prev_addr = 0;
+    k_stk.kernel_callchain_size = &stk->kernel_callchain_size;
+    k_stk.kernel_callchain_pos =  &stk->kernel_callchain_pos;
+    *k_stk.kernel_callchain_pos = 0;
+    k_stk.done = 0;
+#ifdef VTSS_AUTOCONF_DUMP_TRACE_HAVE_BP
+    dump_trace(task, regs_in , NULL, 0, &vtss_stack_ops, &k_stk);
+#else
+    dump_trace(task, regs_in, NULL, &vtss_stack_ops, &k_stk);
+#endif
+    if(k_stk.bp) reg_fp = (void *)k_stk.bp;
+    return reg_fp;
+}
+
+#else /* VTSS_AUTOCONF_STACKTRACE_OPS */
+
+#include <asm/unwind.h>
+
+static void *vtss_stack_unwind_kernel(struct vtss_transport_data* trnd, stack_control_t* stk, struct task_struct* task, struct pt_regs* regs_in, void *reg_fp)
+{
+    struct unwind_state state;
+    unsigned long addr, prev_addr = 0;
+
+    stk->kernel_callchain_pos = 0;
+    for (unwind_start(&state, task, regs_in, NULL); !unwind_done(&state);
+            unwind_next_frame(&state)) {
+        addr = unwind_get_return_address(&state);
+        if (!addr) {
+            break;
+        }
+        if (vtss_stack_store_ip(addr, &prev_addr, stk->kernel_callchain, 
+                    &stk->kernel_callchain_pos, stk->kernel_callchain_size) == -1) {
+            break;
+        }
+    }
+    return reg_fp;
+}
+#endif
+
+static int vtss_check_user_fp(struct task_struct* task, stack_control_t* stk, char* fp)
+{
+    struct vm_area_struct* vma;
+    //allow fp from segments differ than stack
+    //if(fp < stk->user_sp.chp) return -1;
+    //if(fp > stk->bp.chp) return -1;
+    if((unsigned long)fp & (stk->wow64 ? 3 : sizeof(void*) - 1)) return -1;
+    vma = find_vma(task->mm, (unsigned long)fp);
+    if(vma == NULL) return -1;
+    if((unsigned long)fp < vma->vm_start) return -1;
+    if((unsigned long)fp > vma->vm_end) return -1;
+    return 0;
+}
+
+static int vtss_check_user_ip(struct task_struct* task, stack_control_t* stk, char* ip)
+{
+    struct vm_area_struct* vma;
+    vma = find_vma(task->mm, (unsigned long)ip);
+    if(vma == NULL) return -1;
+    if(!(vma->vm_flags & VM_EXEC) && !(vma->vm_flags & VM_MAYEXEC)) return -1;
+    if((unsigned long)ip < vma->vm_start) return -1;
+    if((unsigned long)ip > vma->vm_end) return -1;
+    return 0;
+}
+
+static int vtss_read_user_frame(stack_control_t* stk, char *curr_sp, char **fp, char** ip)
+{
+    char vals[2*sizeof(void*)]; /* fp + ip */
+    int stride = stk->wow64 ? 4 : sizeof(void*);
+
+    if (stk->acc->read(stk->acc, curr_sp, vals, 2*stride) != 2*stride) {
+        return -1;
+    }
+    *fp = (char*)(size_t)(stk->wow64 ? *(u32*)(&vals[0]) : *(u64*)(&vals[0]));
+    *ip = (char*)(size_t)(stk->wow64 ? *(u32*)(&vals[stride]) : *(u64*)(&vals[stride]));
+    return 0;
+}
+
+static int vtss_stack_unwind_user(struct vtss_transport_data* trnd, stack_control_t* stk, struct task_struct* task)
+{
+    char* fp = stk->user_fp.chp;
+    char* ip = stk->user_ip.chp;
+    char* curr_fp = fp;
+    char *prev_ip = 0;
+    void **buffer;
+    
+    stk->user_callchain_pos = 0;
+    if(stk->user_callchain == NULL) {
+        return 0;
+    }
+    buffer = (void**)stk->user_callchain;
+
+    TRACE("fp=%p, sp=%p, stack_base=%p, ip=%p", fp, sp, stk->bp.chp, ip);
+
+    if(vtss_check_user_fp(task, stk, fp) == -1) {
+        return VTSS_ERR_NOTFOUND; // no frames
+#if 0
+        /* try to find the first frame inside stack */
+        char* stack_base = stk->bp.chp;
+        char* sp = stk->user_sp.chp;
+        char *curr_sp = sp;
+        int stride = stk->wow64 ? 4 : sizeof(void*);
+
+        curr_fp = NULL;
+        for(curr_sp = sp; curr_sp < stack_base; curr_sp += stride) {
+        
+            if(read_frame(stk, curr_sp, &fp, &ip) == -1) {
+                return 0;
+            }
+            //find frames from stack only
+            //if(fp < sp) continue;
+            //if(fp > stack_base) continue;
+            if(vtss_check_user_fp(task, stk, fp) == -1) continue;
+            if((sp <= ip) && (ip <= stack_base)) continue;
+            if(vtss_check_user_ip(task, stk, ip) == -1) continue;
+
+            // save found frame
+            curr_fp = fp;
+            vtss_stack_store_ip((unsigned long)ip, (unsigned long*)&prev_ip, 
+                stk->user_callchain, &stk->user_callchain_pos, stk->user_callchain_size);
+            break;
+        }
+        if(curr_fp == NULL) return 0;
+        TRACE("found the frame in stack: fp=%p, ip=%p", fp, ip);
+#endif
+    }
+
+    while(curr_fp) {
+
+        if(vtss_read_user_frame(stk, curr_fp, &fp, &ip) == -1) {
+            break;
+        }
+        if(vtss_check_user_fp(task, stk, fp) == -1) break;
+        if(vtss_check_user_ip(task, stk, ip) == -1) break;
+
+        curr_fp = fp;
+        if(vtss_stack_store_ip((unsigned long)ip, (unsigned long*)&prev_ip, 
+            stk->user_callchain, &stk->user_callchain_pos, stk->user_callchain_size) == -1) {
+            return 0;
+        }
+    }
+    return 0;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,8,0)
+#ifndef this_cpu_ptr
+#define this_cpu_ptr(ptr) SHIFT_PERCPU_PTR(ptr, my_cpu_offset)
+#endif
+#endif
+
+#if 0 && defined(VTSS_CONFIG_KPTI)
+#include <asm/cpu_entry_area.h>
+static unsigned long vtss_get_cea_user_sp(void)
+{
+    int cpu;
+    char *entry_stack;
+
+    preempt_disable();
+    cpu = smp_processor_id();
+    preempt_enable_no_resched();
+    entry_stack = (char*)cpu_entry_stack(cpu);
+    if (entry_stack) {
+        return *(unsigned long*)(entry_stack + sizeof(struct entry_stack) - 8);
+    }
+    return 0;
+}
+#endif
+
+#if defined(CONFIG_X86_64) && !defined(VTSS_USE_NMI)
+static int vtss_in_syscall(void)
+{
+    if (current->audit_context) {
+        if (((int*)current->audit_context)[1]) { /* audit_context->in_syscall */
+            return 1;
+        }
+    }
+    return 0;
+}
+#endif
+
+static unsigned long vtss_get_user_sp(struct task_struct* task, struct pt_regs* regs)
+{
+#if defined(CONFIG_X86_64) && !defined(VTSS_USE_NMI)
+    if (vtss_in_syscall() && !test_tsk_thread_flag(task, TIF_IA32))
+    {
+        if (vtss_syscall_rsp_ptr)
+        {
+            /* inside syscall */
+            unsigned long old_rsp = *this_cpu_ptr((unsigned long*)vtss_syscall_rsp_ptr);
+            if (old_rsp) return old_rsp;
+        }
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,1,0)
+        /* fast system call or could not get old_rsp */
+        if (task->thread.usersp) return task->thread.usersp;
+#endif
+    }
+#endif
+    return REG(sp, regs);
+}
+
+int vtss_stack_dump(struct vtss_transport_data* trnd, stack_control_t* stk, struct task_struct* task, struct pt_regs* regs_in, void* reg_fp, void* user_sp, int in_irq)
+{
+    int rc = 0;
+    void* stack_base = stk->bp.vdp;
+    void *reg_ip, *reg_sp;
+    int kernel_stack = 0;
+    struct pt_regs* regs = regs_in;
+    int user_mode_regs = 0;
+
+#ifdef VTSS_USE_NMI
+    // skip kernel threads
+    if (current->mm == NULL) {
+        return -EFAULT;
+    }
+#endif
+    if ((!regs && reg_fp > (void*)VTSS_MAX_USER_SPACE) || (regs && (!vtss_user_mode(regs))) || (!reg_fp)) {
+        kernel_stack = 1;
+    }
+#ifndef CONFIG_FRAME_POINTER
+    if (!regs) {
+        kernel_stack = 0;
+        TRACE("kernel stack 0 when no frame pointers");
+    }
+#endif
+    if (regs && vtss_user_mode(regs)) {
+        user_mode_regs = 1;
+    }
+
+    /* Unwind kernel stack */
+    if (kernel_stack && stk->kernel_callchain) {
+        if ((unsigned long)reg_fp < 0x1000 || (unsigned long)reg_fp == (unsigned long)-1) reg_fp = 0; //error instead of bp;
+        reg_fp = vtss_stack_unwind_kernel(trnd, stk, task, regs_in, reg_fp);
+    } else {
+        stk->kernel_callchain_pos = 0;
+    }
+    if (current != task) {
+       // we will get crash during user space access while unwiding
+       return rc; 
+    }
+    if (regs == NULL || !vtss_user_mode(regs)) {
+        /* kernel mode regs, so get a user mode regs */
+#if !defined(CONFIG_HIGHMEM)
+        if (current->mm) {
+            regs = task_pt_regs(task); /* get user mode regs */
+        }
+        else {
+            regs = NULL;
+        }
+        if (regs == NULL || !vtss_user_mode(regs))
+#endif
+        {
+            // we might be in kernel thread
+            VTSS_STK_LOG("Cannot get user mode registers");
+            return rc;
+        }
+    }
+
+    /* Get IP and SP registers from user space */
+    reg_ip = (void*)REG(ip, regs);
+    reg_sp = user_mode_regs ? (void*)REG(sp, regs) : (void*)vtss_get_user_sp(task, regs);
+    if (reg_fp > (void*)VTSS_MAX_USER_SPACE || reg_fp < (void*)0x1000)
+        reg_fp = (void*)REG(bp, regs);
+
+    { /* Check for correct stack range in task->mm */
+        struct vm_area_struct* vma;
+
+#ifdef VTSS_CHECK_IP_IN_MAP
+        /* Check IP in module map */
+        vma = find_vma(task->mm, (unsigned long)reg_ip);
+        if (likely(vma != NULL)) {
+            unsigned long vm_start = vma->vm_start;
+            unsigned long vm_end   = vma->vm_end;
+
+            if ((unsigned long)reg_ip < vm_start ||
+                (!((vma->vm_flags & (VM_EXEC | VM_WRITE)) == VM_EXEC &&
+                    vma->vm_file && vma->vm_file->f_path.dentry) &&
+                 !(vma->vm_mm && vma->vm_start == (long)vma->vm_mm->context.vdso)))
+            {
+                VTSS_STK_LOG("ip=%p not in valid vma", reg_ip);
+                return -EFAULT;
+            }
+        } 
+        else {
+            VTSS_STK_LOG("no vma on ip=%p", reg_ip);
+            return -EFAULT;
+        }
+#endif /* VTSS_CHECK_IP_IN_MAP */
+
+        /* Check SP in module map */
+        vma = find_vma(task->mm, (unsigned long)reg_sp);
+        if (likely(vma != NULL)) {
+            unsigned long vm_start = vma->vm_start + ((vma->vm_flags & VM_GROWSDOWN) ? PAGE_SIZE : 0UL);
+            unsigned long vm_end   = vma->vm_end;
+
+            if ((unsigned long)reg_sp < vm_start ||
+                (vma->vm_flags & (VM_READ | VM_WRITE)) != (VM_READ | VM_WRITE))
+            {
+                VTSS_STK_LOG("sp=%p not in valid vma", reg_sp);
+                return -EFAULT;
+            }
+            if (!((unsigned long)stack_base >= vm_start &&
+                  (unsigned long)stack_base <= vm_end)  ||
+                 ((unsigned long)stack_base <= (unsigned long)reg_sp))
+            {
+                if ((unsigned long)stack_base != 0UL) {
+                    TRACE("Fixup stack base to 0x%lx instead of 0x%lx", vm_end, (unsigned long)stack_base);
+                }
+                stack_base = (void*)vm_end;
+                stk->clear(stk);
+            }
+            stk->hugetlb = (vma->vm_flags & VM_HUGETLB);
+
+            if (!stk->hugetlb) {
+                if ((unsigned long)(stack_base - reg_sp) > 0x100000) {
+                    // workaround on the problem with huge stack on HSW/BDW/SKL EP machines
+                    stk->hugetlb = 1;
+                }
+            }
+            
+            if ((unsigned long)stack_base - (unsigned long)reg_sp > reqcfg.stk_sz[vtss_stk_user]){
+                unsigned long stack_base_calc = min((unsigned long)stack_base, ((unsigned long)reg_sp + reqcfg.stk_sz[vtss_stk_user])&(~(reqcfg.stk_pg_sz[vtss_stk_user]-1)));
+                if (stack_base_calc < (unsigned long)stack_base){
+                    TRACE("Limiting stack base to 0x%lx instead of 0x%lx, drop 0x%lx bytes", stack_base_calc, (unsigned long)stack_base, ((unsigned long)stack_base - stack_base_calc));
+                    stack_base = (void*)stack_base_calc;
+                }
+            }
+            else if(stk->hugetlb) {
+                /* scan no more than 32 pages */
+                unsigned long stack_base_calc = ((unsigned long)reg_sp + 32*PAGE_SIZE)&(~(PAGE_SIZE-1));
+                if (stack_base_calc < (unsigned long)stack_base) {
+                    stack_base = (void*)stack_base_calc;
+                }
+            }
+        } else {
+            VTSS_STK_LOG("no vma on sp=%p", reg_sp);
+            return -EFAULT;
+        }
+    }
+
+#if 0
+    if (stk->user_ip.vdp == reg_ip &&
+        stk->user_sp.vdp == reg_sp &&
+        stk->bp.vdp == stack_base &&
+        stk->user_fp.vdp == reg_fp)
+    {
+        VTSS_STK_LOG("The same context");
+        return 0; /* Assume that nothing was changed */
+    }
+#endif
+
+    /* Try to lock vm accessor */
+    if (unlikely((stk->acc == NULL) || stk->acc->trylock(stk->acc, task))) {
+        VTSS_STK_LOG("Unable to lock vm accessor");
+        return -EBUSY;
+    }
+
+    stk->user_ip.vdp = reg_ip;
+    stk->user_sp.vdp = reg_sp;
+    stk->bp.vdp = stack_base;
+    stk->user_fp.vdp = reg_fp;
+
+    /* Unwind using FP */
+    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CLRSTK) {
+        rc = vtss_stack_unwind_user(trnd, stk, task);
+        if(stk->user_callchain_pos != 0) {
+            stk->acc->unlock(stk->acc);
+            return rc;
+        }
+    }
+
+    VTSS_PROFILE(unw, rc = stk->unwind(stk));
+    /* Check unwind result */
+    if (unlikely(rc == VTSS_ERR_NOMEMORY)) {
+        /* Try again with realloced buffer */
+        while (rc == VTSS_ERR_NOMEMORY) {
+            int err = stk->realloc(stk); /* realloc_stack */
+            if (err == VTSS_ERR_NOMEMORY) {
+                /* dump truncated stacks */
+                if(stk->hugetlb && stk->buffer) rc = 0;
+                break;
+            }
+            VTSS_PROFILE(unw, rc = stk->unwind(stk)); /* unwind_stack_fwd */
+        }
+        if (rc == VTSS_ERR_NOMEMORY) {
+            VTSS_STK_LOG("Not enough memory for stack buffer");
+        }
+    }
+    stk->acc->unlock(stk->acc);
+    if (unlikely(rc)) {
+        stk->clear(stk);
+        if(rc != VTSS_ERR_NOMEMORY) VTSS_STK_LOG("unwind error: %d", rc);
+    }
+    TRACE("end, rc = %d", rc);
+    return rc;
+}
+
+int vtss_stack_record_kernel(struct vtss_transport_data* trnd, stack_control_t* stk, pid_t tid, int cpu, unsigned long long stitch_id, int is_safe)
+{
+    int rc = -EFAULT;
+    int stklen = stk->kernel_callchain_pos;
+#ifdef VTSS_USE_UEC
+    stk_trace_kernel_record_t stkrec;
+    if (stklen == 0)
+    {
+        // kernel is empty
+        VTSS_STK_LOG("Kernel stack is empty");
+        return 0;
+    }
+    TRACE("ip=0x%p, sp=0x%p, fp=0x%p: Trace %d bytes", stk->ip.vdp, stk->sp.vdp, stk->fp.vdp, stklen);
+    //implementation is done for UEC NOT USED
+    /// save current alt. stack:
+    /// [flagword - 4b][residx]
+    /// ...[sampled address - 8b][systrace{sts}]
+    ///                       [length - 2b][type - 2b]...
+    stkrec.flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_SYSTRACE;
+    stkrec.residx   = tid;
+    stkrec.size     = sizeof(stkrec.size) + sizeof(stkrec.type);
+    stkrec.type     = (sizeof(void*) == 8) ? UECSYSTRACE_CLEAR_STACK64 : UECSYSTRACE_CLEAR_STACK32;
+    stkrec.size += sizeof(unsigned int);
+    stkrec.idx   = -1;
+    /// correct the size of systrace
+    stkrec.size += (unsigned short)stklen;
+    if (vtss_transport_record_write(trnd, &stkrec, sizeof(stkrec), stk->kernel_callchain, stklen, is_safe)) {
+        TRACE("STACK_record_write() FAIL");
+        VTSS_STK_LOG("Unable to write the record");
+        rc = -EFAULT;
+    }
+
+#else
+    void* entry;
+    stk_trace_kernel_record_t* stkrec;
+
+    if (stklen == 0)
+    {
+        // kernel is empty
+        VTSS_STK_LOG("Kernel stack is empty");
+        return 0;
+    }
+    TRACE("ip=0x%p, sp=0x%p, fp=0x%p: Trace %d bytes", stk->ip.vdp, stk->sp.vdp, stk->fp.vdp, stklen);
+    //implementation is done for UEC NOT USED
+    stkrec = (stk_trace_kernel_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(stk_trace_kernel_record_t) + stklen);
+    if (likely(stkrec)) {
+    /// save current alt. stack:
+    /// [flagword - 4b][residx]
+    /// ...[sampled address - 8b][systrace{sts}]
+    ///                       [length - 2b][type - 2b]...
+    stkrec->flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_SYSTRACE;
+    stkrec->residx   = tid;
+    stkrec->size     = (unsigned short)stklen + sizeof(stkrec->size) + sizeof(stkrec->type);
+    stkrec->type     = (sizeof(void*) == 8) ? UECSYSTRACE_CLEAR_STACK64 : UECSYSTRACE_CLEAR_STACK32;
+    stkrec->size += sizeof(unsigned int);
+    stkrec->idx   = -1;
+    memcpy((char*)stkrec+sizeof(stk_trace_kernel_record_t), stk->kernel_callchain, stklen);
+    rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+#endif
+    return rc;
+}
+
+int vtss_stack_record_user(struct vtss_transport_data* trnd, stack_control_t* stk, pid_t tid, int cpu, int is_safe)
+{
+    int rc = 0;
+    int stklen = stk->user_callchain_pos;
+    
+#ifdef VTSS_USE_UEC
+
+    clrstk_trace_record_t stkrec;
+ 
+    if(stklen == 0)
+    {
+        // user clean stack is empty
+        return 0;
+    }
+    /// save current alt. stack in UEC: [flagword - 4b][residx][cpuidx - 4b][tsc - 8b]
+    ///                                 ...[sampled address - 8b][systrace{sts}]
+    ///                                                          [length - 2b][type - 2b]...
+    stkrec.flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_EXECADDR | UECL1_SYSTRACE;
+    stkrec.residx = tid;
+    stkrec.cpuidx = cpu;
+    stkrec.cputsc = vtss_time_cpu();
+    stkrec.execaddr = (unsigned long long)stk->user_ip.szt;
+
+    stkrec.size = sizeof(stkrec.size) + sizeof(stkrec.type) + sizeof(stkrec.merge_node) + (unsigned short)stklen;
+    stkrec.type = (sizeof(void*) == 8) ? UECSYSTRACE_CLEAR_STACK64 : UECSYSTRACE_CLEAR_STACK32;
+    stkrec.merge_node = 0xffffffff;
+
+    if (vtss_transport_record_write(trnd, &stkrec, sizeof(stkrec), stk->user_callchain, stklen, is_safe))
+    {
+        TRACE("STACK_record_write() FAIL");
+        rc = -EFAULT;
+    }
+
+#else  // VTSS_USE_UEC 
+
+    void* entry;
+    clrstk_trace_record_t* stkrec = (clrstk_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(clrstk_trace_record_t) + stklen);
+
+    if(stklen == 0)
+    {
+        // user clean stack is empty
+        return 0;
+    }
+    if(likely(stkrec))
+    {
+        /// save current alt. stack in UEC: [flagword - 4b][residx][cpuidx - 4b][tsc - 8b]
+        ///                                 ...[sampled address - 8b][systrace{sts}]
+        ///                                                          [length - 2b][type - 2b]...
+        stkrec->flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_EXECADDR | UECL1_SYSTRACE;
+        stkrec->residx   = tid;
+        stkrec->cpuidx   = cpu;
+        stkrec->cputsc   = vtss_time_cpu();
+        stkrec->execaddr = (unsigned long long)stk->user_ip.szt;
+
+        stkrec->size = sizeof(stkrec->size) + sizeof(stkrec->type) + sizeof(stkrec->merge_node) + (unsigned short)stklen;
+        stkrec->type = (sizeof(void*) == 8) ? UECSYSTRACE_CLEAR_STACK64 : UECSYSTRACE_CLEAR_STACK32;
+        stkrec->merge_node = 0xffffffff;
+        memcpy((char*)stkrec + sizeof(clrstk_trace_record_t), stk->user_callchain, stklen);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    else
+    {
+        TRACE("STACK_record_write() FAIL");
+        rc = -EFAULT;
+    }
+#endif //  VTSS_USE_UEC
+    return rc;
+}
+
+int vtss_stack_record(struct vtss_transport_data* trnd, stack_control_t* stk, pid_t tid, int cpu, int is_safe, unsigned long* recid)
+{
+    int rc = -EFAULT;
+    unsigned short sample_type;
+    int sktlen = 0;
+
+    /// collect LBR call stacks if so requested
+    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_LBRCSTK)
+    {
+        return vtss_stack_record_lbr(trnd, stk, tid, cpu, is_safe);
+    }
+    /// skip compress dirty stack if clean is present
+    if (stk->user_callchain_pos == 0)
+    {
+        sktlen = stk->compress(stk);
+    }
+
+    if (stk->kernel_callchain_pos!=0)
+    {
+        rc = vtss_stack_record_kernel(trnd, stk, tid, cpu, 0, is_safe);
+    }
+    else
+    {
+        TRACE("kernel stack is empty");
+    }
+    /// record clean stack
+    if (stk->user_callchain_pos != 0) {
+        return vtss_stack_record_user(trnd, stk, tid, cpu, is_safe);
+    }
+
+    if (unlikely(sktlen == 0)) {
+        VTSS_STK_LOG("User stack is empty");
+        VTSS_STK_LOG("tid=0x%08x, cpu=0x%08x, ip=0x%p, sp=[0x%p,0x%p], fp=0x%p",
+                tid, raw_smp_processor_id(), stk->user_ip.vdp, stk->user_sp.vdp, stk->bp.vdp, stk->user_fp.vdp);
+        return 0;
+    }
+
+    if (stk->is_full(stk)) { /* full stack */
+        sample_type = (sizeof(void*) == 8 && !stk->wow64) ? UECSYSTRACE_STACK_CTX64_V0 : UECSYSTRACE_STACK_CTX32_V0;
+    } else { /* incremental stack */
+        sample_type = (sizeof(void*) == 8 && !stk->wow64) ? UECSYSTRACE_STACK_CTXINC64_V0 : UECSYSTRACE_STACK_CTXINC32_V0;
+    }
+
+#ifdef VTSS_USE_UEC
+    {
+        stk_trace_record_t stkrec;
+        /// save current alt. stack:
+        /// [flagword - 4b][residx][cpuidx - 4b][tsc - 8b]
+        /// ...[sampled address - 8b][systrace{sts}]
+        ///                       [length - 2b][type - 2b]...
+        stkrec.flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_EXECADDR | UECL1_SYSTRACE;
+        stkrec.residx   = tid;
+        stkrec.cpuidx   = cpu;
+        stkrec.cputsc   = vtss_time_cpu();
+        stkrec.execaddr = (unsigned long long)stk->user_ip.szt;
+        stkrec.type     = sample_type;
+
+        if (!stk->wow64) {
+            stkrec.size = 4 + sizeof(void*) + sizeof(void*);
+            stkrec.sp   = stk->user_sp.szt;
+            stkrec.fp   = stk->user_fp.szt;
+        } else { /// a 32-bit stack in a 32-bit process on a 64-bit system
+            stkrec.size = 4 + sizeof(unsigned int) + sizeof(unsigned int);
+            stkrec.sp32 = (unsigned int)stk->user_sp.szt;
+            stkrec.fp32 = (unsigned int)stk->user_fp.szt;
+        }
+        rc = 0;
+        if (sktlen > 0xfffb) {
+            lstk_trace_record_t lstkrec;
+
+            TRACE("ip=0x%p, sp=0x%p, fp=0x%p: Large Trace %d bytes", stk->user_ip.vdp, stk->user_sp.vdp, stk->user_fp.vdp, sktlen);
+            lstkrec.size = (unsigned int)(stkrec.size + sktlen + 2); /* 2 = sizeof(int) - sizeof(short) */
+            lstkrec.flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_EXECADDR | UECL1_LARGETRACE;
+            lstkrec.residx   = stkrec.residx;
+            lstkrec.cpuidx   = stkrec.cpuidx;
+            lstkrec.cputsc   = stkrec.cputsc;
+            lstkrec.execaddr = stkrec.execaddr;
+            lstkrec.type     = stkrec.type;
+            lstkrec.sp       = stkrec.sp;
+            lstkrec.fp       = stkrec.fp;
+            if (vtss_transport_record_write(trnd, &lstkrec, sizeof(lstkrec) - (stk->wow64*8), stk->data(stk), sktlen, is_safe)) {
+                VTSS_STK_LOG("Record was not written");
+                TRACE("STACK_record_write() FAIL");
+                rc = -EFAULT;
+            }
+        } else {
+            /// correct the size of systrace
+            stkrec.size += (unsigned short)sktlen;
+            if (vtss_transport_record_write(trnd, &stkrec, sizeof(stkrec) - (stk->wow64*8), stk->data(stk), sktlen, is_safe)) {
+                TRACE("STACK_record_write() FAIL");
+                VTSS_STK_LOG("Record was not written");
+                rc = -EFAULT;
+            }
+        }
+    }
+#else  /* VTSS_USE_UEC */
+    if (unlikely(sktlen > 0xfffb)) {
+        VTSS_STK_LOG("Too big stack len (%d bytes)", sktlen);
+        return -EFAULT;
+    } else {
+        void* entry;
+        stk_trace_record_t* stkrec = (stk_trace_record_t*)vtss_transport_record_reserve(trnd, &entry, sizeof(stk_trace_record_t) - (stk->wow64*8) + sktlen);
+        if (likely(stkrec)) {
+            /// save current alt. stack:
+            /// [flagword - 4b][residx][cpuidx - 4b][tsc - 8b]
+            /// ...[sampled address - 8b][systrace{sts}]
+            ///                       [length - 2b][type - 2b]...
+            stkrec->flagword = UEC_LEAF1 | UECL1_VRESIDX | UECL1_CPUIDX | UECL1_CPUTSC | UECL1_EXECADDR | UECL1_SYSTRACE;
+            stkrec->residx   = tid;
+            stkrec->cpuidx   = cpu;
+            stkrec->cputsc   = vtss_time_cpu();
+            stkrec->execaddr = (unsigned long long)stk->user_ip.szt;
+            stkrec->size     = (unsigned short)sktlen + sizeof(stkrec->size) + sizeof(stkrec->type);
+            stkrec->type     = sample_type;
+            if (!stk->wow64) {
+                stkrec->size += sizeof(void*) + sizeof(void*);
+                stkrec->sp   = stk->user_sp.szt;
+                stkrec->fp   = stk->user_fp.szt;
+            } else { /* a 32-bit stack in a 32-bit process on a 64-bit system */
+                stkrec->size += sizeof(unsigned int) + sizeof(unsigned int);
+                stkrec->sp32 = (unsigned int)stk->user_sp.szt;
+                stkrec->fp32 = (unsigned int)stk->user_fp.szt;
+            }
+            memcpy((char*)stkrec+sizeof(stk_trace_record_t)-(stk->wow64*8), stk->compressed, sktlen);
+            rc = vtss_transport_record_commit(trnd, entry, is_safe);
+            if (rc != 0){
+               VTSS_STK_LOG("Cannot write the record");
+            }
+        }
+    }
+#endif /* VTSS_USE_UEC */
+    return rc;
+}
+
diff --git a/drivers/misc/intel/sepdk/vtsspp/task_map.c b/drivers/misc/intel/sepdk/vtsspp/task_map.c
new file mode 100644
index 000000000000..76180a0922f6
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/task_map.c
@@ -0,0 +1,326 @@
+/*[6~
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "memory_pool.h"
+#include "task_map.h"
+
+#include <linux/jhash.h>
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/nmi.h>
+
+#define VTSS_DEBUG_TASKMAP TRACE
+
+#ifdef VTSS_CONFIG_REALTIME
+static DEFINE_RAW_SPINLOCK(vtss_task_map_lock);
+#else
+static DEFINE_SPINLOCK(vtss_task_map_lock);
+#endif
+
+/* Should be 2^n */
+#define HASH_TABLE_SIZE (1 << 10)
+
+static struct hlist_head vtss_task_map_hash_table[HASH_TABLE_SIZE] = { {NULL} };
+static atomic_t  vtss_map_initialized = ATOMIC_INIT(0);
+/** Compute the map hash */
+static inline u32 vtss_task_map_hash(pid_t key) __attribute__ ((always_inline));
+static inline u32 vtss_task_map_hash(pid_t key)
+{
+    return (jhash_1word(key, 0) & (HASH_TABLE_SIZE - 1));
+}
+
+/**
+ * Reclaim an item after grace period is expired.
+ * Returns void.
+ */
+void vtss_task_map_reclaim_item(struct rcu_head *rp)
+{
+    vtss_task_map_item_t *item = container_of(rp, vtss_task_map_item_t, rcu);
+    VTSS_DEBUG_TASKMAP("start, item = %p", item);
+    if (atomic_read(&item->usage) == 0) {
+        VTSS_DEBUG_TASKMAP("usage 0, item = %p", item);
+        if (item->dtor)
+            item->dtor(item, NULL);
+        item->dtor = NULL;
+        vtss_kfree(item);
+    }
+    VTSS_DEBUG_TASKMAP("end");
+}
+
+/**
+ * Get an item if it's present in the hash table and increment its usage.
+ * Returns NULL if not present.
+ */
+vtss_task_map_item_t* vtss_task_map_get_item(pid_t key)
+{
+    struct hlist_head *head;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+    struct hlist_node *node = NULL;
+#endif
+    vtss_task_map_item_t *item;
+    if (atomic_read(&vtss_map_initialized)==0) return NULL;
+
+    rcu_read_lock();
+    head = &vtss_task_map_hash_table[vtss_task_map_hash(key)];
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+    hlist_for_each_entry_rcu(item, node, head, hlist)
+#else
+    hlist_for_each_entry_rcu(item, head,  hlist)
+#endif
+    {
+        if (key == item->key) {
+            if (atomic_read(&item->in_list)){
+                atomic_inc(&item->usage);
+            } else {
+                item = NULL;
+            }
+            rcu_read_unlock();
+            return item;
+        }
+    }
+    rcu_read_unlock();
+    return NULL;
+}
+
+/**
+ * Decrement count and destroy if usage == 0.
+ * Returns 1 if item was destroyed otherwise 0.
+ */
+int vtss_task_map_put_item(vtss_task_map_item_t* item)
+{
+    unsigned long flags;
+
+    if ((item != NULL) && atomic_dec_and_test(&item->usage)) {
+        //Here soebody can increment usage!
+        if (atomic_read(&item->in_list)) {
+            vtss_spin_lock_irqsave(&vtss_task_map_lock, flags);
+            if (atomic_read(&item->in_list)) {
+                VTSS_DEBUG_TASKMAP("removing from the list");
+                atomic_set(&item->in_list, 0);
+                hlist_del_init_rcu(&item->hlist);
+            }
+            vtss_spin_unlock_irqrestore(&vtss_task_map_lock, flags);
+        }
+        if (atomic_read(&item->usage) == 0) { // do not remove this check
+            VTSS_DEBUG_TASKMAP("before call_rcu, item = %p",item );
+            call_rcu(&item->rcu, vtss_task_map_reclaim_item);
+            VTSS_DEBUG_TASKMAP("after call_rcu");
+            return 1;
+        }
+    }
+
+    return 0;
+}
+
+/**
+ * Add the item into the hash table with incremented usage.
+ * Remove the item with the same key.
+ * Returns 1 if old item was destroyed otherwise 0.
+ */
+int vtss_task_map_add_item(vtss_task_map_item_t* item2)
+{
+    unsigned long flags;
+    int replaced = 0;
+    struct hlist_head *head;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+    struct hlist_node *node = NULL;
+#endif
+    struct hlist_node *temp = NULL;
+    vtss_task_map_item_t *item = NULL;
+
+    if ((item2 != NULL) && !atomic_read(&item2->in_list)) {
+        vtss_spin_lock_irqsave(&vtss_task_map_lock, flags);
+        if (!atomic_read(&item2->in_list))
+        {
+            head = &vtss_task_map_hash_table[vtss_task_map_hash(item2->key)];
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+            hlist_for_each_entry_safe(item, node, temp, head, hlist)
+#else
+            hlist_for_each_entry_safe(item, temp, head, hlist)
+#endif
+            {
+                if (item2->key == item->key) {
+                    /* already there, replace it */
+                    atomic_set(&item->in_list, 0);
+                    atomic_inc(&item2->usage);
+                    hlist_replace_rcu(&item->hlist, &item2->hlist);
+                    atomic_set(&item2->in_list, 1);
+                    if (atomic_read(&item->usage) != 0) {
+                        if (atomic_dec_and_test(&item->usage)) {
+                            VTSS_DEBUG_TASKMAP("before call_rcu, item = %p",item );
+                            call_rcu(&item->rcu, vtss_task_map_reclaim_item);
+                            VTSS_DEBUG_TASKMAP("after call_rcu");
+                        }
+                    }
+                    replaced = 1;
+                    break;
+                }
+            }
+            if (!replaced) {
+                atomic_inc(&item2->usage);
+                hlist_add_head_rcu(&item2->hlist, head);
+                atomic_set(&item2->in_list, 1);
+            }
+        }
+        vtss_spin_unlock_irqrestore(&vtss_task_map_lock, flags);
+    }
+    return replaced;
+}
+
+/**
+ * Remove the item from the hash table and destroy if usage == 0.
+ * Returns 1 if item was destroyed otherwise 0.
+ */
+int vtss_task_map_del_item(vtss_task_map_item_t* item)
+{
+    unsigned long flags;
+
+    if (item != NULL) {
+        if (atomic_read(&item->in_list)) {
+            vtss_spin_lock_irqsave(&vtss_task_map_lock, flags);
+            if (atomic_read(&item->in_list)) {
+                atomic_set(&item->in_list, 0);
+                hlist_del_init_rcu(&item->hlist);
+            }
+            vtss_spin_unlock_irqrestore(&vtss_task_map_lock, flags);
+        }
+        if (atomic_dec_and_test(&item->usage)) {
+            VTSS_DEBUG_TASKMAP("before call_rcu, item = %p",item );
+            call_rcu(&item->rcu, vtss_task_map_reclaim_item);
+            VTSS_DEBUG_TASKMAP("after call_rcu");
+            return 1;
+        }
+    }
+    return 0;
+}
+
+/**
+ * allocate item + data but not insert it into the hash table, usage = 1
+ */
+vtss_task_map_item_t* vtss_task_map_alloc(pid_t key, size_t size, vtss_task_map_func_t* dtor, gfp_t flags)
+{
+    vtss_task_map_item_t *item = NULL;
+    
+    if (atomic_read(&vtss_map_initialized) == 0)
+        return NULL;
+    item = (vtss_task_map_item_t*)vtss_kmalloc(sizeof(vtss_task_map_item_t) + size, flags);
+    if (item != NULL) {
+        memset(item, 0, sizeof(vtss_task_map_item_t) + size);
+        atomic_set(&item->usage, 1);
+        item->key     = key;
+        atomic_set(&item->in_list,0);
+        item->dtor    = dtor;
+    }
+    return item;
+}
+
+int vtss_task_map_foreach(vtss_task_map_func_t* func, void* args)
+{
+    int i;
+    struct hlist_head *head;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+    struct hlist_node *node = NULL;
+#endif
+    vtss_task_map_item_t *item;
+
+    if (func == NULL) {
+        ERROR("Function pointer is NULL");
+        return -EINVAL;
+    }
+    rcu_read_lock();
+    for (i = 0; i < HASH_TABLE_SIZE; i++) {
+        head = &vtss_task_map_hash_table[i];
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+        hlist_for_each_entry_rcu(item, node, head, hlist)
+#else
+        hlist_for_each_entry_rcu(item, head, hlist)
+#endif
+        {
+            func(item, args);
+        }
+    }
+    rcu_read_unlock();
+    return 0;
+}
+
+int vtss_task_map_init(void)
+{
+    int i;
+    unsigned long flags;
+    struct hlist_head *head;
+
+    vtss_spin_lock_irqsave(&vtss_task_map_lock, flags);
+    for (i = 0; i < HASH_TABLE_SIZE; i++) {
+        head = &vtss_task_map_hash_table[i];
+        INIT_HLIST_HEAD(head);
+    }
+    vtss_spin_unlock_irqrestore(&vtss_task_map_lock, flags);
+    synchronize_rcu();
+    
+    atomic_set(&vtss_map_initialized,1);
+
+    return 0;
+}
+
+void vtss_task_map_fini(void)
+{
+    int i;
+    unsigned long flags;
+    struct hlist_head *head;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+    struct hlist_node *node = NULL;
+#endif
+    struct hlist_node *temp = NULL;
+    vtss_task_map_item_t *item = NULL;
+
+    atomic_set(&vtss_map_initialized,0);
+    vtss_spin_lock_irqsave(&vtss_task_map_lock, flags);
+    for (i = 0; i < HASH_TABLE_SIZE; i++) {
+        head = &vtss_task_map_hash_table[i];
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0)
+        hlist_for_each_entry_safe(item, node, temp, head, hlist)
+#else
+        hlist_for_each_entry_safe(item, temp, head, hlist)
+#endif
+        {
+            atomic_set(&item->in_list, 0);
+            hlist_del_init_rcu(&item->hlist);
+            if (atomic_read(&item->usage) != 0) {
+                if (atomic_dec_and_test(&item->usage)) {
+                    VTSS_DEBUG_TASKMAP("before call_rcu, item = %p", item);
+                    call_rcu(&item->rcu, vtss_task_map_reclaim_item);
+                    VTSS_DEBUG_TASKMAP("after call_rcu");
+                }
+            }
+        }
+        INIT_HLIST_HEAD(head);
+    }
+    vtss_spin_unlock_irqrestore(&vtss_task_map_lock, flags);
+    synchronize_rcu();
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/transport.c b/drivers/misc/intel/sepdk/vtsspp/transport.c
new file mode 100644
index 000000000000..42cad39b921b
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/transport.c
@@ -0,0 +1,2187 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "transport.h"
+#include "procfs.h"
+#include "globals.h"
+#include "memory_pool.h"
+#ifdef VTSS_USE_UEC
+#include "uec.h"
+#else
+#include <linux/ring_buffer.h>
+#include <asm/local.h>
+#endif
+
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/poll.h>
+#include <linux/delay.h>        /* for msleep_interruptible() */
+#include <linux/fs.h>           /* for struct file_operations */
+#include <linux/namei.h>        /* for struct nameidata       */
+#include <linux/spinlock.h>
+#include <asm/uaccess.h>
+#include <linux/slab.h>
+#include <linux/nmi.h>
+
+#include "vtsstrace.h"
+#include "time.h"
+#define DEBUG_TR TRACE
+
+#ifndef VTSS_MERGE_MEM_LIMIT
+#define VTSS_MERGE_MEM_LIMIT 0x200 /* max pages allowed */
+#endif
+
+/* Define this to wake up transport by timeout */
+/* transprot timer interval in jiffies  (default 10ms) */
+#define VTSS_TRANSPORT_TIMER_INTERVAL   (10 * HZ / 1000)
+#define VTSS_TRANSPORT_COMPLETE_TIMEOUT 10000 /*< wait count about 100sec */
+
+#ifdef CONFIG_X86_64
+#define VTSS_ALLOC_BUFSIZE_MAX 0x10000000L
+#else
+#define VTSS_ALLOC_BUFSIZE_MAX 0x2000000L
+#endif
+
+#define VTSS_PREALLOC_TR_SIZE 5
+
+#ifndef VTSS_USE_UEC
+
+struct vtss_transport_entry
+{
+    unsigned long  seqnum;
+    unsigned short size;
+    unsigned long  rb_start;
+    unsigned long long  cputsc; //record creation time
+    char           data[0];
+};
+
+struct vtss_transport_temp
+{
+    struct vtss_transport_temp* prev;
+    struct vtss_transport_temp* next;
+    unsigned long seq_begin;
+    unsigned long seq_end;
+    size_t        size;
+    unsigned int  order;
+    char          data[0];
+};
+
+/* The value was gotten from the kernel's ring_buffer code. */
+#define VTSS_RING_BUFFER_PAGE_SIZE      4080
+#define VTSS_TRANSPORT_MAX_RESERVE_SIZE (VTSS_RING_BUFFER_PAGE_SIZE - \
+                                        sizeof(struct ring_buffer_event) - \
+                                        sizeof(struct vtss_transport_entry) - 64)
+#define VTSS_TRANSPORT_IS_EMPTY(trnd)   (1 + (atomic_read(&trnd->seqnum) - atomic_read(&trnd->seqdone)) == 0)
+#define VTSS_TRANSPORT_DATA_READY(trnd) (1 + atomic_read(&trnd->commited) - atomic_read(&trnd->seqdone) > 0/*VTSS_MERGE_MEM_LIMIT/4 || atomic_read(&trnd->is_overflow)*/)
+
+struct rb_page
+{
+    u64     ts;
+    local_t commit;
+    char    data[VTSS_RING_BUFFER_PAGE_SIZE];
+};
+
+
+#endif /* VTSS_USE_UEC */
+
+
+#define VTSS_RB_STEP 0x1000
+#define VTSS_RB_MASK (VTSS_RB_STEP-1)
+#define VTSS_RB_MARK(num)(num - (num&VTSS_RB_MASK)+ VTSS_RB_STEP)
+
+extern int uid;
+extern int gid;
+extern int mode;
+
+static struct timer_list vtss_transport_timer;
+
+#ifdef VTSS_CONFIG_REALTIME
+static DEFINE_RAW_SPINLOCK(vtss_transport_list_lock);
+#else
+static DEFINE_SPINLOCK(vtss_transport_list_lock);
+#endif
+static LIST_HEAD(vtss_transport_list);
+static atomic_t vtss_free_tr_cnt = ATOMIC_INIT(0);
+static atomic_t vtss_transport_mode = ATOMIC_INIT(VTSS_TR_MODE_REG);
+static atomic_t vtss_transport_npages = ATOMIC_INIT(0);
+static atomic_t vtss_is_transport_init = ATOMIC_INIT(0);
+static atomic_t vtss_kernel_task_in_progress = ATOMIC_INIT(0);
+static atomic_t vtss_ring_buffer_stopped = ATOMIC_INIT(0);
+static atomic_t vtss_ring_buffer_paused = ATOMIC_INIT(0);
+
+void vtss_transport_start_ring_bufer(void)
+{
+    atomic_set(&vtss_ring_buffer_stopped, 0);
+}
+
+void vtss_transport_stop_ring_bufer(void)
+{
+    atomic_set(&vtss_ring_buffer_stopped, 1);
+}
+
+void vtss_transport_resume_ring_bufer(void)
+{
+    atomic_set(&vtss_ring_buffer_paused, 0);
+}
+
+void vtss_transport_pause_ring_bufer(void)
+{
+    atomic_set(&vtss_ring_buffer_paused, 1);
+}
+
+#define VTSS_TR_REG    0x1
+#define VTSS_TR_CFG    0x2 /* aux */
+#define VTSS_TR_RB     0x4 /* ring buffer */
+
+#if defined(__i386__)
+#define VTSS_UEC_CHAIN_SIZE 16
+#else
+#define VTSS_UEC_CHAIN_SIZE 32
+#endif
+
+#define VTSS_TRANSPORT_COPY_TO_USER(src, len) do { \
+    if (buf){ \
+        if (copy_to_user(buf, (void*)(src), (len))) { \
+            ERROR("copy_to_user(0x%p, 0x%p, %zu): error", buf, (src), (len)); \
+        } \
+        size -= (len); \
+        buf += (len); \
+    } \
+    rc += (len); \
+} while (0)
+
+struct vtss_transport_data
+{
+    struct list_head    list;
+    struct file*        file;
+    wait_queue_head_t   waitq;
+    char                name[36];    /* enough for "%d-%d.%d.aux" */
+
+    atomic_t            refcount;
+    atomic_t            loscount;
+    atomic_t            is_attached;
+    atomic_t            is_complete;
+    atomic_t            is_overflow;
+
+    atomic_t            reserved;
+
+    atomic_t            seqdone;
+    atomic_t            rb_mark;
+    int                 magic;
+
+#ifdef VTSS_USE_UEC
+    uec_t*              uec;
+    uec_t uec_chain[VTSS_UEC_CHAIN_SIZE];
+#else
+    struct vtss_transport_temp* head;
+
+    struct ring_buffer* buffer;
+
+    unsigned long       seqcpu[NR_CPUS];
+    atomic_t            seqnum;
+    atomic_t            commited;
+    int                 is_abort;
+
+    atomic_t            locked_size_cpu[NR_CPUS];
+    int       ring_buffer_size;
+    //0 - writing
+    //1 - clearning
+    //2 - reading
+    atomic_t processing_state;
+
+    unsigned long long bufcputsc;
+#endif
+    int type;
+};
+
+void vtss_transport_addref(struct vtss_transport_data* trnd)
+{
+    atomic_inc(&trnd->refcount);
+}
+
+int vtss_transport_delref(struct vtss_transport_data* trnd)
+{
+    return atomic_dec_return(&trnd->refcount);
+}
+
+char *vtss_transport_get_filename(struct vtss_transport_data* trnd)
+{
+    return trnd->name;
+}
+
+int vtss_transport_is_overflowing(struct vtss_transport_data* trnd)
+{
+    return atomic_read(&trnd->is_overflow);
+}
+int vtss_transport_is_attached(struct vtss_transport_data* trnd)
+{
+    return atomic_read(&trnd->is_attached);
+}
+int vtss_transport_is_ready(struct vtss_transport_data* trnd)
+{
+    /*if (atomic_read(&trnd->is_complete)) {
+        TRACE("Transport is COMPLETED");
+        return 0;
+    }
+    return (trnd->seqdone > 1 || waitqueue_active(&trnd->waitq));*/
+    return atomic_read(&trnd->is_attached);
+}
+
+#if 0
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_transport_data_wake_up_work(struct work_struct *work)
+#else
+static void vtss_transport_data_wake_up_work(void *work)
+#endif
+{
+    struct vtss_work* my_work = (struct vtss_work*)work;
+    struct vtss_transport_data* trnd = NULL;
+
+    if (!my_work){
+        ERROR("empty work!");
+        return;
+    }
+    if (atomic_read(&vtss_is_transport_init) == 0)
+    {
+        vtss_kfree(my_work);
+        return;
+    }
+    
+    trnd = *((struct vtss_transport_data**)(&my_work->data));
+
+    if (atomic_read(&trnd->is_attached) == 0) 
+    {
+        vtss_kfree(my_work);
+        return;
+    }
+
+    if (atomic_read(&trnd->is_complete))
+    {
+        vtss_kfree(my_work);
+        return;
+    }
+
+
+    if (trnd->type & VTSS_TR_RB)
+    {
+        vtss_kfree(my_work);
+        return;
+    }
+
+    
+    while (!waitqueue_active(&trnd->waitq))
+    {
+        msleep_interruptible(10);
+        if (atomic_read(&vtss_is_transport_init) == 0)
+        {
+           vtss_kfree(my_work);
+           return;
+        }
+    }
+
+    wake_up_interruptible(&trnd->waitq);
+    
+    vtss_kfree(my_work);
+}
+#endif
+
+#ifdef VTSS_USE_UEC
+
+void vtss_transport_callback(uec_t* uec, int reason, void *context)
+{
+    TRACE("context=0x%p, reason=%d", context, reason);
+}
+
+#define UEC_FREE_SIZE(uec) \
+({ (uec->tail <= uec->head) ? \
+        uec->hsize - (size_t)(uec->head - uec->tail) \
+    : \
+        (size_t)(uec->tail - uec->head); \
+})
+
+#define UEC_FILLED_SIZE(uec) \
+({  size_t tsize = 0; \
+    if (uec->head > uec->tail) { \
+        tsize = (size_t)(uec->head - uec->tail); \
+    } else if (uec->head < uec->tail || (uec->head == uec->tail && uec->ovfl)) { \
+        tsize = (size_t)(uec->tsize - (uec->tail - uec->buffer)); \
+    } \
+    tsize; \
+})
+
+#define VTSS_TRANSPORT_IS_EMPTY(trnd)   (UEC_FILLED_SIZE(trnd->uec) == 0)
+#define VTSS_TRANSPORT_DATA_READY(trnd) (UEC_FILLED_SIZE(trnd->uec) != 0)
+ 
+int vtss_transport_record_write(struct vtss_transport_data* trnd, void* part0, size_t size0, void* part1, size_t size1, int is_safe)
+{
+    int rc = 0;
+
+    if (trnd == NULL) {
+        ERROR("Transport is NULL");
+        return -EINVAL;
+    }
+
+    if (atomic_read(&trnd->is_complete)) {
+        ERROR("Transport is COMPLETED");
+        return -EINVAL;
+    }
+
+    /* Don't use spill notifications from uec therefore its UECMODE_SAFE always */
+    if (trnd->type & VTSS_TR_RB) {
+        rc = trnd->uec_chain->put_record(trnd->uec_chain, part0, size0, part1, size1, UECMODE_SAFE);
+    } else {
+        rc = trnd->uec->put_record(trnd->uec, part0, size0, part1, size1, UECMODE_SAFE);
+
+#ifndef VTSS_USE_NMI
+        if (is_safe) {
+            DEBUG_TR("WAKE UP");
+            if (waitqueue_active(&trnd->waitq))
+                wake_up_interruptible(&trnd->waitq);
+        }
+#endif
+    }
+    if (rc) {
+        atomic_inc(&trnd->loscount);
+    }
+    return rc;
+}
+
+#else  /* VTSS_USE_UEC */
+
+static void vtss_transport_temp_free_all(struct vtss_transport_data* trnd, struct vtss_transport_temp** head)
+{
+    struct vtss_transport_temp* temp;
+    struct vtss_transport_temp** pstore = head;
+
+    while ((temp = *pstore) != NULL) {
+        if (temp->prev) {
+            pstore = &(temp->prev);
+            continue;
+        }
+        if (temp->next) {
+            pstore = &(temp->next);
+            continue;
+        }
+        TRACE("'%s' [%lu, %lu), size=%zu of %lu",
+                trnd->name, temp->seq_begin, temp->seq_end,
+                temp->size, (PAGE_SIZE << temp->order));
+        {
+            unsigned int temp_order = temp->order;
+            vtss_free_pages((unsigned long)temp, temp->order);
+            atomic_sub(1<<temp_order, &vtss_transport_npages);
+        }
+        *pstore = NULL;
+        pstore = head; /* restart from head */
+    }
+}
+
+static struct vtss_transport_temp* vtss_transport_temp_merge(struct vtss_transport_data* trnd, struct vtss_transport_temp** pstore)
+{
+    struct vtss_transport_temp* temp = *pstore;
+
+    if (temp != NULL) {
+        struct vtss_transport_temp* prev = temp->prev;
+        struct vtss_transport_temp* next = temp->next;
+
+        /* try to merge with prev element... */
+        if (prev != NULL && (prev->seq_end == temp->seq_begin) && ((VTSS_RB_MASK&temp->seq_begin) != 1)&&
+            /* check for enough space in buffer */
+            ((prev->size + temp->size + sizeof(struct vtss_transport_temp)) < (PAGE_SIZE << prev->order)))
+        {
+            TRACE("'%s' [%lu - %lu), size=%zu <+ [%lu - %lu), size=%zu", trnd->name,
+                prev->seq_begin, prev->seq_end, prev->size,
+                temp->seq_begin, temp->seq_end, temp->size);
+            memcpy(&(prev->data[prev->size]), temp->data, temp->size);
+            prev->size += temp->size;
+            prev->seq_end = temp->seq_end;
+            if (prev->next) {
+                ERROR("'%s' [%lu, %lu) incorrect next link", trnd->name,
+                        prev->seq_begin, prev->seq_end);
+                vtss_transport_temp_free_all(trnd, &(prev->next));
+            }
+            prev->next = temp->next;
+            *pstore = prev;
+            {
+                unsigned int temp_order = temp->order;
+                vtss_free_pages((unsigned long)temp, temp->order);
+                atomic_sub(1<<temp_order, &vtss_transport_npages);
+            }
+            return prev;
+        }
+        /* try to merge with next element... */
+        if (next != NULL && (next->seq_begin == temp->seq_end) && ((VTSS_RB_MASK&next->seq_begin) != 1)&&
+            /* check for enough space in buffer */
+            ((next->size + temp->size + sizeof(struct vtss_transport_temp)) < (PAGE_SIZE << temp->order)))
+        {
+            TRACE("'%s' [%lu - %lu), size=%zu +> [%lu - %lu), size=%zu", trnd->name,
+                temp->seq_begin, temp->seq_end, temp->size,
+                next->seq_begin, next->seq_end, next->size);
+            memcpy(&(temp->data[temp->size]), next->data, next->size);
+            temp->size += next->size;
+            temp->seq_end = next->seq_end;
+            temp->next = next->next;
+            if (next->prev) {
+                ERROR("'%s' [%lu, %lu) incorrect prev link", trnd->name,
+                        next->seq_begin, next->seq_end);
+                vtss_transport_temp_free_all(trnd, &(next->prev));
+            }
+            {
+                unsigned int next_order = next->order;
+                vtss_free_pages((unsigned long)next, next->order);
+                atomic_sub(1<<next_order, &vtss_transport_npages);
+            }
+            return temp;
+        }
+    }
+    return temp;
+}
+
+static int vtss_transport_temp_store_data(struct vtss_transport_data* trnd, unsigned long seqnum, void* data, unsigned short size)
+{
+    struct vtss_transport_temp* temp = NULL;
+    struct vtss_transport_temp** pstore = &(trnd->head);
+    unsigned int order = get_order(size + sizeof(struct vtss_transport_temp));
+
+    while (((temp = vtss_transport_temp_merge(trnd, pstore)) != NULL) && (seqnum != temp->seq_end)) {
+        TRACE("0: '%s' new [%lu - %lu), size=%u", trnd->name, seqnum, seqnum + 1, size);
+        pstore = (seqnum < temp->seq_begin) ? &(temp->prev) : &(temp->next);
+    }
+    if (temp == NULL) {
+        struct vtss_transport_temp* temp1;
+        struct vtss_transport_temp** pstore1 = &(trnd->head);
+        while (((temp1 = *pstore1) != NULL) && ((seqnum + 1) != temp1->seq_begin)) {
+            pstore1 = (seqnum < temp1->seq_begin) ? &(temp1->prev) : &(temp1->next);
+        }
+        if (temp1 != NULL) { /* try to prepend */
+            /* check for enough space in buffer */
+            if ((temp1->size + size + sizeof(struct vtss_transport_temp)) < (PAGE_SIZE << temp1->order) && (VTSS_RB_MASK&seqnum) != 1) {
+                TRACE("1: '%s' [%lu - %lu), size=%u +> [%lu - %lu), size=%zuv",
+                        trnd->name, seqnum, seqnum + 1, size,
+                        temp1->seq_begin, temp1->seq_end, temp1->size);
+                memmove(&(temp1->data[size]), temp1->data, temp1->size);
+                memcpy(temp1->data, data, size);
+                temp1->seq_begin = seqnum;
+                temp1->size += size;
+                vtss_transport_temp_merge(trnd, pstore1);
+                return 0;
+            }
+        }
+        TRACE("2: '%s' new [%lu - %lu), size=%u", trnd->name, seqnum, seqnum + 1, size);
+        temp = (struct vtss_transport_temp*)vtss_get_free_pages(GFP_NOWAIT, order);
+        if (temp == NULL) {
+            return -ENOMEM;
+        }
+        atomic_add(1<<order, &vtss_transport_npages);
+        temp->prev  = NULL;
+        temp->next  = NULL;
+        temp->seq_begin = seqnum;
+        temp->size  = 0;
+        temp->order = order;
+        if (*pstore) {
+            ERROR("'%s' new [%lu - %lu), size=%u ==> [%lu - %lu)", trnd->name,
+                    seqnum, seqnum + 1, size, (*pstore)->seq_begin, (*pstore)->seq_end);
+        }
+        *pstore = temp;
+    } else {
+        /* check for enough space in buffer */
+        if ((temp->size + size + sizeof(struct vtss_transport_temp)) >= (PAGE_SIZE << temp->order) || (VTSS_RB_MASK&seqnum) == 1) {
+            struct vtss_transport_temp* next;
+            TRACE("3: '%s' new [%lu - %lu), size=%u, temp->size=%zu", trnd->name,
+                    seqnum, seqnum + 1, size, temp->size);
+            next = (struct vtss_transport_temp*)vtss_get_free_pages(GFP_NOWAIT, order);
+            if (next == NULL) {
+                return -ENOMEM;
+            }
+            atomic_add(1<<order, &vtss_transport_npages);
+            next->prev  = NULL;
+            next->next  = temp->next;
+            next->seq_begin = seqnum;
+            next->size  = 0;
+            next->order = order;
+            temp->next  = next;
+            pstore = &(temp->next);
+            temp = next;
+        } else {
+            TRACE("4:'%s' [%lu - %lu), size=%zu <+ [%lu - %lu), size=%u", trnd->name,
+                    temp->seq_begin, temp->seq_end, temp->size,
+                    seqnum, seqnum + 1, size);
+        }
+    }
+    memcpy(&(temp->data[temp->size]), data, size);
+    temp->seq_end = seqnum + 1;
+    temp->size += size;
+    vtss_transport_temp_merge(trnd, pstore);
+    return 0;
+}
+
+static int vtss_transport_temp_store_blob(struct vtss_transport_data* trnd, unsigned long seqnum, struct vtss_transport_temp* blob)
+{
+    struct vtss_transport_temp* temp;
+    struct vtss_transport_temp** pstore = &(trnd->head);
+
+    TRACE("'%s' blob [%lu - %lu), size=%zu", trnd->name, seqnum, seqnum + 1, blob->size);
+    while (((temp = vtss_transport_temp_merge(trnd, pstore)) != NULL) && (seqnum != temp->seq_end)) {
+        pstore = (seqnum < temp->seq_begin) ? &(temp->prev) : &(temp->next);
+    }
+    blob->prev      = NULL;
+    blob->seq_begin = seqnum;
+    blob->seq_end   = seqnum + 1;
+    if (temp == NULL) {
+        blob->next = NULL;
+        if (*pstore) {
+            ERROR("'%s' blob [%lu - %lu), size=%zu ==> [%lu - %lu)", trnd->name,
+                    seqnum, seqnum + 1, blob->size, (*pstore)->seq_begin, (*pstore)->seq_end);
+        }
+        *pstore = blob;
+    } else {
+        blob->next = temp->next;
+        temp->next = blob;
+    }
+    return 0;
+}
+
+static size_t vtss_transport_temp_flush(struct vtss_transport_data* trnd, char __user* buf, size_t size, int max_seqdone)
+{
+    size_t rc = 0;
+
+    if (!trnd->is_abort) {
+        struct vtss_transport_temp* temp;
+        struct vtss_transport_temp** pstore = &(trnd->head);
+
+        TRACE("'%s' -== flush begin ==- :: %d (%d bytes) at seq=%d", trnd->name,
+                atomic_read(&vtss_transport_npages), (int)(atomic_read(&vtss_transport_npages)*PAGE_SIZE), atomic_read(&trnd->seqdone));
+        /* Look for output seq with merge on the way */
+        while ((temp = vtss_transport_temp_merge(trnd, pstore)) != NULL) {
+            if (atomic_read(&trnd->seqdone) == temp->seq_begin) {
+                if (buf && (size < temp->size))
+                    break;
+                TRACE("'%s' output [%lu, %lu), size=%zu", trnd->name,
+                        temp->seq_begin, temp->seq_end, temp->size);
+                if (max_seqdone > 0 && temp->seq_begin > max_seqdone){
+                    DEBUG_TR("flush stopped, temp->seq_begin = %d, maxseqdone = %d", (int)temp->seq_begin, max_seqdone);
+                    break;
+                }
+                VTSS_TRANSPORT_COPY_TO_USER(temp->data, temp->size);
+                atomic_set(&trnd->seqdone,temp->seq_end);
+                *pstore = temp->next;
+                if (temp->prev) {
+                    ERROR("'%s' [%lu, %lu) incorrect prev link", trnd->name, temp->seq_begin, temp->seq_end);
+                    vtss_transport_temp_free_all(trnd, &(temp->prev));
+                }
+                {
+                    unsigned int temp_order = temp->order;
+                    DEBUG_TR("temp = %p, temp->next = %p, *pstore = %p", temp, temp->next, *pstore);
+                    vtss_free_pages((unsigned long)temp, temp->order);
+                    atomic_sub(1<<temp_order, &vtss_transport_npages);
+                }
+                pstore = &(trnd->head); /* restart from head */
+            } else {
+                pstore = (atomic_read(&trnd->seqdone) < temp->seq_begin) ? &(temp->prev) : &(temp->next);
+            }
+        }
+        TRACE("'%s' -== flush  end  ==- :: %d (%d bytes) at seq=%d", trnd->name,
+                atomic_read(&vtss_transport_npages), (int)(atomic_read(&vtss_transport_npages)*PAGE_SIZE), atomic_read(&trnd->seqdone));
+    }
+    return rc;
+}
+
+static size_t vtss_transport_temp_parse_event(struct vtss_transport_data* trnd, struct ring_buffer_event* event, char __user* buf, size_t size, int cpu)
+{
+    size_t rc = 0;
+    struct vtss_transport_entry* data = (struct vtss_transport_entry*)ring_buffer_event_data(event);
+    unsigned long seqnum = data->seqnum;
+
+    atomic_sub(data->size + sizeof(struct vtss_transport_entry), &trnd->locked_size_cpu[cpu]);
+    trnd->seqcpu[cpu] = seqnum;
+    if (trnd->is_abort || atomic_read(&trnd->seqdone) > seqnum) {
+        if (data->size) {
+            DEBUG_TR("DROP seqdone=%d, seq=%lu, size=%u, from cpu%d", atomic_read(&trnd->seqdone), seqnum, data->size, cpu);
+        } else { /* blob */
+            struct vtss_transport_temp* blob = *((struct vtss_transport_temp**)(data->data));
+            unsigned int blob_order;
+            DEBUG_TR("DROP seq=%lu, size=%zu, from cpu%d", seqnum, blob->size, cpu);
+            blob_order = blob->order;
+            vtss_free_pages((unsigned long)blob, blob->order);
+            atomic_sub(1<<blob_order, &vtss_transport_npages);
+    }
+#ifndef VTSS_NO_MERGE
+    } else if (atomic_read(&trnd->seqdone) != seqnum) { /* disordered event */
+        if (data->size) {
+            if (vtss_transport_temp_store_data(trnd, seqnum, data->data, data->size)) {
+                ERROR("'%s' seq=%d => store data seq=%lu error", trnd->name,atomic_read(&trnd->seqdone), seqnum);
+            }
+        } else { /* blob */
+            struct vtss_transport_temp* blob = *((struct vtss_transport_temp**)(data->data));
+             if (vtss_transport_temp_store_blob(trnd, seqnum, blob)) {
+                ERROR("'%s' seq=%d => store blob seq=%lu error", trnd->name, (int)atomic_read(&trnd->seqdone), seqnum);
+            }
+        }
+#endif
+    } else { /* ordered event */
+        if (data->size) {
+            TRACE("'%s' output [%lu - %lu), size=%u from cpu%d", trnd->name, seqnum, seqnum+1, data->size, cpu);
+#ifndef VTSS_NO_MERGE
+            if (buf && (size < data->size)) {
+                if (vtss_transport_temp_store_data(trnd, seqnum, data->data, data->size)) {
+                    ERROR("'%s' seq=%d => store data seq=%lu error", trnd->name, atomic_read(&trnd->seqdone), seqnum);
+                }
+            } else
+#endif
+            {
+                VTSS_TRANSPORT_COPY_TO_USER(data->data, (size_t)data->size);
+                atomic_inc(&trnd->seqdone);
+            }
+        } else { /* blob */
+            struct vtss_transport_temp* blob = *((struct vtss_transport_temp**)(data->data));
+            TRACE("'%s' output [%lu - %lu), size=%zu from cpu%d", trnd->name, seqnum, seqnum+1, blob->size, cpu);
+#ifndef VTSS_NO_MERGE
+            if (buf && (size < blob->size)) {
+                if (vtss_transport_temp_store_blob(trnd, seqnum, blob)) {
+                    ERROR("'%s' seq=%d => store blob seq=%lu error", trnd->name, atomic_read(&trnd->seqdone), seqnum);
+                }
+            } else
+#endif
+            {
+                unsigned int blob_order;
+                VTSS_TRANSPORT_COPY_TO_USER(blob->data,
+#ifndef VTSS_NO_MERGE
+                    blob->size
+#else
+                    (size_t)8UL /* FIXME: just something is not overflowed output buffer */
+#endif
+                );
+                blob_order = blob->order;
+                DEBUG_TR("remove blob = %p", blob);
+                vtss_free_pages((unsigned long)blob, blob->order);
+                atomic_sub(1<<blob_order, &vtss_transport_npages);
+                atomic_inc(&trnd->seqdone);
+
+            }
+        }
+    }
+    return rc;
+}
+
+static ssize_t vtss_transport_read_rb(struct vtss_transport_data* trnd, char __user* buf, size_t size)
+{
+    int i, cpu;
+    size_t len;
+    ssize_t rc = 0;
+
+    for (i = 0;
+        (i < 30) && /* no more 30 loops on each online cpu */
+        (size >= VTSS_RING_BUFFER_PAGE_SIZE) && /* while buffer size is enough */
+        !VTSS_TRANSPORT_IS_EMPTY(trnd) /* have something to output */
+        ; i++)
+    {
+        int fast = 1;
+        void* bpage;
+        struct ring_buffer_event *event;
+        unsigned long seqdone = (unsigned long)atomic_read(&trnd->seqdone);
+
+        for_each_online_cpu(cpu) {
+#ifndef VTSS_NO_MERGE
+            /* Flush buffers if possible first of all */
+            len = vtss_transport_temp_flush(trnd, buf, size, -1);
+            size -= len;
+            buf += len;
+            rc += len;
+#endif
+            if (ring_buffer_entries_cpu(trnd->buffer, cpu) == 0)
+                continue; /* nothing to read on this cpu */
+            DEBUG_TR("cpu = %d, seqcpu(cpu) = %d, seqdone = %d, locked_size=%d", cpu,(int)trnd->seqcpu[cpu], (int)seqdone, atomic_read(&trnd->locked_size_cpu[cpu]) );
+            if  (trnd->seqcpu[cpu] > seqdone &&
+                (((1 + trnd->seqcpu[cpu] - seqdone) > VTSS_MERGE_MEM_LIMIT/3 &&
+                !trnd->is_abort && !atomic_read(&trnd->is_complete)) || (atomic_read(&vtss_transport_npages) > VTSS_MERGE_MEM_LIMIT)))
+            {
+                DEBUG_TR("'%s' cpu%d=%lu :: %u (%lu bytes) at seq=%lu", trnd->name,
+                        cpu, trnd->seqcpu[cpu], atomic_read(&vtss_transport_npages), atomic_read(&vtss_transport_npages)*PAGE_SIZE, seqdone);
+                continue; /* skip it for a while */
+            }
+            if (atomic_read(&vtss_transport_npages) > VTSS_MERGE_MEM_LIMIT/2) {
+                DEBUG_TR("'%s' cpu%d=%lu :: %u (%lu bytes) at seq=%lu", trnd->name,
+                        cpu, trnd->seqcpu[cpu], atomic_read(&vtss_transport_npages), atomic_read(&vtss_transport_npages)*PAGE_SIZE, seqdone);
+                fast = 0; // Carefully get events to avoid memory overflow
+            }
+#ifdef VTSS_AUTOCONF_RING_BUFFER_ALLOC_READ_PAGE
+            bpage = ring_buffer_alloc_read_page(trnd->buffer, cpu);
+#else
+            bpage = ring_buffer_alloc_read_page(trnd->buffer);
+#endif
+            if (bpage == NULL) {
+                ERROR("'%s' cannot allocate free rb read page", trnd->name);
+                return -EFAULT;
+            }
+            if (fast && ring_buffer_read_page(trnd->buffer, &bpage, PAGE_SIZE, cpu, (!trnd->is_abort && !atomic_read(&trnd->is_complete))) >= 0) {
+                int i, inc;
+                struct rb_page* rpage = (struct rb_page*)bpage;
+                /* The commit may have missed event flags set, clear them */
+                unsigned long commit = local_read(&rpage->commit) & 0xfffff;
+
+                TRACE("page[%d]=[0 - %4lu) :: rc=%zd, size=%zu", cpu, commit, rc, size);
+                for (i = 0; i < commit; i += inc) {
+                    if (i >= (PAGE_SIZE - offsetof(struct rb_page, data))) {
+                        ERROR("'%s' incorrect data index", trnd->name);
+                        break;
+                    }
+                    inc = -1;
+                    event = (void*)&rpage->data[i];
+                    switch (event->type_len) {
+                    case RINGBUF_TYPE_PADDING:
+                        /* failed writes or may be discarded events */
+                        inc = event->array[0] + 4;
+                        break;
+                    case RINGBUF_TYPE_TIME_EXTEND:
+                        inc = 8;
+                        break;
+                    case RINGBUF_TYPE_TIME_STAMP:
+                        inc = 16;
+                        break;
+                    case 0:
+                        len = vtss_transport_temp_parse_event(trnd, event, buf, size, cpu);
+                        size -= len;
+                        buf += len;
+                        rc += len;
+                        if (!event->array[0]) {
+                            ERROR("'%s' incorrect event data", trnd->name);
+                            break;
+                        }
+                        inc = event->array[0] + 4;
+                        break;
+                    default:
+                       if (event->type_len <= RINGBUF_TYPE_DATA_TYPE_LEN_MAX){
+                            len = vtss_transport_temp_parse_event(trnd, event, buf, size, cpu);
+                            size -= len;
+                            buf += len;
+                            rc += len;
+                            inc = ((event->type_len + 1) * 4);
+                        }
+                    }
+                    if (inc <= 0) {
+                        ERROR("'%s' incorrect next data index", trnd->name);
+                        break;
+                    }
+                } /* for each event in page */
+                TRACE("page[%d] -==end==-  :: rc=%zd, size=%zu", cpu, rc, size);
+            } else { /* reader page is not full of careful, so read one by one */
+                u64 ts;
+                int count;
+
+                for (count = 0; count < 10 && NULL !=
+#ifdef VTSS_AUTOCONF_RING_BUFFER_LOST_EVENTS
+                    (event = ring_buffer_peek(trnd->buffer, cpu, &ts, NULL))
+#else
+                    (event = ring_buffer_peek(trnd->buffer, cpu, &ts))
+#endif
+                    ; count++)
+                {
+                    struct vtss_transport_entry* data = (struct vtss_transport_entry*)ring_buffer_event_data(event);
+
+                    trnd->seqcpu[cpu] = data->seqnum;
+                    if ((1 + trnd->seqcpu[cpu] - seqdone) > VTSS_MERGE_MEM_LIMIT/4 &&
+                        !trnd->is_abort && !atomic_read(&trnd->is_complete))
+                    {
+                        break; /* will not read this event */
+                    }
+#ifdef VTSS_AUTOCONF_RING_BUFFER_LOST_EVENTS
+                    event = ring_buffer_consume(trnd->buffer, cpu, &ts, NULL);
+#else
+                    event = ring_buffer_consume(trnd->buffer, cpu, &ts);
+#endif
+                    if (event != NULL) {
+                        len = vtss_transport_temp_parse_event(trnd, event, buf, size, cpu);
+                        size -= len;
+                        buf += len;
+                        rc += len;
+                    }
+                } /* for */
+            }
+#ifdef VTSS_AUTOCONF_RING_BUFFER_FREE_READ_PAGE
+            ring_buffer_free_read_page(trnd->buffer, cpu, bpage);
+#else
+            ring_buffer_free_read_page(trnd->buffer, bpage);
+#endif
+#ifndef VTSS_NO_MERGE
+            /* Flush buffers if possible */
+            len = vtss_transport_temp_flush(trnd, buf, size, -1);
+            size -= len;
+            buf += len;
+            rc += len;
+#endif
+            if (size < VTSS_RING_BUFFER_PAGE_SIZE) {
+                TRACE("'%s' read %zd bytes [%d]...", trnd->name, rc, i);
+                return rc;
+            }
+        } /* for each online cpu */
+    } /* while have something to output */
+    if (rc == 0 && !trnd->is_abort && !atomic_read(&trnd->is_complete)) { /* !!! something wrong !!! */
+        ERROR("'%s' [%d] rb=%lu :: %u (%lu bytes) evtstore=%d of %d", trnd->name, i,
+                ring_buffer_entries(trnd->buffer), atomic_read(&vtss_transport_npages), atomic_read(&vtss_transport_npages)*PAGE_SIZE,
+                atomic_read(&trnd->seqdone)-1, atomic_read(&trnd->seqnum));
+        if  (!ring_buffer_empty(trnd->buffer)) {
+            for_each_online_cpu(cpu) {
+                unsigned long count = ring_buffer_entries_cpu(trnd->buffer, cpu);
+                if (count)
+                    ERROR("'%s' evtcount[%03d]=%lu", trnd->name, cpu, count);
+            }
+        }
+        /* We cannot return 0 if transport is not complete, so write the magic */
+        *((unsigned int*)buf) = UEC_MAGIC;
+        buf += sizeof(unsigned int);
+        *((unsigned int*)buf) = UEC_MAGICVALUE;
+        buf += sizeof(unsigned int);
+        size -= 2*sizeof(unsigned int);
+        rc += 2*sizeof(unsigned int);
+    }
+    atomic_set(&trnd->is_overflow, 0);
+    return rc;
+}
+
+void vtss_consume_ring_buffer(struct vtss_transport_data* trnd, unsigned long maxdone)
+{
+    int cpu;
+    int num_cpu_overflowed = 0;
+    int num_entries_to_clean = 0;
+    //unsigned long maxdone = 0;
+    unsigned long seqdone = atomic_read(&trnd->seqdone);
+
+    if (!trnd){
+        ERROR("Wrong arguments");
+        return;
+    }
+    
+    if (!(trnd->type & VTSS_TR_RB)) {
+        ERROR("Wrong arguments, wrong trnd type");
+        return;
+    }
+
+    if (!trnd->buffer){
+        ERROR("Wrong arguments, invalid trnd");
+        return;
+    }
+
+    if (maxdone == 0) {
+        for_each_online_cpu(cpu) {
+            unsigned long locked_size_cpu = atomic_read(&trnd->locked_size_cpu[cpu]);
+            if (locked_size_cpu > (trnd->ring_buffer_size - trnd->ring_buffer_size/4)) {
+                unsigned long num_entries = ring_buffer_entries_cpu(trnd->buffer, cpu);
+                num_entries_to_clean += num_entries/4;
+                num_cpu_overflowed++;
+            }
+        }
+        maxdone = seqdone + num_entries_to_clean;
+    }
+
+    if (maxdone && VTSS_TRANSPORT_DATA_READY(trnd))
+    {
+        int old_state = atomic_cmpxchg(&trnd->processing_state, 0, 1);
+        if (old_state == 0) {
+            size_t len = 0;
+            int rc = 0;
+            void* data = NULL;
+            int cpu;
+#ifndef VTSS_NO_MERGE
+            /* Flush buffers if possible first of all */
+            len = vtss_transport_temp_flush(trnd, NULL, 0, maxdone);
+#endif
+            TRACE("start: seqdone=%d, maxdone=%d, num_cpu_overflowed=%d, num_entries_to_clean=%d", (int)seqdone, (int)maxdone, num_cpu_overflowed, num_entries_to_clean);
+            while (((seqdone <= maxdone) && (atomic_read(&trnd->processing_state) == 1))) {
+                for_each_online_cpu(cpu) {
+                    u64 ts;
+                    int count;
+                    struct ring_buffer_event *event;
+
+                    if (ring_buffer_entries_cpu(trnd->buffer, cpu) == 0)
+                        continue; // nothing to read on this cpu
+                    for (count = 0; count < 10 && NULL !=
+#ifdef VTSS_AUTOCONF_RING_BUFFER_LOST_EVENTS
+                            (event = ring_buffer_peek(trnd->buffer, cpu, &ts, NULL))
+#else
+                            (event = ring_buffer_peek(trnd->buffer, cpu, &ts))
+#endif
+                            ; count++)
+                    {
+                        struct vtss_transport_entry* data = (struct vtss_transport_entry*)ring_buffer_event_data(event);
+
+                        /// skip unordered event
+                        //if(data->seqnum != seqdone) break;
+                        
+                        /// skip unwanted event
+                        if(data->seqnum > maxdone) break;
+
+                        trnd->seqcpu[cpu] = data->seqnum;
+
+                        if ((1 + trnd->seqcpu[cpu] - atomic_read(&trnd->seqdone)) > VTSS_MERGE_MEM_LIMIT/4 &&
+                                !trnd->is_abort && !atomic_read(&trnd->is_complete))
+                        {
+                            break; /* will not read this event */
+                        }
+#ifdef VTSS_AUTOCONF_RING_BUFFER_LOST_EVENTS
+                        event = ring_buffer_consume(trnd->buffer, cpu, &ts, NULL);
+#else
+                        event = ring_buffer_consume(trnd->buffer, cpu, &ts);
+#endif
+                        if (event != NULL) {
+                            if ((len = vtss_transport_temp_parse_event(trnd, event, NULL, 0, cpu)) > 0)
+                            {
+                                TRACE("consume[cpu=%d]: num_entries=%ld, buf_size=%d, (maxdone=%d, trnd->seqdone=%d)",
+                                        cpu, ring_buffer_entries_cpu(trnd->buffer, cpu), 
+                                        (int)atomic_read(&trnd->locked_size_cpu[cpu]),
+                                        (int)maxdone, (int)atomic_read(&trnd->seqdone));
+                            }
+                        }
+                        if (atomic_read(&trnd->seqdone) > maxdone) break;
+                    } /* for */
+                    if (atomic_read(&trnd->seqdone) > maxdone) break;
+                } // for each cpu
+                len = vtss_transport_temp_flush(trnd, NULL, 0, maxdone);
+                if (seqdone < atomic_read(&trnd->seqdone)){
+                    seqdone = atomic_read(&trnd->seqdone);
+                } else {
+                    ERROR("Nothing to consume");
+                    break;
+                }
+            }
+            atomic_cmpxchg(&trnd->processing_state, 1, 0);
+        }// old state = 0
+    }
+}
+
+static void vtss_consume_ring_buffer_cpu(struct vtss_transport_data* trnd, int cpu)
+{
+    struct ring_buffer_event* event = NULL;
+    struct vtss_transport_entry* data = NULL;
+    u64 ts;
+
+    if (ring_buffer_entries_cpu(trnd->buffer, cpu) != 0)
+    {
+#ifdef VTSS_AUTOCONF_RING_BUFFER_LOST_EVENTS
+        event = ring_buffer_peek(trnd->buffer, cpu, &ts, NULL);
+#else
+        event = ring_buffer_peek(trnd->buffer, cpu, &ts);
+#endif
+        if (event != NULL) {
+            struct vtss_transport_entry* data = (struct vtss_transport_entry*)ring_buffer_event_data(event);
+            unsigned long maxdone = data->seqnum + 1;
+            if(maxdone > atomic_read(&trnd->seqdone)) {
+                TRACE("maxdone=%d (seqcpu[%d]=%ld) [seqdone=%d - seqnum=%d] nev=%d)", 
+                    (int)maxdone, cpu, trnd->seqcpu[cpu],
+                    atomic_read(&trnd->seqdone), atomic_read(&trnd->seqnum),
+                    1 + (atomic_read(&trnd->seqnum) - atomic_read(&trnd->seqdone)));
+                vtss_consume_ring_buffer(trnd, maxdone);
+            }
+        }
+    }
+}
+
+struct ring_buffer_event* vtss_ring_buffer_lock_reserve(struct vtss_transport_data* trnd, size_t size)
+{
+    struct ring_buffer_event* event = NULL;
+    int cnt = (trnd->type & VTSS_TR_RB) ? 100 : 10;
+    while(!event && cnt > 0){
+#ifdef VTSS_AUTOCONF_RING_BUFFER_FLAGS
+        event = ring_buffer_lock_reserve(trnd->buffer, size + sizeof(struct vtss_transport_entry), 0);
+#else
+        event = ring_buffer_lock_reserve(trnd->buffer, size + sizeof(struct vtss_transport_entry));
+#endif
+        if (trnd->type & VTSS_TR_RB)
+        {
+            if (event == NULL && atomic_read(&vtss_ring_buffer_paused) == 0)
+            {
+                int cnt2 = 100000;
+                while(atomic_read(&trnd->processing_state) == 1 && cnt2 > 0)
+                {
+                    if(cnt2%1000 == 0) touch_nmi_watchdog();
+                    cnt2--;
+                }
+                if(cnt2) {
+                    int cpu = raw_smp_processor_id();
+                    vtss_consume_ring_buffer_cpu(trnd, cpu);
+                }
+            }
+        }
+        cnt--;
+    }
+    if (event){
+        int cpu = raw_smp_processor_id();
+        atomic_add(size + sizeof(struct vtss_transport_entry), &trnd->locked_size_cpu[cpu]);
+    }
+    return event;
+}
+static struct vtss_transport_entry* vtss_ring_buffer_event_data(struct vtss_transport_data* trnd, struct ring_buffer_event* event, unsigned long* chunk_id)
+{
+    struct vtss_transport_entry* data;
+    data = (struct vtss_transport_entry*)ring_buffer_event_data(event);
+    data->seqnum = atomic_inc_return(&trnd->seqnum);
+    data->rb_start = 0;
+    if (chunk_id && (trnd->type & VTSS_TR_RB)){
+        *chunk_id = data->seqnum - ((data->seqnum-1)&VTSS_RB_MASK);
+        data->rb_start = *chunk_id;
+    }
+    if ((data->seqnum&VTSS_RB_MASK) == 1){
+        atomic_set(&trnd->rb_mark, data->rb_start);
+    }
+    data->cputsc = vtss_time_cpu();
+    return data;
+}
+
+void* vtss_transport_record_reserve_internal(struct vtss_transport_data* trnd, void** entry, size_t size, unsigned long* chunk_id)
+{
+    struct ring_buffer_event* event;
+    struct vtss_transport_entry* data;
+
+    if (unlikely(trnd == NULL || entry == NULL)) {
+        ERROR("Transport or Entry is NULL");
+        return NULL;
+    }
+
+    if (unlikely(!atomic_read(&vtss_is_transport_init))) {
+        ERROR("'%s' is initialized", trnd->name);
+        return NULL;
+    }
+    //return NULL;
+    atomic_inc(&trnd->reserved);
+    if (unlikely(atomic_read(&trnd->is_complete))) {
+        ERROR("'%s' is COMPLETED", trnd->name);
+        atomic_dec(&trnd->reserved);
+        return NULL;
+    }
+
+    if (unlikely(size == 0 || size > 0xffff /* max short */)) {
+        ERROR("'%s' incorrect size (%zu bytes)", trnd->name, size);
+        atomic_dec(&trnd->reserved);
+        return NULL;
+    }
+
+    if(atomic_read(&vtss_ring_buffer_stopped)) {
+        /* collection was stopped */
+        atomic_dec(&trnd->reserved);
+        return NULL;
+    }
+
+    if (likely(size < VTSS_TRANSPORT_MAX_RESERVE_SIZE)) {
+        event = vtss_ring_buffer_lock_reserve(trnd, size);
+        if (unlikely(event == NULL)) {
+            atomic_inc(&trnd->loscount);
+            atomic_inc(&trnd->is_overflow);
+            DEBUG_TR("'%s' ring_buffer_lock_reserve failed 1, size = %d", trnd->name, (int)(size + sizeof(struct vtss_transport_entry)));
+            atomic_dec(&trnd->reserved);
+            return NULL;
+        }
+        *entry = (void*)event;
+        data = vtss_ring_buffer_event_data(trnd, event, chunk_id);
+        data->size = size;
+        return (void*)data->data;
+    } else { /* blob */
+        unsigned int order = get_order(size + sizeof(struct vtss_transport_temp));
+        struct vtss_transport_temp* blob;
+        /*if (atomic_read(&vtss_transport_npages) > VTSS_MERGE_MEM_LIMIT/3) {
+            DEBUG_TR("'%s' memory limit for blob %zu bytes", trnd->name, size);
+            atomic_inc(&trnd->is_overflow);
+            atomic_inc(&trnd->loscount);
+            return NULL;
+        } else if (atomic_read(&vtss_transport_npages) > VTSS_MERGE_MEM_LIMIT/4) {
+            DEBUG_TR("'%s' memory limit for blob is  going to happen soon; %zu bytes", trnd->name, size);
+            atomic_inc(&trnd->is_overflow);
+        }*/
+        if (order > 10){
+            DEBUG_TR("'%s' cannot allocate %zu bytes", trnd->name, size);
+            atomic_inc(&trnd->loscount);
+            atomic_dec(&trnd->reserved);
+            return NULL;
+        }
+        DEBUG_TR("'%s' allocated size  = %zu", trnd->name, size);
+        blob = (struct vtss_transport_temp*)vtss_get_free_pages(GFP_NOWAIT, order);
+        if (blob == NULL)
+        {
+            vtss_consume_ring_buffer(trnd,0); //this function helps to free memory
+            blob = (struct vtss_transport_temp*)vtss_get_free_pages(GFP_NOWAIT, order);
+        }
+        if (unlikely(blob == NULL)) {
+            DEBUG_TR("'%s' no memory for blob %zu bytes", trnd->name, size);
+            atomic_inc(&trnd->loscount);
+            atomic_dec(&trnd->reserved);
+            return NULL;
+        }
+        atomic_add(1<<order, &vtss_transport_npages);
+        blob->size  = size;
+        blob->order = order;
+        event = vtss_ring_buffer_lock_reserve(trnd, sizeof(void*) + sizeof(struct vtss_transport_entry));
+        if (unlikely(event == NULL)) {
+            vtss_free_pages((unsigned long)blob, order);
+            atomic_sub(1<<order, &vtss_transport_npages);
+            atomic_inc(&trnd->loscount);
+            atomic_inc(&trnd->is_overflow);
+            DEBUG_TR("'%s' ring_buffer_lock_reserve failed overflow", trnd->name);
+            atomic_dec(&trnd->reserved);
+            return NULL;
+        }
+        blob->prev = NULL;
+        blob->next = NULL;
+        *entry = (void*)event;
+        data = vtss_ring_buffer_event_data(trnd, event, chunk_id);
+        data->size   = 0;
+        *((void**)&(data->data)) = (void*)blob;
+        return (void*)blob->data;
+    }
+}
+void* vtss_transport_record_reserve(struct vtss_transport_data* trnd, void** entry, size_t size)
+{
+    return vtss_transport_record_reserve_internal(trnd, entry, size, NULL);
+}
+void* vtss_transport_activity_record_reserve(struct vtss_transport_data* trnd, void** entry, size_t size, unsigned long* chunk_id)
+{
+    return vtss_transport_record_reserve_internal(trnd, entry, size, chunk_id);
+}
+
+
+int vtss_transport_record_commit(struct vtss_transport_data* trnd, void* entry, int is_safe)
+{
+    int rc = 0;
+    struct ring_buffer_event* event = (struct ring_buffer_event*)entry;
+
+    if (unlikely(trnd == NULL || entry == NULL)) {
+        ERROR("Transport or Entry is NULL");
+        return -EINVAL;
+    }
+#ifdef VTSS_AUTOCONF_RING_BUFFER_FLAGS
+    rc = ring_buffer_unlock_commit(trnd->buffer, event, 0);
+#else
+    rc = ring_buffer_unlock_commit(trnd->buffer, event);
+#endif
+    if (rc) {
+        struct vtss_transport_entry* data = (struct vtss_transport_entry*)ring_buffer_event_data(event);
+        ERROR("'%s' commit error: seq=%lu, size=%u", trnd->name, data->seqnum, data->size);
+    } else {
+        atomic_inc(&trnd->commited);
+        if (unlikely(is_safe && trnd->type != VTSS_TR_RB && VTSS_TRANSPORT_DATA_READY(trnd))) {
+            if (waitqueue_active(&trnd->waitq))
+            {
+                DEBUG_TR("commit OK, waitqueue active");
+                wake_up_interruptible(&trnd->waitq);
+            } 
+        }
+    }
+    atomic_dec(&trnd->reserved);
+    return rc;
+}
+
+int vtss_transport_record_write(struct vtss_transport_data* trnd, void* part0, size_t size0, void* part1, size_t size1, int is_safe)
+{
+    int rc = -EFAULT;
+    void* entry;
+    void* p = vtss_transport_record_reserve(trnd, &entry, size0 + size1);
+    if (p) {
+        memcpy(p, part0, size0);
+        if (size1)
+            memcpy(p + size0, part1, size1);
+        rc = vtss_transport_record_commit(trnd, entry, is_safe);
+    }
+    return rc;
+}
+
+#endif /* VTSS_USE_UEC */
+
+int vtss_transport_record_write_all(void* part0, size_t size0, void* part1, size_t size1, int is_safe)
+{
+    int rc = 0;
+    unsigned long flags;
+    struct list_head *p;
+    struct vtss_transport_data *trnd = NULL;
+
+    vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+    list_for_each(p, &vtss_transport_list) {
+        trnd = list_entry(p, struct vtss_transport_data, list);
+        TRACE("put_record(%d) to trnd=0x%p => '%s'", atomic_read(&trnd->is_complete), trnd, trnd->name);
+        if (likely((trnd->name[0] != '\0') && (!atomic_read(&trnd->is_complete)) && trnd->type == VTSS_TR_REG)) {
+#ifdef VTSS_USE_UEC
+            /* Don't use spill notifications from uec therefore its UECMODE_SAFE always */
+            int rc1;    
+            if (VTSS_PT_FLUSH_MODE){
+                rc1 = trnd->uec_chain->put_record(trnd->uec_chain, part0, size0, part1, size1, UECMODE_SAFE);
+            } else {
+                rc1 = trnd->uec->put_record(trnd->uec, part0, size0, part1, size1, UECMODE_SAFE);
+            }
+            if (rc1) {
+                atomic_inc(&trnd->loscount);
+                rc = -EFAULT;
+            }
+#ifndef VTSS_USE_NMI
+            if (unlikely(is_safe && VTSS_TRANSPORT_DATA_READY(trnd))) {
+                TRACE("WAKE UP");
+                if (waitqueue_active(&trnd->waitq))
+                    wake_up_interruptible(&trnd->waitq);
+            }
+#endif
+#else  /* VTSS_USE_UEC */
+            void* entry;
+            void* p = vtss_transport_record_reserve(trnd, &entry, size0 + size1);
+            if (likely(p)) {
+                memcpy(p, part0, size0);
+                if (size1)
+                    memcpy(p + size0, part1, size1);
+                rc = vtss_transport_record_commit(trnd, entry, is_safe) ? -EFAULT : rc;
+            } else {
+                rc = -EFAULT;
+            }
+#endif /* VTSS_USE_UEC */
+        }
+    }
+    vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+    return rc;
+}
+
+static unsigned int vtss_magic_marker[2] = {UEC_MAGIC, UEC_MAGICVALUE};
+
+static ssize_t vtss_transport_read(struct file *file, char __user* buf, size_t size, loff_t* ppos)
+{
+    ssize_t rc = 0;
+    int rc1 = 0;
+    int magic = 0;
+#ifndef VTSS_USE_UEC
+    int cnt = 0;
+#endif
+    struct vtss_transport_data* trnd = (struct vtss_transport_data*)file->private_data;
+
+    if (unlikely(trnd == NULL || trnd->file == NULL || buf == NULL || size < sizeof(vtss_magic_marker))){
+        ERROR("Failed read file!, trnd = %p, buf = %p, size = %d", trnd, buf, (int)size);
+        return -EINVAL;
+    }
+    DEBUG_TR("read, file = %p, name = %s", file, trnd->name ? trnd->name : "null");
+    while (!atomic_read(&trnd->is_complete) && !VTSS_TRANSPORT_DATA_READY(trnd)) {
+#ifndef VTSS_USE_UEC
+        cnt++;
+        if (cnt == 1000){
+             int data_size = 1 + (atomic_read(&trnd->seqnum) - atomic_read(&trnd->seqdone));
+             DEBUG_TR("Trying to read not ready data, data size = %d", data_size);
+        }
+#endif
+        if (file->f_flags & O_NONBLOCK){
+            return -EAGAIN;
+        }
+#if defined(VTSS_CONFIG_REALTIME)
+        {
+            unsigned long delay;
+            delay = msecs_to_jiffies(1000);
+            rc = wait_event_interruptible_timeout(trnd->waitq,
+                 (atomic_read(&trnd->is_complete) || VTSS_TRANSPORT_DATA_READY(trnd)), delay);
+        }
+#else
+        rc = wait_event_interruptible(trnd->waitq,
+             (atomic_read(&trnd->is_complete) || VTSS_TRANSPORT_DATA_READY(trnd)));
+#endif
+        if (rc < 0){
+            DEBUG_TR("Waitq is empty");
+            return -ERESTARTSYS;
+        }
+    }
+
+#ifndef VTSS_USE_UEC
+    if (trnd->type & VTSS_TR_RB) {
+        {
+            int old_state = atomic_cmpxchg(&trnd->processing_state, 0, 2);
+            int state_error = 0;
+            if (old_state == 4) atomic_cmpxchg(&trnd->processing_state, 4, 2);
+            while (old_state == 1)
+            {
+                old_state = atomic_cmpxchg(&trnd->processing_state, 0, 2);
+                state_error++;
+                if (state_error == 1000)
+                {
+                  DEBUG_TR("awaiting right state takes too long!!!");
+                  return 0;
+                }
+            }
+        }
+    }
+#endif
+    rc = 0;
+    if (trnd->magic == 0){
+        VTSS_TRANSPORT_COPY_TO_USER((void*)vtss_magic_marker, sizeof(vtss_magic_marker));
+        trnd->magic++;
+        magic++;
+    }
+
+
+#ifdef VTSS_USE_UEC
+    if (trnd->type & VTSS_TR_RB) {
+        rc1 = trnd->uec_chain->pull(trnd->uec_chain, buf, size);
+    } else {
+        rc1 = trnd->uec->pull(trnd->uec, buf, size);
+    }
+#else
+    rc1  = vtss_transport_read_rb(trnd, buf, size);
+#endif /* VTSS_USE_UEC */
+    if (rc1 < 0) {
+        if (magic > 0) trnd->magic--;
+        return rc1;
+    }
+    rc += rc1;
+    if(trnd->type & VTSS_TR_RB)
+        TRACE("'%s' read %zd bytes", trnd->name, rc);
+    return rc;
+}
+
+static ssize_t vtss_transport_write(struct file *file, const char __user * buf, size_t count, loff_t * ppos)
+{
+    /* the transport is read only */
+    return -EINVAL;
+}
+
+static unsigned int vtss_transport_poll(struct file *file, poll_table* poll_table)
+{
+    unsigned int rc = 0;
+    struct vtss_transport_data* trnd = NULL;
+    
+    if (file == NULL){
+        ERROR("Invalid poll. File is empty");
+        return (POLLERR | POLLNVAL);
+    }
+    trnd = (struct vtss_transport_data*)file->private_data;
+    if (trnd == NULL){
+        ERROR("Invalid poll. File data is empty");
+        return (POLLERR | POLLNVAL);
+    }
+    if (trnd->file == NULL){
+        ERROR("Invalid poll. File was already closed.");
+        return 0;
+    }
+    poll_wait(file, &trnd->waitq, poll_table);
+    if (atomic_read(&trnd->is_complete) || (VTSS_TRANSPORT_DATA_READY(trnd) && trnd->type != VTSS_TR_RB))
+        rc = (POLLIN | POLLRDNORM);
+    else
+        atomic_set(&trnd->is_overflow, 0);
+    DEBUG_TR("%s: file=0x%p, trnd=0x%p", (rc ? "READY" : "-----"), file, trnd);
+    return rc;
+}
+
+static int vtss_transport_open(struct inode *inode, struct file *file)
+{
+    int rc;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+    struct vtss_transport_data *trnd = (struct vtss_transport_data *)PDE(inode)->data;
+#else
+    struct vtss_transport_data *trnd = (struct vtss_transport_data *)PDE_DATA(inode);
+#endif
+    DEBUG_TR("inode=0x%p, file=0x%p, trnd=0x%p", inode, file, trnd);
+    if (trnd == NULL)
+        return -ENOENT;
+
+    rc = generic_file_open(inode, file);
+    if (rc)
+        return rc;
+
+    if (atomic_read(&trnd->is_complete) && VTSS_TRANSPORT_IS_EMPTY(trnd)) {
+        return -EINVAL;
+    }
+    if (atomic_inc_return(&trnd->is_attached) > 1) {
+        atomic_dec(&trnd->is_attached);
+        return -EBUSY;
+    }
+    trnd->file = file;
+    file->private_data = trnd;
+    /* Increase the priority for trace reader to avoid lost events */
+    set_user_nice(current, -19);
+    return rc;
+}
+
+static int vtss_transport_close(struct inode *inode, struct file *file)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+    struct vtss_transport_data *trnd = (struct vtss_transport_data*)PDE(inode)->data;
+#else
+    struct vtss_transport_data *trnd = (struct vtss_transport_data*)PDE_DATA(inode);
+#endif
+    if (!file)
+    {
+        ERROR("File is empty");
+        return 0;
+    }
+    file->private_data = NULL;
+    /* Restore default priority for trace reader */
+    set_user_nice(current, 0);
+    DEBUG_TR("inode=0x%p, file=0x%p, trnd=0x%p", inode, file, trnd);
+    if (trnd == NULL)
+    {
+        ERROR("Internal error");
+        return -ENOENT;
+    }
+    DEBUG_TR("Closing the file %s", trnd->name);
+    trnd->file = NULL;
+    if (!atomic_dec_and_test(&trnd->is_attached)) {
+        ERROR("'%s' wrong state", trnd->name);
+        atomic_set(&trnd->is_attached, 0);
+        return -EFAULT;
+    }
+    return 0;
+}
+
+static struct file_operations vtss_transport_fops = {
+    .owner   = THIS_MODULE,
+    .read    = vtss_transport_read,
+    .write   = vtss_transport_write,
+    .open    = vtss_transport_open,
+    .release = vtss_transport_close,
+    .poll    = vtss_transport_poll,
+};
+
+static void vtss_transport_remove(struct vtss_transport_data* trnd)
+{
+    struct proc_dir_entry *procfs_root = NULL;
+    if (trnd->name[0]=='\0') return;
+    procfs_root = vtss_procfs_get_root();
+
+    if (procfs_root != NULL) {
+        remove_proc_entry(trnd->name, procfs_root);
+    }
+    trnd->name[0]='\0';
+}
+
+static void vtss_transport_destroy_trnd(struct vtss_transport_data* trnd)
+{
+#ifdef VTSS_USE_UEC
+    if (trnd->uec){
+        destroy_uec(trnd->uec);
+        vtss_kfree(trnd->uec);
+    }
+    trnd->uec_chain->destroy(trnd->uec_chain);
+#else
+    int seqnum = atomic_read(&trnd->seqnum);
+    int seqdone = atomic_read(&trnd->seqdone);
+    
+    if ((seqdone - 1) != seqnum) {
+        ERROR("'%s' drop %d events seqnum = %d, segdone = %d", trnd->name, seqnum - seqdone - 1, seqnum, seqdone);
+    }
+    if (trnd->buffer) {
+        ring_buffer_free(trnd->buffer);
+        trnd->buffer = NULL;
+    }
+    if (trnd->head){
+        vtss_transport_temp_free_all(trnd, &(trnd->head));
+        trnd->head = NULL;
+    }
+#endif
+    DEBUG_TR("trnd = %p  destroyed", trnd);
+    vtss_kfree(trnd);
+}
+
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_transport_destroy_trnd_work(struct work_struct *work)
+#else
+static void vtss_transport_destroy_trnd_work(void *work)
+#endif
+{
+    struct vtss_work* my_work = (struct vtss_work*)work;
+    struct vtss_transport_data* trnd = NULL;
+    if (!my_work){
+        ERROR("empty work!");
+        return;
+    }
+    trnd = *((struct vtss_transport_data**)(&my_work->data));
+    if (trnd)
+    {
+        vtss_transport_destroy_trnd(trnd);
+    }
+    else
+    {
+        ERROR("Trying to destroy empty transport data");
+    }
+    vtss_kfree(my_work);
+    atomic_dec(&vtss_kernel_task_in_progress);
+}
+
+#ifndef VTSS_USE_UEC
+static unsigned long vtss_ring_buffer_alloc(struct vtss_transport_data* trnd, unsigned long size)
+{
+    if (size < 2*PAGE_SIZE) size = 2*PAGE_SIZE;
+    if (size > VTSS_ALLOC_BUFSIZE_MAX) size = VTSS_ALLOC_BUFSIZE_MAX;
+    trnd->buffer = ring_buffer_alloc(size, 0);
+    return trnd->buffer ? size : 0;
+}
+#endif
+
+static void vtss_transport_init_trnd(struct vtss_transport_data* trnd)
+{
+#ifndef VTSS_USE_UEC
+    int cpu;
+#endif
+    init_waitqueue_head(&trnd->waitq);
+    atomic_set(&trnd->refcount,    1);
+    atomic_set(&trnd->loscount,    0);
+    atomic_set(&trnd->is_attached, 0);
+    atomic_set(&trnd->is_complete, 0);
+    atomic_set(&trnd->is_overflow, 0);
+    trnd->file = NULL;
+    atomic_set(&trnd->seqdone,1);
+    trnd->magic    = 0;
+#ifndef VTSS_USE_UEC
+    atomic_set(&trnd->processing_state, 0);
+    atomic_set(&trnd->reserved, 0);
+    trnd->is_abort = 0;
+    if (trnd->head){
+         vtss_transport_temp_free_all(trnd, &(trnd->head));
+         trnd->head  = NULL;
+    }
+    atomic_set(&trnd->seqnum, 0);
+    atomic_set(&trnd->commited, 0);
+    trnd->bufcputsc = 0;
+    for_each_online_cpu(cpu) atomic_set(&trnd->locked_size_cpu[cpu], 0);
+// the line below leads hang on 52.166 machine
+//    if (trnd->buffer) ring_buffer_reset(trnd->buffer); 
+#endif
+}
+
+#define VTSS_AH_BUFSIZE_SEC 0x200000L
+#define VTSS_PT_BUFSIZE_SEC 0x1800000L
+
+struct vtss_transport_data* vtss_transport_create_trnd(int type, struct vtss_transport_data* buf_src_trnd)
+{
+    unsigned long rb_size = 0;
+    struct vtss_transport_data* trnd = NULL;
+#ifdef VTSS_USE_UEC
+    int rc = -1;
+#endif
+    // This function cannot be called in irqs disabled mode, as it allocates huge ammount of memory.
+    // Return 0 for the case
+    if (irqs_disabled()){
+        DEBUG_TR("The attempt to create transport in irqs disabled mode failed.");
+        return NULL;
+    }
+    trnd = (struct vtss_transport_data*)vtss_kmalloc(sizeof(struct vtss_transport_data), GFP_KERNEL);
+    DEBUG_TR("trnd = %p  allocated", trnd);
+    if (trnd == NULL) {
+        ERROR("Not enough memory for transport data");
+        return NULL;
+    }
+    memset(trnd, 0, sizeof(struct vtss_transport_data));
+    vtss_transport_init_trnd(trnd);
+    trnd->type = type;
+#ifdef VTSS_USE_UEC
+    if (reqcfg.ipt_cfg.size)
+    {
+        int msec = reqcfg.ipt_cfg.size;
+
+        if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT)
+        {
+            rb_size = (hardcfg.cpu_no/2 * msec * VTSS_PT_BUFSIZE_SEC) / 1000;
+        }
+        else
+        {
+            int si = vtss_cpuevents_get_sampling_interval();
+            unsigned long cpuevents_size = 0;
+ 
+            if (reqcfg.cpuevent_count_v1 > 10) cpuevents_size = reqcfg.cpuevent_count_v1*8*1000;
+            rb_size = (hardcfg.cpu_no/2 * msec * (VTSS_AH_BUFSIZE_SEC + cpuevents_size))/(1000*si);
+        }
+        DEBUG_TR("RB: %s mode: %d msec, %ld bytes (%ldMb)", 
+            reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT ? "PT" : "EBS", msec, rb_size, rb_size/0x100000L);
+    }
+    rc = init_uec_chain(trnd->uec_chain, rb_size/VTSS_UEC_CHAIN_SIZE, NULL, VTSS_UEC_CHAIN_SIZE);
+    if (rc) {
+            ERROR("Unable to init UEC chain");
+            vtss_transport_destroy_trnd(trnd);
+            return NULL;
+    }
+    if (buf_src_trnd) {
+        DEBUG_TR("take new buffer");
+        trnd->uec = buf_src_trnd->uec;
+        buf_src_trnd->uec = NULL;
+        rc = 0;
+    }
+    else
+    {
+        trnd->uec = (uec_t*)vtss_kmalloc(sizeof(uec_t), GFP_KERNEL);
+        rc = -1;
+    }
+    if (trnd->uec != NULL) {
+        size_t size = VTSS_UEC_BUFSIZE;
+        trnd->uec->callback = vtss_transport_callback;
+        trnd->uec->context  = (void*)trnd;
+        if (rc == -1) rc = init_uec(trnd->uec, size, NULL, 0);
+        if (rc) {
+            ERROR("Unable to init UEC");
+            vtss_transport_destroy_trnd(trnd);
+            return NULL;
+        }
+        TRACE("Use %zu bytes for UEC", size);
+    } else {
+        ERROR("Could not create UEC");
+        vtss_transport_destroy_trnd(trnd);                    
+        return NULL;
+    }
+#else
+    trnd->buffer = NULL;
+
+
+    if (buf_src_trnd){
+         if (buf_src_trnd->type < type){
+             //we cannot use this buffer! the size is different
+             buf_src_trnd = NULL;
+         }
+    }
+    if (buf_src_trnd && buf_src_trnd->buffer){
+        trnd->buffer = buf_src_trnd->buffer;
+        buf_src_trnd->buffer = NULL;
+    //    ring_buffer_reset(trnd->buffer);
+    }
+    else {
+        int msec = reqcfg.ipt_cfg.size ? reqcfg.ipt_cfg.size : 1000;
+
+        if (type == VTSS_TR_RB) {
+            if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) {
+                //PT writes only headers to RB
+                rb_size = (msec * VTSS_AH_BUFSIZE_SEC)/1000;
+            }
+            else {
+                int si = vtss_cpuevents_get_sampling_interval();
+                unsigned long cpuevents_size = 0;
+ 
+                if (reqcfg.cpuevent_count_v1 > 10) cpuevents_size = reqcfg.cpuevent_count_v1*8*1000;
+                rb_size = (msec * (VTSS_AH_BUFSIZE_SEC + cpuevents_size))/(1000*si);
+                DEBUG_TR("cpuevent_count=%d, sampling_interval=%d", reqcfg.cpuevent_count_v1, si);
+            }
+            DEBUG_TR("RB: %s mode: %d msec, %ld bytes (%ldMb)", 
+                reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT ? "PT" : "EBS", msec, rb_size, rb_size/0x100000L);
+        }
+        else {
+            if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_IPT) {
+                rb_size = VTSS_AH_BUFSIZE_SEC;
+            }
+            else {
+                rb_size = (num_present_cpus() > 32) ? 32*PAGE_SIZE : 64*PAGE_SIZE;
+            }
+        }
+        rb_size = PAGE_ALIGN(rb_size);
+        trnd->ring_buffer_size = vtss_ring_buffer_alloc(trnd, rb_size);
+        DEBUG_TR("allocated size=%x, type=%d", trnd->ring_buffer_size, type);
+    }
+    if (trnd->buffer == NULL) {
+        trnd->ring_buffer_size = 0;
+        ERROR("Unable to allocate %lu bytes for transport buffer (nth=%d)", rb_size, num_present_cpus()/2);
+        vtss_transport_destroy_trnd(trnd);
+        return NULL;
+    }
+#endif
+   return trnd;
+}
+
+#if 0
+#ifdef VTSS_AUTOCONF_INIT_WORK_TWO_ARGS
+static void vtss_transport_prealloc_item_work(struct work_struct *work)
+#else
+static void vtss_transport_prealloc_item_work(void *work)
+#endif
+{
+    struct vtss_work* my_work = (struct vtss_work*)work;
+    int* type_ptr = NULL;
+    struct vtss_transport_data *trnd = NULL;
+    unsigned long flags;
+
+    if (!my_work){
+        ERROR("empty work!");
+        return;
+    }
+    type_ptr = (int*)my_work->data;
+    if (!type_ptr){
+        ERROR("unknown type");
+        vtss_kfree(my_work);
+        return;
+    }
+    trnd=vtss_transport_create_trnd(*type_ptr, NULL);
+    if (trnd) {
+        vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+        list_add_tail(&trnd->list, &vtss_transport_list);
+        atomic_inc(&vtss_free_tr_cnt);
+        vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+    }
+    vtss_kfree(my_work);
+}
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(3,6,32)
+static int vtss_transport_prealloc_transport_items(void)
+{
+    int i;
+    int aux_type = (atomic_read(&vtss_transport_mode) == VTSS_TR_MODE_RB) ? VTSS_TR_RB : VTSS_TR_CFG;
+
+    int max = (VTSS_PREALLOC_TR_SIZE + VTSS_PREALLOC_TR_SIZE);
+    unsigned long flags;
+    if (atomic_read(&vtss_free_tr_cnt) >= max) return 0;
+    //preallocate transport items
+    for(i = 0; i < VTSS_PREALLOC_TR_SIZE; i++){
+        struct vtss_transport_data *trnd = NULL;
+        trnd=vtss_transport_create_trnd(aux_type, NULL);
+        if (trnd) {
+            vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+            list_add_tail(&trnd->list, &vtss_transport_list);
+            atomic_inc(&vtss_free_tr_cnt);
+            vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+        } else {
+            if (i == 0) return -1;
+            else break;
+        }
+
+        trnd=vtss_transport_create_trnd(VTSS_TR_REG, NULL);
+        if (trnd){
+            vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+            list_add_tail(&trnd->list, &vtss_transport_list);
+            atomic_inc(&vtss_free_tr_cnt);
+            vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+        } else {
+            if (i == 0) return -1;
+            else break;
+        }
+    }
+    return 0;
+}
+#endif
+
+struct vtss_transport_data* vtss_transport_get_trnd(int type)
+{
+    struct vtss_transport_data* trnd = NULL;
+    struct vtss_transport_data* trnd_ret = NULL;
+    struct list_head* p = NULL;
+    struct list_head* tmp = NULL;
+    unsigned long flags;
+    int cnt = 100;
+    
+    while ((!trnd_ret) && (cnt > 0))
+    {
+        vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+        if(atomic_read(&vtss_is_transport_init) == 0){
+            vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+            return NULL;
+        }
+        list_for_each_safe(p, tmp, &vtss_transport_list) {
+            trnd = list_entry(p, struct vtss_transport_data, list);
+            if (trnd == NULL){
+                ERROR("trnd in list is NULL");
+                continue;
+            }
+            if (trnd->type != type){
+                continue;
+            }
+            if (atomic_read(&trnd->is_attached)) {
+                continue;
+            }
+            if  (VTSS_TRANSPORT_IS_EMPTY(trnd)){
+                if (trnd->name[0]=='\0'){
+                    trnd_ret = trnd;
+                    atomic_dec(&vtss_free_tr_cnt);
+                    list_del(p);
+                } else {
+#ifdef VTSS_USE_UEC
+                    if (!trnd->uec)
+#else
+                    if (!trnd->buffer)
+#endif
+                        continue;
+                    trnd_ret = vtss_transport_create_trnd(type, trnd /* use already allocated buffers*/ );
+                }
+                break;
+            }
+        }
+        vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+        cnt--;
+        if (!trnd_ret){
+            if (!irqs_disabled()){
+                trnd_ret = vtss_transport_create_trnd(type, NULL);
+            } else {
+              break;
+            }
+        }
+    }
+    DEBUG_TR("end, trnd_ret = %p", trnd_ret);
+    return trnd_ret;
+}
+
+static void vtss_transport_create_trnd_name(struct vtss_transport_data* trnd, pid_t ppid, pid_t pid, uid_t cuid, gid_t cgid)
+{
+    int seq = -1;
+    struct path path;
+    char buf[MODULE_NAME_LEN + sizeof(trnd->name) + 8 /* strlen("/proc/<MODULE_NAME>/%d-%d.%d") */];
+
+    do { /* Find out free name */
+        if (++seq > 0) path_put(&path);
+        snprintf(trnd->name, sizeof(trnd->name)-1, "%d-%d.%d", ppid, pid, seq);
+        snprintf(buf, sizeof(buf)-1, "%s/%s", vtss_procfs_path(), trnd->name);
+        TRACE("lookup '%s'", buf);
+    } while (!kern_path(buf, 0, &path));
+    /* Doesn't exist, so create it */
+    return;
+}
+
+int vtss_transport_create_pde (struct vtss_transport_data* trnd, uid_t cuid, gid_t cgid)
+{
+    unsigned long flags;
+    struct proc_dir_entry* pde;
+    struct proc_dir_entry* procfs_root = vtss_procfs_get_root();
+
+    if (procfs_root == NULL) {
+        ERROR("Unable to get PROCFS root");
+        return 1;
+    }
+    pde = proc_create_data(trnd->name, (mode_t)(mode ? (mode & 0444) : 0440), procfs_root, &vtss_transport_fops, trnd);
+
+    if (pde == NULL) {
+        ERROR("Could not create '%s/%s'", vtss_procfs_path(), trnd->name);
+        vtss_transport_destroy_trnd(trnd);
+        return 1;
+    }
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+#ifdef VTSS_AUTOCONF_PROCFS_OWNER
+    pde->owner = THIS_MODULE;
+#endif
+#endif
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,10,0)
+    pde->uid = cuid ? cuid : uid;
+    pde->gid = cgid ? cgid : gid;
+#else
+#if defined CONFIG_UIDGID_STRICT_TYPE_CHECKS || (LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0))
+{
+    kuid_t kuid = KUIDT_INIT(cuid ? cuid : uid);
+    kgid_t kgid = KGIDT_INIT(cgid ? cgid : gid);
+    proc_set_user(pde, kuid, kgid);
+}
+#else
+    proc_set_user(pde, cuid ? cuid : uid, cgid ? cgid : gid);
+#endif
+#endif
+    vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+    list_add_tail(&trnd->list, &vtss_transport_list);
+    vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+    TRACE("trnd=0x%p => '%s' done", trnd, trnd->name);
+    return 0;
+}
+struct vtss_transport_data* vtss_transport_create(pid_t ppid, pid_t pid, uid_t cuid, gid_t cgid)
+{
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(3,6,32)
+    struct vtss_transport_data* trnd = vtss_transport_create_trnd(VTSS_TR_REG, NULL);
+#else
+    struct vtss_transport_data* trnd = vtss_transport_get_trnd(VTSS_TR_REG);
+#endif
+
+    if (trnd == NULL) {
+        if (!irqs_disabled()) ERROR("Not enough memory for transport data");
+        return NULL;
+    }
+    vtss_transport_create_trnd_name(trnd, ppid, pid, cuid, cgid);
+    /* Doesn't exist, so create it */
+    if (vtss_transport_create_pde(trnd, cuid, cgid)){
+        ERROR("Could not create '%s/%s'", vtss_procfs_path(), trnd->name);
+        vtss_transport_destroy_trnd(trnd);
+        return NULL;
+    }
+    DEBUG_TR("returned outside trnd = %p", trnd);
+    return trnd;
+}
+struct vtss_transport_data* vtss_transport_create_aux(struct vtss_transport_data* main_trnd, uid_t cuid, gid_t cgid, int is_rb)
+{
+    char* main_trnd_name = main_trnd->name;
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(3,6,32)
+    struct vtss_transport_data* trnd = vtss_transport_create_trnd(is_rb == VTSS_TR_MODE_RB ? VTSS_TR_RB : VTSS_TR_CFG, NULL);
+#else
+    struct vtss_transport_data* trnd = vtss_transport_get_trnd(is_rb == VTSS_TR_MODE_RB ? VTSS_TR_RB : VTSS_TR_CFG);
+#endif
+
+    if (trnd == NULL) {
+        ERROR("Not enough memory for transport data");
+        return NULL;
+    }
+#ifndef VTSS_USE_UEC
+    if (trnd->type == VTSS_TR_RB) {
+        trnd->bufcputsc = vtss_time_cpu();
+    }
+#endif
+    memcpy((void*)trnd->name, (void*)main_trnd_name, strlen(main_trnd_name));
+    memcpy((void*)trnd->name+strlen(main_trnd_name),(void*)".aux", 5);
+    /* Doesn't exist, so create it */
+    if (vtss_transport_create_pde(trnd, cuid, cgid)){
+        ERROR("Could not create '%s/%s'", vtss_procfs_path(), trnd->name);
+        vtss_transport_destroy_trnd(trnd);
+        return NULL;
+    }
+    DEBUG_TR("returned outside trnd = %p", trnd);
+    return trnd;
+}
+
+int vtss_transport_complete(struct vtss_transport_data* trnd)
+{
+    if (trnd == NULL)
+        return -ENOENT;
+    DEBUG_TR("start, trnd->name = %s", trnd->name);
+    if (atomic_read(&trnd->refcount)) {
+        ERROR("'%s' refcount=%d != 0", trnd->name, atomic_read(&trnd->refcount));
+    }
+    atomic_inc(&trnd->is_complete);
+    if (waitqueue_active(&trnd->waitq)) {
+        DEBUG_TR("wake up!");
+        wake_up_interruptible(&trnd->waitq);
+    }
+
+    return 0;
+}
+void vtss_transport_wake_up_all(void)
+{
+    unsigned long flags;
+    struct list_head *p;
+    struct vtss_transport_data *trnd = NULL;
+#ifdef VTSS_USE_NMI
+    if(!vtss_spin_trylock_irqsave(&vtss_transport_list_lock, flags))
+        return;
+#else
+    vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+#endif
+    list_for_each(p, &vtss_transport_list) {
+        touch_nmi_watchdog();
+        trnd = list_entry(p, struct vtss_transport_data, list);
+        if (trnd == NULL){
+             ERROR("tick: trnd in list is NULL");
+             continue;
+        }
+	
+	if (trnd->type & VTSS_TR_RB && (!atomic_read(&trnd->is_complete))) {
+#ifndef VTSS_USE_UEC
+        //currently we use vtss_consume_ring_buffer_cpu
+        //vtss_consume_ring_buffer(trnd, 0);
+#endif
+	    continue;
+	}
+        if (atomic_read(&trnd->is_attached)) {
+            if (waitqueue_active(&trnd->waitq)) {
+                DEBUG_TR("trnd=0x%p => '%s'", trnd, trnd->name);
+                wake_up_interruptible(&trnd->waitq);
+            }
+        }
+    }
+    vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+
+}
+#ifdef VTSS_TRANSPORT_TIMER_INTERVAL
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,15,0)
+static void vtss_transport_tick(struct timer_list *unused)
+#else
+static void vtss_transport_tick(unsigned long val)
+#endif
+{
+    if (atomic_read(&vtss_is_transport_init)==0){
+        DEBUG_TR("Transport tick while uninit. Ignore.");
+        return;
+    }
+    vtss_transport_wake_up_all();
+    mod_timer(&vtss_transport_timer, jiffies + VTSS_TRANSPORT_TIMER_INTERVAL);
+
+}
+#endif /* VTSS_TRANSPORT_TIMER_INTERVAL */
+
+int vtss_transport_debug_info(struct seq_file *s)
+{
+    unsigned long flags;
+    struct list_head *p;
+    struct vtss_transport_data *trnd = NULL;
+
+    seq_printf(s, "\n[transport]\nnbuffers=%u (%lu bytes)\n", atomic_read(&vtss_transport_npages), atomic_read(&vtss_transport_npages)*PAGE_SIZE);
+    vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+    list_for_each(p, &vtss_transport_list) {
+        trnd = list_entry(p, struct vtss_transport_data, list);
+        seq_printf(s, "\n[proc %s]\nis_attached=%s\nis_complete=%s\nis_overflow=%s\nrefcount=%d\nloscount=%d\nevtcount=%lu\n",
+                    trnd->name,
+                    atomic_read(&trnd->is_attached) ? "true" : "false",
+                    atomic_read(&trnd->is_complete) ? "true" : "false",
+                    atomic_read(&trnd->is_overflow) ? "true" : "false",
+                    atomic_read(&trnd->refcount),
+                    atomic_read(&trnd->loscount),
+#ifdef VTSS_USE_UEC
+                    0UL);
+#else
+
+                    ring_buffer_entries(trnd->buffer));
+        if (!ring_buffer_empty(trnd->buffer)) {
+            int cpu;
+            for_each_online_cpu(cpu) {
+                unsigned long count = ring_buffer_entries_cpu(trnd->buffer, cpu);
+                if (count)
+                    seq_printf(s, "evtcount[%03d]=%lu\n", cpu, count);
+            }
+        }
+        seq_printf(s, "evtstore=%d of %d\n", atomic_read(&trnd->seqdone)-1, atomic_read(&trnd->seqnum));
+        seq_printf(s, "is_abort=%s\n", trnd->is_abort ? "true" : "false");
+#endif /* VTSS_USE_UEC */
+    }
+    vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+    return 0;
+}
+
+int vtss_transport_init(int rb)
+{
+#if LINUX_VERSION_CODE > KERNEL_VERSION(3,6,32)
+    int rc;
+#endif
+    unsigned long flags;
+    atomic_set(&vtss_transport_npages, 0);
+    atomic_set(&vtss_transport_mode, rb);
+    atomic_set(&vtss_free_tr_cnt, 0);
+    atomic_set(&vtss_ring_buffer_stopped, 0);
+    atomic_set(&vtss_ring_buffer_paused, 0);
+
+    vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+    INIT_LIST_HEAD(&vtss_transport_list);
+    vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(3,6,32)
+    rc = vtss_transport_prealloc_transport_items();
+    if (rc == -1) {
+        ERROR("Cannot preallocate transport items");
+        return -1;
+    }
+#endif
+#ifdef VTSS_TRANSPORT_TIMER_INTERVAL
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,15,0)
+    timer_setup(&vtss_transport_timer, vtss_transport_tick, 0);
+    vtss_transport_timer.expires = jiffies + VTSS_TRANSPORT_TIMER_INTERVAL;
+#else
+    init_timer(&vtss_transport_timer);
+    vtss_transport_timer.expires  = jiffies + VTSS_TRANSPORT_TIMER_INTERVAL;
+    vtss_transport_timer.function = vtss_transport_tick;
+    vtss_transport_timer.data     = 0;
+#endif
+    add_timer(&vtss_transport_timer);
+#endif
+    atomic_set(&vtss_is_transport_init, 1);
+#ifdef VTSS_USE_UEC
+    INFO("TRANSPORT: use UEC");
+#endif
+    return 0;
+}
+
+void vtss_transport_fini(void)
+{
+    int wait_count = VTSS_TRANSPORT_COMPLETE_TIMEOUT;
+    unsigned long flags, count;
+    struct list_head* p = NULL;
+    struct list_head* tmp = NULL;
+    struct vtss_transport_data *trnd = NULL;
+
+    DEBUG_TR("start, in_atomic = %d", in_atomic());
+    if (!atomic_dec_and_test(&vtss_is_transport_init))
+    {
+        ERROR("transport does not initialized: %d", atomic_read(&vtss_is_transport_init));
+        atomic_set(&vtss_is_transport_init, 0);
+        return;
+    }
+#ifdef VTSS_TRANSPORT_TIMER_INTERVAL
+    del_timer_sync(&vtss_transport_timer);
+#endif
+    count = 0;
+    while (atomic_read(&vtss_kernel_task_in_progress)){
+         count++;
+         if (count == 10000) ERROR("kernel task in progress, atomic_read(&vtss_kernel_task_in_progress)=%d", atomic_read(&vtss_kernel_task_in_progress));
+    }
+again:
+    //vtss_transport_wake_up_all();
+    vtss_spin_lock_irqsave(&vtss_transport_list_lock, flags);
+    list_for_each_safe(p, tmp, &vtss_transport_list) {
+        trnd = list_entry(p, struct vtss_transport_data, list);
+        touch_nmi_watchdog();
+        if (trnd == NULL){
+             ERROR("fini: trnd in list is NULL");
+             continue;
+        }
+#ifndef VTSS_USE_UEC
+        if (trnd->type & VTSS_TR_RB) {
+            int old_state = atomic_cmpxchg(&trnd->processing_state, 0, 3);
+            while (old_state == 1) old_state = atomic_cmpxchg(&trnd->processing_state, 0, 3);
+            if (old_state == 2 || old_state == 4) atomic_set(&trnd->processing_state, 3);
+        }
+#endif
+        atomic_inc(&trnd->is_complete);
+#ifndef VTSS_USE_UEC
+        while  (atomic_read(&trnd->reserved))
+        {
+            DEBUG_TR("trnd is reserved");
+            msleep_interruptible(1);
+        }
+#endif
+        if (atomic_read(&trnd->is_attached)) {
+            int cnt = 1000;
+            if (--wait_count > 0) {
+                //if (waitqueue_active(&trnd->waitq)) {
+                //    wake_up_interruptible(&trnd->waitq);
+                //}
+                vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+                while (!waitqueue_active(&trnd->waitq) && (cnt--)) {
+                    msleep_interruptible(1);
+                }
+                if (waitqueue_active(&trnd->waitq)) {
+                    wake_up_interruptible(&trnd->waitq);
+                }
+                else
+                {
+                  INFO("%s: queue is not active!", trnd->name);
+                }
+                msleep_interruptible(20);
+                goto again;
+            }
+            ERROR("%s: complete timeout", trnd->name);
+            if (trnd->file){
+                trnd->file->private_data = NULL;
+                trnd->file = NULL;
+            }
+        }
+        list_del(p);
+        if (atomic_read(&trnd->loscount)) {
+            ERROR("TRANSPORT: '%s' lost %d events", trnd->name, atomic_read(&trnd->loscount));
+        }
+        vtss_transport_remove(trnd);
+#ifdef VTSS_CONFIG_REALTIME
+        atomic_inc(&vtss_kernel_task_in_progress);
+        if( !in_atomic() || vtss_queue_work(-1, vtss_transport_destroy_trnd_work, &trnd, sizeof(trnd)))
+        {
+                ERROR("Cannot remove transport!");
+                //vtss_transport_destroy_trnd(trnd);
+                atomic_dec(&vtss_kernel_task_in_progress);
+        }
+
+#else
+        vtss_transport_destroy_trnd(trnd);
+#endif
+        wait_count = VTSS_TRANSPORT_COMPLETE_TIMEOUT;
+    }
+    INIT_LIST_HEAD(&vtss_transport_list);
+    vtss_spin_unlock_irqrestore(&vtss_transport_list_lock, flags);
+    while (atomic_read(&vtss_kernel_task_in_progress))
+    {
+      count++;
+      if (count == 10000) DEBUG_TR("2:kernel task in progress, atomic_read(&vtss_kernel_task_in_progress)=%d", atomic_read(&vtss_kernel_task_in_progress));
+    }
+    if (atomic_read(&vtss_transport_npages)) {
+        ERROR("lost %u (%lu bytes) buffers", atomic_read(&vtss_transport_npages), atomic_read(&vtss_transport_npages)*PAGE_SIZE);
+    }
+    atomic_set(&vtss_transport_npages, 0);
+    INFO("TRANSPORT: stopped");
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/uec.c b/drivers/misc/intel/sepdk/vtsspp/uec.c
new file mode 100644
index 000000000000..4ae9485b7f12
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/uec.c
@@ -0,0 +1,633 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+#include "globals.h"
+#include "uec.h"
+#include "time.h"
+#include "memory_pool.h"
+
+#include <linux/slab.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,12,0)
+#include <linux/uaccess.h>
+#endif
+#include <asm/uaccess.h>
+
+//The maximum allowed value for order is 10 or 11 (corresponding to 1024 or 2048 pages), depending on the architecture.
+// but these are huge memory chunks fro 32-bit kernel, so decrease to 8
+#define VTSS_MAX_ORDER_SIZE 8
+#if defined(__i386__)
+#define VTSS_MAX_UEC_ORDER_SIZE 8
+#else
+#define VTSS_MAX_UEC_ORDER_SIZE 10
+#endif
+#define VTSS_MIN_UEC_ORDER_SIZE 4
+
+/**
+// Universal Event Collector: system mode implementation
+*/
+
+/// chained UEC operations
+
+int init_uec_chain(uec_t* uec, size_t size, char* name, int chain_size)
+{
+    int order = 0, alloc_order = 0;
+    int i;
+    int rc = 0;
+    uec_t* curr = uec;
+
+    for(i = 0; i < chain_size; i++, uec++)
+    {
+        /// initialize methods
+        uec->put_record = put_record_chain;
+        uec->destroy = destroy_uec_chain;
+        uec->pull = pull_uec_chain;
+        uec->init = init_uec_chain;
+
+        if (reqcfg.ipt_cfg.size) {
+            /// allocate buffers
+            alloc_order = order = get_order(size);
+            if (alloc_order > VTSS_MAX_UEC_ORDER_SIZE) alloc_order =  VTSS_MAX_UEC_ORDER_SIZE;
+            if (alloc_order <= VTSS_MIN_UEC_ORDER_SIZE) alloc_order = VTSS_MIN_UEC_ORDER_SIZE + 1; 
+            while (alloc_order > VTSS_MIN_UEC_ORDER_SIZE) {
+                if ((uec->buffer = (char*)__get_free_pages(GFP_KERNEL | __GFP_NOWARN, alloc_order))) {
+                    ERROR("order %d, allocated order %d", order, alloc_order);
+                    break;
+                }
+                alloc_order--;
+           }
+            if (alloc_order == VTSS_MIN_UEC_ORDER_SIZE) {
+                ERROR("No memory for UEC chain buffer[%d]", i);
+                rc = VTSS_ERR_NOMEMORY;
+            }
+        }
+        else {
+            uec->buffer = NULL;
+        }
+        /// initialize fields
+        uec->last_rec_ptr = uec->head = uec->head_ = uec->tail = uec->tail_ = uec->buffer;
+        uec->hsize = uec->tsize = alloc_order ? (PAGE_SIZE << alloc_order) : 0;
+
+        uec->ovfl = 0;
+        uec->spill_active = 0;
+        uec->chain_busy = 0;
+        uec->writer_count = 0;
+        uec->reader_count = 0;
+        uec->cputsc = 0;
+
+        vtss_spin_lock_init(&uec->lock);
+        /// chain up the UECs
+        uec->curr = curr;
+        uec->next = uec + 1;
+        uec->full_next = uec + 1;
+    }
+    (uec - chain_size + 1)->next = curr;
+    (uec - 1)->full_next = curr;
+    return rc;
+}
+
+/// for safety reasons (quick initialization)
+static int put_record_stub(uec_t* uec, void *part0, size_t size0, void *part1, size_t size1, int mode)
+{
+    return 0;
+}
+void destroy_uec_chain(uec_t* uec)
+{
+//    static unsigned int marker[2] = {UEC_MAGIC, UEC_MAGICVALUE};
+//    uec->put_record(uec, marker, 8, 0, 0, UECMODE_NORMAL);
+    /// find head of chain
+    while(uec != uec->curr)
+    {
+        uec = uec->next;
+    }
+    /// destroy UECs one by one
+    do
+    {
+        uec = uec->full_next;
+        destroy_uec(uec);
+    }
+    while(uec != uec->curr);
+}
+
+#if 0
+void spill_uec_chain(uec_t* uec)
+{
+    uec_t* chain = uec;
+    unsigned long flags;
+    INFO("start");
+    /// synchronize spills and buffer switches
+    vtss_spin_lock_irqsave(&uec->lock, flags);
+
+    if(chain->chain_busy)
+    {
+        vtss_spin_unlock_irqrestore(&uec->lock, flags);
+        INFO("end1");
+        return;
+    }
+    chain->chain_busy = 1;
+
+    vtss_spin_unlock_irqrestore(&uec->lock, flags);
+
+
+    /// find head of chain
+    while(uec != uec->curr)
+    {
+        uec = uec->next;
+    }
+    /// spill UECs one by one
+    do
+    {
+        uec = uec->next;
+       //spill_uec_file(uec);
+    }
+    while(uec != uec->curr);
+
+    /// synchronize spills and buffer switches
+    vtss_spin_lock_irqsave(&uec->lock, flags);
+
+    chain->chain_busy = 0;
+
+    vtss_spin_unlock_irqrestore(&uec->lock, flags);
+    INFO("end");
+}
+#endif
+
+int put_record_chain(uec_t* uec, void* part0, size_t size0, void* part1, size_t size1, int mode)
+{
+    uec_t* chain = uec, *extra_uec = NULL;
+    int res = 0;
+    unsigned long flags = 0, flags1 = 0;
+    unsigned long extra_count = 0;
+    long long buffer_time = 0;
+    
+    /// find head; write; if nomem, lock{find head, if same, switch buffers, else, continue}
+    for(;;)
+    {
+        uec = chain;
+        /// find head of chain
+        while(uec != uec->curr)
+        {
+            uec = uec->next;
+        }
+        /// write a record and switch UECs if the current one gets full
+        if((uec->tail == uec->head && uec->ovfl) ||
+            (res = put_record_async(uec, part0, size0, part1, size1, UECMODE_SAFE)) == VTSS_ERR_BUFFERFULL)
+        {
+            uec_t* uec_locked = uec;
+            /// synchronize spills and buffer switches
+            vtss_spin_lock_irqsave(&uec_locked->lock, flags);
+
+            if(chain->chain_busy)
+            {
+                /// lose data when spilling the chain
+                vtss_spin_unlock_irqrestore(&uec->lock, flags);
+                return res;
+            }
+            if(uec->curr != uec)
+            {
+                /// buffer switched, retry
+                vtss_spin_unlock_irqrestore(&uec->lock, flags);
+                continue;
+            }
+
+            // increase buffers in tail only
+            if (uec->next != uec->full_next)
+            {
+                if (uec->cputsc)
+                {
+#if defined(__i386__)
+                    if (hardcfg.cpu_freq)
+                    {
+                        buffer_time = 1000*(vtss_time_cpu() - uec->cputsc);
+                        do_div(buffer_time, hardcfg.cpu_freq);
+                        if (buffer_time)
+                        {
+                            extra_count = reqcfg.ipt_cfg.size;
+                            do_div(extra_count, buffer_time);
+                        }
+                        else
+                        {
+                            extra_count = 1;
+                        }
+                    }
+#else
+                    if (hardcfg.cpu_freq > 0 && (buffer_time = (1000 * (vtss_time_cpu() - uec->cputsc)) / hardcfg.cpu_freq) > 0)
+                    {
+                        extra_count = reqcfg.ipt_cfg.size / buffer_time;
+                    }
+#endif
+                    else
+                    {
+                        extra_count = 1;
+                    }
+                    if (extra_count)
+                    {
+                        extra_count += 1;
+                        extra_uec = uec;
+                        while(extra_count && extra_uec->full_next != extra_uec->next)
+                        {
+                            extra_count--;
+                            extra_uec->full_next->next = extra_uec->next;
+                            extra_uec->next = extra_uec->full_next;
+                            extra_uec = extra_uec->next;
+                        }
+                    }
+                }
+                else uec->cputsc = vtss_time_cpu();
+                extra_count = 0;
+            }
+
+            /// switch UECs
+            uec->curr = uec->next;
+            uec = uec->next;
+            uec->curr = uec;
+
+            /// clear UEC circular buffer
+            if(chain != uec)
+            {
+                vtss_spin_lock_irqsave(&uec->lock, flags1);
+            }
+            uec->last_rec_ptr = uec->head = uec->head_ = uec->tail = uec->tail_ = uec->buffer;
+            uec->ovfl = 0;
+
+            if(chain != uec)
+            {
+                vtss_spin_unlock_irqrestore(&uec->lock, flags1);
+            }
+
+            /// synchronize spills and buffer switches
+            vtss_spin_unlock_irqrestore(&uec_locked->lock, flags);
+        }
+        else
+        {
+            break;
+        }
+    }
+    return res;
+}
+
+int pull_uec_chain(uec_t* uec, char __user* buffer, size_t len)
+{
+    /// TODO: implement transparent puuling of data from chained UECs
+    ///       may be needed when exposing PT traces to a debugger
+    uec_t* chain = uec;
+    unsigned long flags;
+    int rc = 0;
+    /// synchronize spills and buffer switches
+    vtss_spin_lock_irqsave(&uec->lock, flags);
+
+    if(chain->chain_busy)
+    {
+        vtss_spin_unlock_irqrestore(&uec->lock, flags);
+        return VTSS_ERR_BUSY;
+    }
+    chain->chain_busy= 1;
+
+    vtss_spin_unlock_irqrestore(&uec->lock, flags);
+
+
+    /// find head of chain
+    while(uec != uec->curr)
+    {
+        uec = uec->next;
+    }
+    
+    /// spill UECs one by one
+    do
+    {
+        int rc1 = 0;
+        uec = uec->next;
+        buffer = buffer + rc;
+    
+        rc1 = pull_uec(uec, buffer, len);
+        if (rc1 < 0){
+            if (rc > 0) break;
+            vtss_spin_lock_irqsave(&uec->lock, flags);
+            chain->chain_busy = 0;
+            vtss_spin_unlock_irqrestore(&uec->lock, flags);
+            ERROR("Cannot pull record, rc = %d, rc1 =%d", rc, rc1);
+            return rc1;
+        }
+        if(len < rc1) break;
+
+        len -= rc1;
+        rc += rc1;
+    }
+    while(uec != uec->curr);
+
+    /// synchronize spills and buffer switches
+    vtss_spin_lock_irqsave(&uec->lock, flags);
+
+    chain->chain_busy = 0;
+
+    vtss_spin_unlock_irqrestore(&uec->lock, flags);
+    return rc;
+}
+
+//normal uec implementation
+
+
+int init_uec(uec_t* uec, size_t size, char *name, int instance)
+{
+    int order, alloc_order;
+    /// initialize methods
+    uec->put_record = put_record_async;
+    uec->init = init_uec;
+    uec->destroy = destroy_uec;
+    uec->pull = pull_uec;
+
+    if (size == 0) { /// change name request
+        ERROR("UEC size is 0");
+        return VTSS_ERR_INTERNAL;
+    }
+
+    /// allocate buffers
+    alloc_order = order = get_order(size);
+    if (alloc_order > VTSS_MAX_UEC_ORDER_SIZE) alloc_order =  VTSS_MAX_UEC_ORDER_SIZE;
+
+    while (alloc_order > VTSS_MIN_UEC_ORDER_SIZE) {
+        if ((uec->buffer = (char*)__get_free_pages(GFP_KERNEL | __GFP_NOWARN, alloc_order))) {
+            break;
+        }
+        alloc_order--;
+    }
+    if (alloc_order == VTSS_MIN_UEC_ORDER_SIZE) {
+        ERROR("No memory for UEC buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+
+    uec->last_rec_ptr = uec->head = uec->head_ = uec->tail = uec->tail_ = uec->buffer;
+    uec->hsize = uec->tsize = PAGE_SIZE << alloc_order;
+
+    uec->ovfl = 0;
+    uec->spill_active = 0;
+    uec->writer_count = 0;
+    uec->reader_count = 0;
+
+    vtss_spin_lock_init(&uec->lock);
+    /// notify on the creation of a new trace
+//    uec->callback(uec, UECCB_NEWTRACE, uec->context);
+    return 0;
+}
+
+void destroy_uec(uec_t* uec)
+{
+    uec->put_record = put_record_stub;
+
+    if (uec->buffer) {
+        free_pages((unsigned long)uec->buffer, get_order(uec->hsize));
+        uec->buffer = NULL;
+    }
+}
+
+#define safe_memcpy(dst, src, size) memcpy(dst, src, size)
+#define spill_uec() /* empty */
+
+int put_record_async(uec_t* uec, void *part0, size_t size0, void *part1, size_t size1, int mode)
+{
+    size_t tsize;                  /// total record size
+    size_t fsize = 0;              /// free area length
+    size_t psize;                  /// partial size
+    size_t tmp;
+    char *last_rec_ptr;
+    char *head = 0;
+    char *tail = 0;
+    size_t hsize = 0;
+    unsigned long flags;
+
+    if (!uec->buffer || !part0 || !size0 || ((!size1) ^ (!part1))) {
+        return VTSS_ERR_BADARG;
+    }
+    tsize = size0 + size1;
+
+    /// lock UEC
+#ifdef VTSS_USE_NMI
+    if(!vtss_spin_lock_irqsave_timeout(&uec->lock, flags, 10000))
+        return VTSS_ERR_BUSY;
+#else
+    vtss_spin_lock_irqsave(&uec->lock, flags);
+#endif
+
+    /// sample the uec variables
+    head = (char*)uec->head_;
+    tail = (char*)uec->tail;
+    last_rec_ptr = head;
+    hsize = uec->hsize;
+
+    /// is buffer full?
+    if (uec->ovfl) {
+        *((unsigned int*)uec->last_rec_ptr) |= UEC_OVERFLOW;
+        /// signal overflow to overlying components
+//        uec->callback(uec, UECCB_OVERFLOW, uec->context);
+        vtss_spin_unlock_irqrestore(&uec->lock, flags);
+        spill_uec();
+        return VTSS_ERR_BUFFERFULL;
+    }
+    /// compute free size
+    if (tail <= head) {
+        fsize = uec->hsize - (size_t)(head - tail);
+    } else {
+        fsize = (size_t)(tail - head);
+    }
+    /// handle 'no room' case
+    if (fsize < tsize) {
+        uec->ovfl = 1;
+        *((unsigned int*)uec->last_rec_ptr) |= UEC_OVERFLOW;
+        /// signal overflow to overlying components
+//        uec->callback(uec, UECCB_OVERFLOW, uec->context);
+        vtss_spin_unlock_irqrestore(&uec->lock, flags);
+        spill_uec();
+        return VTSS_ERR_BUFFERFULL;//VTSS_ERR_NOMEMORY;
+    }
+    /// allocate uec region
+    psize = (size_t)(uec->buffer + hsize - head);
+
+    if (psize > tsize) {
+        uec->head_ = head + tsize;
+    } else {
+        uec->head_ = uec->buffer + tsize - psize;
+    }
+    if (uec->head_ == tail) {
+        uec->ovfl = 1;
+    }
+    /// increment the writers' count
+    uec->writer_count++;
+
+    /// unlock UEC
+    vtss_spin_unlock_irqrestore(&uec->lock, flags);
+
+    /// do the write to the allocated uec region
+
+    if (tail <= head) {
+        if (psize > tsize) {
+            memcpy(head, part0, size0);
+            head += size0;
+            if (size1) {
+                safe_memcpy(head, part1, size1);
+                head += size1;
+            }
+        } else {
+            tmp = size0 > psize ? psize : size0;
+            memcpy(head, part0, tmp);
+            head += tmp;
+
+            if (tmp == psize) {
+                head = uec->buffer;
+                if ((size0 - tmp) > 0) {
+                    memcpy(head, ((char*)part0) + tmp, size0 - tmp);
+                    head += size0 - tmp;
+                }
+            }
+            if (size1) {
+                psize -= tmp;
+                if (psize) {
+                    safe_memcpy(head, part1, psize);
+                    head = uec->buffer;
+                    if ((size1 - psize) > 0) {
+                        memcpy(head, ((char*)part1) + psize, size1 - psize);
+                        head += size1 - psize;
+                    }
+                } else {
+                    safe_memcpy(head, part1, size1);
+                    head += size1;
+                }
+            }
+        }
+    } else /// tail > head
+    {
+        memcpy(head, part0, size0);
+        head += size0;
+        if (size1) {
+            safe_memcpy(head, part1, size1);
+            head += size1;
+        }
+    }
+
+    /// lock UEC
+    vtss_spin_lock_irqsave(&uec->lock, flags);
+
+    /// decrement the writers' count
+    uec->writer_count--;
+
+    uec->last_rec_ptr = last_rec_ptr;
+
+    /// update uec variables
+    if (!uec->writer_count) {
+        uec->head = uec->head_;
+    }
+    /// unlock UEC
+    vtss_spin_unlock_irqrestore(&uec->lock, flags);
+
+    spill_uec();
+
+    return 0;
+}
+
+int pull_uec(uec_t* uec, char __user* buffer, size_t len)
+{
+    int rc = 0;
+    char *head;
+    char *tail;
+    size_t size;
+    int ovfl;
+
+    size_t copylen = 0;
+    size_t partlen;
+
+    unsigned long flags;
+
+    /// sample the UEC state, copy the sampled contents to the specified buffer,
+    /// and free the read part of the UEC buffer
+
+    if (!uec->buffer || !buffer || !len) {
+        return VTSS_ERR_BADARG;
+    }
+    /// sample data region
+#ifdef VTSS_USE_NMI
+    if(!vtss_spin_lock_irqsave_timeout(&uec->lock, flags, 10000))
+        return VTSS_ERR_BUSY;
+#else
+    vtss_spin_lock_irqsave(&uec->lock, flags);
+#endif
+
+    if (uec->spill_active) {
+        vtss_spin_unlock_irqrestore(&uec->lock, flags);
+        return VTSS_ERR_BUSY;
+    }
+    uec->spill_active = 1;
+
+    head = (char*)uec->head;
+    tail = (char*)uec->tail;
+    size = uec->tsize;
+    ovfl = uec->ovfl;
+
+    if ((head == tail && !ovfl) || (head == tail && ovfl && head != uec->head_)) {
+        uec->spill_active = 0;
+        vtss_spin_unlock_irqrestore(&uec->lock, flags);
+        return 0; ///empty
+    }
+
+    vtss_spin_unlock_irqrestore(&uec->lock, flags);
+
+    /// spill the sampled region
+    if (head > tail) {
+        copylen = (size_t)(head - tail);
+        copylen = copylen > len ? len : copylen;
+
+        rc = copy_to_user(buffer, (void*)tail, copylen);
+
+        tail += copylen;
+    } else if (head < tail || (head == tail && ovfl)) {
+        copylen = partlen = (size_t)(size - (tail - uec->buffer));
+        copylen = copylen > len ? len : copylen;
+
+        rc = copy_to_user(buffer, (void*)tail, copylen);
+
+        tail += copylen;
+
+        if (copylen == partlen && copylen < len) {
+            /// copy the second part
+            partlen = (size_t)(head - uec->buffer);
+            partlen = partlen > (len - copylen) ? (len - copylen) : partlen;
+
+            rc |= copy_to_user(&((char*)buffer)[copylen], (void*)uec->buffer, partlen);
+
+            copylen += partlen;
+            /// assert(copylen <= len);
+
+            tail = uec->buffer + partlen;
+        }
+    }
+
+    vtss_spin_lock_irqsave(&uec->lock, flags);
+    uec->tail = tail;
+    uec->ovfl = 0;
+    uec->spill_active = 0;
+    vtss_spin_unlock_irqrestore(&uec->lock, flags);
+
+    return rc ? -1 : copylen;
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/unwind.c b/drivers/misc/intel/sepdk/vtsspp/unwind.c
new file mode 100644
index 000000000000..e6f8a2002c20
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/unwind.c
@@ -0,0 +1,685 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+/**
+//
+// Stack Unwinding and Related Support Functions
+//
+*/
+#include "vtss_config.h"
+#include "unwind.h"
+#include "memory_pool.h"
+
+/**
+// Stack unwinding functions
+*/
+
+static int  realloc_stack(stack_control_t* stk);
+static void destroy_stack(stack_control_t* stk);
+static void clear_stack(stack_control_t* stk);
+static int  unwind_stack_fwd(stack_control_t* stk);
+static int  validate_stack(stack_control_t* stk);   /// callback for validating IPs
+static void augment_stack(stack_control_t* stk);
+static int  is_full_stack(stack_control_t* stk);
+static int  compress_stack(stack_control_t* stk);
+static void setup_stack(stack_control_t* stk, user_vm_accessor_t* acc, void *ip, void *sp, void *bp, void *fp, int wow64);
+static char *data_stack(stack_control_t* stk);
+static void lock_stack(stack_control_t* stk);
+static int  trylock_stack(stack_control_t* stk);
+static void unlock_stack(stack_control_t* stk);
+
+/// initialize the stack control object
+int vtss_init_stack(stack_control_t * stk)
+{
+    memset(stk, 0, sizeof(stack_control_t));
+
+    vtss_spin_lock_init(&stk->spin_lock);
+    stk->init     = vtss_init_stack;
+    stk->realloc  = realloc_stack;
+    stk->destroy  = destroy_stack;
+    stk->clear    = clear_stack;
+    stk->unwind   = unwind_stack_fwd;
+    stk->validate = validate_stack;
+    stk->augment  = augment_stack;
+    stk->is_full  = is_full_stack;
+    stk->compress = compress_stack;
+    stk->setup    = setup_stack;
+    stk->data     = data_stack;
+    stk->lock     = lock_stack;
+    stk->trylock  = trylock_stack;
+    stk->unlock   = unlock_stack;
+
+    stk->kernel_callchain_size = VTSS_DYNSIZE_STACKS;
+    stk->kernel_callchain = (char *)vtss_get_free_pages(GFP_NOWAIT, get_order(VTSS_DYNSIZE_STACKS));
+    if (!stk->kernel_callchain) ERROR("Not enough memory for kernel callchain buffer");
+    stk->kernel_callchain_pos = 0;
+
+    stk->user_callchain_size = VTSS_DYNSIZE_STACKS;
+    stk->user_callchain = NULL;
+    if (reqcfg.trace_cfg.trace_flags & VTSS_CFGTRACE_CLRSTK) {
+        stk->user_callchain = (char *)vtss_get_free_pages(GFP_NOWAIT, get_order(VTSS_DYNSIZE_STACKS));
+        if (!stk->user_callchain) ERROR("Not enough memory for user callchain buffer");
+    }
+    stk->user_callchain_pos = 0;
+
+    stk->acc = vtss_user_vm_accessor_init(1, vtss_time_limit);
+    return realloc_stack(stk);
+}
+
+/// grow the stack map
+static int realloc_stack(stack_control_t* stk)
+{
+    int order;
+#ifdef VTSS_USE_NMI
+    // we cannot allocate memory in irqs_disabled mode
+    unsigned int len = 16*VTSS_DYNSIZE_STACKS;
+#else
+    unsigned int len = VTSS_DYNSIZE_STACKS;
+#endif
+    char* buf;
+
+    if(stk->buffer)
+    {
+        len = stk->size;
+    }
+    if(len > 16*VTSS_DYNSIZE_STACKS)
+    {
+        TRACE("limit of allocation stack buffer");
+        return VTSS_ERR_NOMEMORY;
+    }
+    order = get_order(len + len);
+    if(!(buf = (char*)vtss_get_free_pages(GFP_NOWAIT, order)))
+    {
+        return VTSS_ERR_NOMEMORY;
+    }
+    if(stk->buffer)
+    {
+        vtss_free_pages((unsigned long)stk->buffer, get_order(stk->size));
+    }
+    stk->buffer = buf;
+    stk->size = (PAGE_SIZE << order);
+    stk->compressed = stk->buffer + stk->size / 2;
+    stk->stkmap_end = stk->stkmap_start = stk->stkmap_common = (stkmap_t*)stk->compressed;
+    return 0;
+}
+
+/// destroy the stack control object
+static void destroy_stack(stack_control_t* stk)
+{
+    trylock_stack(stk);
+    if (stk->acc)
+    {
+        vtss_user_vm_accessor_fini(stk->acc);
+        stk->acc = NULL;
+    }
+    stk->acc = NULL;
+    unlock_stack(stk);
+    if(stk->buffer)
+    {
+        vtss_free_pages((unsigned long)stk->buffer, get_order(stk->size));
+        stk->buffer = NULL;
+        stk->size = 0;
+    }
+    if(stk->kernel_callchain)
+    {
+        vtss_free_pages((unsigned long)stk->kernel_callchain, get_order(stk->kernel_callchain_size));
+        stk->kernel_callchain = NULL;
+    }
+    if(stk->user_callchain)
+    {
+        vtss_free_pages((unsigned long)stk->user_callchain, get_order(stk->user_callchain_size));
+        stk->user_callchain = NULL;
+    }
+}
+
+/// clear stack map
+static void clear_stack(stack_control_t* stk)
+{
+    stk->compressed = stk->buffer + stk->size / 2;
+    stk->stkmap_end = stk->stkmap_start = stk->stkmap_common = (stkmap_t*)stk->compressed;
+}
+
+/// build an incremental map of stack, frame, and instruction pointers
+
+///#define SWAP(a, b) (a) += (b); (b) = (a) - (b); (a) = (a) - (b)
+///#define SWAP(a, b) {size_t c; c = (a); (a) = (b); (b) = c;}
+
+/// walk from SP up to stack base
+static int unwind_stack_fwd(stack_control_t* stk)
+{
+    stkptr_t value;
+    stkptr_t search_sp;
+    stkptr_t search_border;
+
+    char* sp = stk->user_sp.chp;
+    char* bp = stk->bp.chp;
+
+    int find_changed_region = 1;
+
+    stkmap_t* stkmap_curr;
+    stkmap_t* stkmap_end = stk->stkmap_end;
+    stkmap_t* stkmap_start = stk->stkmap_start;
+    stkmap_t* stkmap_common = stk->stkmap_common;
+
+    int stkmap_size = stk->size / 2;
+    int stkmap_commsize = 0;
+
+    int wow64 = stk->wow64;
+    int stride = wow64 ? 4 : sizeof(void*);
+
+    size_t tmp = stk->user_sp.szt;
+    size_t val_idx = VTSS_STACK_CACHE_SIZE;
+    char* vals = stk->value_cache;
+    size_t val_size = 0;
+
+    /* check for bad args */
+
+    if(!stk->buffer)
+    {
+        return VTSS_ERR_NOMEMORY;
+    }
+
+    /* do argument corrections */
+
+    /// align sp to machine word size
+    if(!wow64)
+    {
+        sp = (char*)(tmp + ((sizeof(void*) - (tmp & (sizeof(void*) - 1))) & ((!(tmp & (sizeof(void*) - 1))) - 1)));
+    }
+    else
+    {
+        sp = (char*)(tmp + ((sizeof(int) - (tmp & (sizeof(int) - 1))) & ((!(tmp & (sizeof(int) - 1))) - 1)));
+    }
+    /// correct initial stack map pointers
+    if((unsigned char*)stkmap_start == stk->compressed)
+    {
+        stk->compressed = stk->buffer;
+        stkmap_common = stkmap_end = stkmap_start = stk->stkmap_start = (stkmap_t*)(stk->buffer + stkmap_size);
+    }
+
+    /// clear the border between changed/unchanged stack map regions
+    stkmap_common = 0;
+
+    /* check if the stack and stack map intersect */
+    search_border.chp = 0;
+    if(stkmap_start != stkmap_end)  /// the map is not empty
+    {
+        if((stkmap_end - 1)->sp.chp >= bp || (stkmap_end - 1)->sp.chp < sp)
+        {
+            /// the stack map is beyond the actual stack, clear it
+            stkmap_common = stkmap_end = stkmap_start;
+            /// nothing to find
+            find_changed_region = 0;
+            /// search the entire stack, the map is emptied
+            search_border.chp = bp - stride;
+        }
+    }
+    else    /// empty stack
+    {
+        /// clear the stack map
+        stkmap_common = stkmap_end = stkmap_start;
+        /// nothing to find
+        find_changed_region = 0;
+        /// search the entire stack, the map is emptied
+        search_border.chp = bp - stride;
+    }
+
+    /* search the stack and the stack map to detect the changed/unchanged regions */
+    value.szt = 0;
+    if(find_changed_region)
+    {
+        /// TODO: enable the try-except block in case of numerous 'empty' stack records
+        ///__try
+        ///{
+            for(stkmap_curr = stkmap_start; stkmap_curr < stkmap_end; stkmap_curr++)
+            {
+                touch_nmi_watchdog();
+                /// check if within the actual stack
+                if(stkmap_curr->sp.chp < sp)
+                {
+                    /// remember the changed border
+                    stkmap_common = stkmap_curr;
+                    continue;
+                }
+                /// read in the actual stack contents
+                if (stk->acc->read(stk->acc, stkmap_curr->sp.szp, &vals[0], stride) != stride) {
+                    TRACE("SP=0x%p: break search, [0x%p - 0x%p], ip=0x%p", stkmap_curr->sp.szp, stk->user_sp.vdp, stk->bp.vdp, stk->user_ip.vdp);
+                    /// clear the stack map
+                    stkmap_common = stkmap_end = stkmap_start;
+                    /// search the entire stack, the map is emptied
+                    search_border.chp = bp - stride;
+                    goto end_of_search;
+                }
+                value.szt = 0;
+                value.szt = wow64 ? *(u32*)(&vals[0]) : *(u64*)(&vals[0]);
+                //value.szt = (size_t)(wow64 ? *stkmap_curr->sp.uip : *stkmap_curr->sp.szp);
+                /// check if the current element has changed
+                if(stkmap_curr->value.szt != value.szt)
+                {
+                    /// remember the changed border
+                    stkmap_common = stkmap_curr;
+                }
+                else
+                {
+                    /// check the stack above the last map element
+                    if(stkmap_curr + 1 == stkmap_end)
+                    {
+                        /* Not required for Linux */
+#if 0
+                        for(search_sp.chp = stkmap_curr->sp.chp + stride; search_sp.chp < bp; search_sp.chp += stride)
+                        {
+                            value.szt = 0; // = (size_t)(wow64 ? *search_sp.uip : *search_sp.szp);
+                            if (stk->acc->read(stk->acc, search_sp.szp, &value.szt, stride) != stride) {
+                                TRACE("SP=0x%p: break search, [0x%p - 0x%p], ip=0x%p", search_sp.szp, stk->sp.vdp, stk->bp.vdp, stk->ip.vdp);
+                                /// clear the stack map
+                                stkmap_common = stkmap_end = stkmap_start;
+                                /// search the entire stack, the map is emptied
+                                search_border.chp = bp - stride;
+                                goto end_of_search;
+                            }
+
+                            if(value.chp < sp || value.chp >= bp)
+                            {
+                                if (stk->acc->validate(stk->acc, (unsigned long)value.szt))
+                                {
+                                    /// remember the changed border
+                                    stkmap_common = stkmap_curr;
+                                    break;
+                                }
+                            }
+                        }
+#endif
+                    }
+                    else
+                    {
+                        if(value.chp < sp || value.chp >= bp)
+                        {
+                            /// do extra IP search above the current IP element
+                            if(stkmap_curr + 1 < stkmap_end)
+                            {
+                                search_border.chp = min((stkmap_curr + 1)->sp.chp, stkmap_curr->sp.chp + IP_SEARCH_RANGE * stride);
+                            }
+                            else
+                            {
+                                search_border.chp = min(bp, stkmap_curr->sp.chp + IP_SEARCH_RANGE * stride);
+                            }
+                            //stk->value = value;
+                            /// search for IPs from the same module
+                            val_size = 0;
+                            for(search_sp.chp = stkmap_curr->sp.chp + stride; search_sp.chp < search_border.chp; search_sp.chp += stride, val_idx += stride)
+                            {
+                                touch_nmi_watchdog();
+                                value.szt = 0;
+                                if (val_idx >= val_size)
+                                {
+                                val_idx = 0;
+                                val_size = min(PAGE_SIZE-(unsigned long)(search_sp.szt&(~PAGE_MASK)), (unsigned long)(search_border.chp-search_sp.chp+stride));
+                                val_size = min((size_t)IP_SEARCH_RANGE * stride, val_size);
+                                if (stk->acc->read(stk->acc, search_sp.szp, &vals[val_idx], val_size) != val_size) {
+                                    TRACE("SP=0x%p: break search, [0x%p - 0x%p], ip=0x%p", search_sp.szp, stk->user_sp.vdp, stk->bp.vdp, stk->user_ip.vdp);
+                                    /// clear the stack map
+                                    stkmap_common = stkmap_end = stkmap_start;
+                                    /// search the entire stack, the map is emptied
+                                    search_border.chp = bp - stride;
+                                    val_idx = VTSS_STACK_CACHE_SIZE;
+                                    val_size = 0;
+                                    goto end_of_search;
+                                }
+                                }
+                                value.szt = wow64 ? *(u32*)(&vals[val_idx]) : *(u64*)(&vals[val_idx]);
+                                /// this is a relaxed IP search condition (to increase the performance)
+                                if(stk->acc->validate(stk->acc, (unsigned long)value.szt))
+                                {
+                                    /// remember the changed border
+                                    stkmap_common = stkmap_curr;
+                                    break;
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+            if(stkmap_common)   /// found the chnaged/unchanged border
+            {
+                /// use the map element following the changed one
+                stkmap_common++;
+            }
+            else                /// unchanged stack
+            {
+                stkmap_common = stkmap_start;
+            }
+            /// search below the unchanged region
+            search_border.chp = stkmap_common == stkmap_end ? bp - stride : stkmap_common->sp.chp - stride;
+            /// switch to a new stack map
+            {
+                void* p = (void*)stkmap_start;
+                stkmap_start = (stkmap_t*)stk->compressed;
+                stk->compressed = (unsigned char*)p;
+            }
+        ///}
+        ///__except(EXCEPTION_EXECUTE_HANDLER)
+        ///{
+        ///    /// clear the stack map
+        ///    stkmap_common = stkmap_end = stkmap_start;
+        ///    /// search the entire stack, the map is emptied
+        ///    search_border.chp = bp - stride;
+        ///}
+    }
+end_of_search:
+
+    /// compute the size of the unchanged stack map region
+    stkmap_commsize = (int)((stkmap_end - stkmap_common) * sizeof(stkmap_t));
+    /// correct the free size of stack map
+    stkmap_size -= stkmap_commsize;
+    val_idx = VTSS_STACK_CACHE_SIZE;
+
+    /* search the stack for IPs and FPs and update the stack map */
+    val_size = 0;
+
+    for(search_sp.chp = sp, stkmap_curr = stkmap_start; search_sp.chp <= search_border.chp; search_sp.chp += stride, val_idx +=stride)
+    {
+        touch_nmi_watchdog();
+        if((char*)stkmap_curr - (char*)stkmap_start >= stkmap_size)
+        {
+            /// the map is full
+            if(stk->hugetlb) {
+                if((char*)stkmap_curr - (char*)stkmap_start >= stk->size/2)
+                {
+                    // the map completely full
+                    stk->stkmap_start = stkmap_start;
+                    stk->stkmap_end = stk->stkmap_common = stkmap_curr;
+                    return VTSS_ERR_NOMEMORY;
+                }
+                else {
+                    // try to continue to search entire stack
+                    stkmap_size = stk->size/2; //use entire buf
+                    search_border.chp = stk->bp.chp; //reset border
+                    stkmap_common = stkmap_end; //full stack
+                }
+            }
+            else {
+                return VTSS_ERR_NOMEMORY;
+            }
+        }
+        if (val_idx >= val_size)
+        {
+            /// read a value from the stack
+            val_size = min(VTSS_STACK_READ_SIZE-(unsigned long)(search_sp.szt&(~VTSS_STACK_READ_MASK)), (unsigned long)(search_border.chp-search_sp.chp+stride));
+            val_size = min((size_t)VTSS_STACK_CACHE_SIZE, val_size);
+            val_idx = 0;
+            value.szt = 0;
+            if (stk->acc->read(stk->acc, search_sp.szp, &vals[val_idx], val_size) != val_size) {
+                TRACE("SP=0x%p: skip page, [0x%p - 0x%p], ip=0x%p", search_sp.szp, stk->user_sp.vdp, stk->bp.vdp, stk->user_ip.vdp);
+                if(stk->hugetlb) break;
+#ifdef VTSS_USE_NMI
+                break;
+#else
+                if ((search_border.szt - (search_sp.szt & VTSS_STACK_READ_MASK)) > VTSS_STACK_READ_SIZE) {
+                    search_sp.chp += VTSS_STACK_READ_SIZE;
+                    search_sp.szt &= VTSS_STACK_READ_MASK;
+                    search_sp.chp -= stride;
+                    val_idx = VTSS_STACK_CACHE_SIZE;
+                    val_size = 0;
+                    continue;
+                } else {
+                    break;
+                }
+#endif
+            }
+        }
+
+            value.szt = wow64 ? *(u32*)(&vals[val_idx]) : *(u64*)(&vals[val_idx]);
+            /// validate the value
+            if(value.chp < search_sp.chp || value.chp >= bp)
+            {
+                /// it's not FP, check for IP
+                if( !stk->acc->validate(stk->acc, (unsigned long)value.szt) )
+                {
+                    /// not IP
+                    continue;
+                }
+            }
+            else
+            {
+                /// validate FP
+                if(value.szt & (wow64 ? 3 : sizeof(void*) - 1))
+                {
+                    continue;
+                }
+            }
+            /// it's either FP or IP, store it  
+            stkmap_curr->sp = search_sp;
+            stkmap_curr->value = value;
+            stkmap_curr++;
+    }
+    if(stkmap_common != stkmap_end)
+    {
+        /// merge in the unchanged stack map region
+        memcpy(stkmap_curr, stkmap_common, stkmap_commsize);
+        /// include one common element into the current stack increment
+        stkmap_curr++;
+        stkmap_commsize -= sizeof(stkmap_t);
+    }
+    /// store the map parameters
+    stk->stkmap_start = stkmap_start;
+    stk->stkmap_end = (stkmap_t*)((char*)stkmap_curr + stkmap_commsize);
+    stk->stkmap_common = stkmap_curr;
+    return 0;
+}
+
+/// validate a return address (callback)
+static int validate_stack(stack_control_t* stk)
+{
+    int rc = 0;
+    VTSS_PROFILE(vld, rc = stk->acc->validate(stk->acc, (unsigned long)stk->value.szt));
+    return rc;
+}
+
+/// ensure a full stack map is generated
+static void augment_stack(stack_control_t* stk)
+{
+    stk->stkmap_common = stk->stkmap_end;
+}
+
+/// check if the stack map increment is zero
+static int is_full_stack(stack_control_t* stk)
+{
+    return stk->stkmap_common == stk->stkmap_end;
+}
+
+/// compress the collected stack map
+static int compress_stack(stack_control_t* stk)
+{
+    size_t* map;
+    int count;
+    size_t ip;
+    size_t sp;
+    size_t fp;
+    unsigned char* compressed;
+    int i, j;
+    int prefix;
+    size_t value;
+    size_t base;
+    size_t offset;
+    int sign;
+    size_t tmp;
+    size_t stksize = stk->size / 2;
+
+    compressed = stk->compressed;
+
+    base = stk->bp.szt;
+    ip = stk->user_ip.szt;
+    sp = stk->user_sp.szt;
+    fp = stk->user_fp.szt;
+
+    map = (size_t*)stk->stkmap_start;
+    count = (int)(stk->stkmap_common - stk->stkmap_start) * 2;
+
+    /// correct sp
+    if(!stk->wow64)
+    {
+        sp = sp + ((sizeof(void*) - (sp & (sizeof(void*) - 1))) & ((!(sp & (sizeof(void*) - 1))) - 1));
+    }
+    else
+    {
+        sp = sp + ((sizeof(int) - (sp & (sizeof(int) - 1))) & ((!(sp & (sizeof(int) - 1))) - 1));
+    }
+
+    for(i = 0; i < count; i++)
+    {
+        touch_nmi_watchdog();
+        /// check the border
+        if((size_t)(compressed - stk->compressed) >= stksize)
+        {
+            return 0;
+        }
+
+        /// [[prefix][stack pointer][prefix][value]]...
+        ///  [prefix bits: |7: frame(1)/code(0)|6: sign|5:|4:|3-0: bytes per value]
+        ///  [prefix bits for sp: |7: (1) - idicates value is encoded in prefix|6-0: scaled sp value]
+        ///                           (0) - |6:|5:|4:|3-0: bytes per value]
+        ///  [prefix bits for fp: |7: frame(1)/code(0)|6: sign|5: in-prefix (0) - |4:|3-0: bytes per value]
+        ///                                                                 (1) - |4-0: scaled fp value]
+        ///  [value: difference from previous value of the same type]
+
+        /// compress a stack pointer
+        value = map[i] - sp;    /// assert(value >= 0);
+        value >>= 2;            /// scale the stack pointer down by 4
+
+        if(value < 0x80)
+        {
+            prefix = 0x80 | (int)value;
+            *compressed++ = (unsigned char)prefix;
+        }
+        else
+        {
+
+            for(j = sizeof(size_t) - 1; j >= 0; j--)
+            {
+                if(value & (((size_t)0xff) << (j << 3)))
+                {
+                    break;
+                }
+            }
+            prefix = j + 1;
+            *compressed++ = (unsigned char)prefix;
+
+            for(; j >= 0; j--)
+            {
+                *compressed++ = (unsigned char)(value & 0xff);
+                value >>= 8;
+            }
+        }
+        sp = map[i++];
+
+        /// test & compress a value
+        value = map[i];
+
+        if(value >= sp && value < base)
+        {
+            offset = fp;
+            fp = value;
+            value -= offset;
+            tmp = (value & (((size_t)1) << ((sizeof(size_t) << 3) - 1)));
+            value = (value >> 2) | tmp | (tmp >> 1);
+
+            if(value < 0x20)
+            {
+                prefix = 0xa0 | (int)value;
+                *compressed++ = (unsigned char)prefix;
+                continue;
+            }
+            else if(value > (size_t)-32)
+            {
+                prefix = 0xe0 | (int)(value & 0xff);
+                *compressed++ = (unsigned char)prefix;
+                continue;
+            }
+            prefix = 0x80;
+        }
+        else
+        {
+            offset = ip;
+            ip = value;
+            prefix = 0;
+            value -= offset;
+        }
+        sign = (value & (((size_t)1) << ((sizeof(size_t) << 3) - 1))) ? 0xff : 0;
+
+        for(j = sizeof(size_t) - 1; j >= 0; j--)
+        {
+            if(((value >> (j << 3)) & 0xff) != sign)
+            {
+                break;
+            }
+        }
+        prefix |= sign ? 0x40 : 0;
+        prefix |= j + 1;
+        *compressed++ = (unsigned char)prefix;
+
+        for(; j >= 0; j--)
+        {
+            *compressed++ = (unsigned char)(value & 0xff);
+            value >>= 8;
+        }
+    }
+    return (int)(compressed - stk->compressed);
+}
+
+/// setup current stack parameters
+static void setup_stack(stack_control_t* stk, user_vm_accessor_t* acc,
+                        void* ip, void* sp, void* bp, void* fp, int wow64)
+{
+    stk->acc    = acc;
+    stk->user_ip.vdp = ip;
+    stk->user_sp.vdp = sp;
+    stk->bp.vdp = bp;
+    stk->user_fp.vdp = fp;
+    stk->wow64  = wow64;
+}
+
+/// get a pointer to the compressed stack data
+static char* data_stack(stack_control_t* stk)
+{
+    return stk->compressed;
+}
+
+static void lock_stack(stack_control_t* stk)
+{
+    vtss_spin_lock(&stk->spin_lock);
+}
+
+static int trylock_stack(stack_control_t* stk)
+{
+    if (!stk->acc) return 0;
+    return vtss_spin_trylock(&stk->spin_lock);
+}
+
+static void unlock_stack(stack_control_t* stk)
+{
+    vtss_spin_unlock(&stk->spin_lock);
+}
diff --git a/drivers/misc/intel/sepdk/vtsspp/user_vm.c b/drivers/misc/intel/sepdk/vtsspp/user_vm.c
new file mode 100644
index 000000000000..d8f3480a2092
--- /dev/null
+++ b/drivers/misc/intel/sepdk/vtsspp/user_vm.c
@@ -0,0 +1,736 @@
+/*
+  Copyright (C) 2010-2015 Intel Corporation.  All Rights Reserved.
+
+  This file is part of SEP Development Kit
+
+  SEP Development Kit is free software; you can redistribute it
+  and/or modify it under the terms of the GNU General Public License
+  version 2 as published by the Free Software Foundation.
+
+  SEP Development Kit is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with SEP Development Kit; if not, write to the Free Software
+  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA
+
+  As a special exception, you may use this file as part of a free software
+  library without restriction.  Specifically, if other files instantiate
+  templates or use macros or inline functions from this file, or you compile
+  this file and link it with other files to produce an executable, this
+  file does not by itself cause the resulting executable to be covered by
+  the GNU General Public License.  This exception does not however
+  invalidate any other reasons why the executable file might be covered by
+  the GNU General Public License.
+*/
+#include "vtss_config.h"
+
+#ifndef VTSS_VMA_TIME_LIMIT
+#define VTSS_VMA_TIME_LIMIT (tsc_khz * 30ULL)
+#endif
+
+#include "user_vm.h"
+#include "memory_pool.h"
+
+#include <linux/slab.h>
+#include <linux/vmstat.h>
+#include <linux/highmem.h>      /* for kmap()/kunmap() */
+#include <linux/pagemap.h>      /* for page_cache_release() */
+#include <linux/kallsyms.h>
+#include <linux/kprobes.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,11,0)
+#include <linux/sched/task.h>
+#include <linux/sched/mm.h>
+#include <linux/sched/signal.h>
+#endif
+#include <asm/fixmap.h>         /* VSYSCALL_START */
+#include <asm/page.h>
+#include <asm/cacheflush.h>
+#include <asm/uaccess.h>
+
+
+/*Virtual memory map with 4 level page tables:
+
+0000000000000000 - 00007fffffffffff (=47 bits) user space, different per mm
+hole caused by [48:63] sign extension
+ffff800000000000 - ffff80ffffffffff (=40 bits) guard hole
+ffff880000000000 - ffffc7ffffffffff (=64 TB) direct mapping of all phys. memory
+ffffc80000000000 - ffffc8ffffffffff (=40 bits) hole
+ffffc90000000000 - ffffe8ffffffffff (=45 bits) vmalloc/ioremap space
+ffffe90000000000 - ffffe9ffffffffff (=40 bits) hole
+ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)
+... unused hole ...
+ffffffff80000000 - ffffffffa0000000 (=512 MB)  kernel text mapping, from phys 0
+ffffffffa0000000 - fffffffffff00000 (=1536 MB) module mapping space
+
+The direct mapping covers all memory in the system up to the highest
+memory address (this means in some cases it can also include PCI memory
+holes).
+
+vmalloc space is lazily synchronized into the different PML4 pages of
+the processes using the page fault handler, with init_level4_pgt as
+reference.
+
+Current X86-64 implementations only support 40 bits of address space,
+but we support up to 46 bits. This expands into MBZ space in the page tables.
+
+-Andi Kleen, Jul 2004
+
+
+#ifdef VTSS_VMA_SEARCH_BOOST
+#define VTSS_USER_SPACE_HIGH 0x7fffffffffff
+#endif
+*/
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+
+typedef int (gup_huge_pmd_t) (pmd_t pmd, unsigned long addr, unsigned long end, int write, struct page **pages, int *nr);
+static gup_huge_pmd_t* gup_huge_pmd = NULL;
+
+typedef int (gup_huge_pud_t) (pud_t pud, unsigned long addr, unsigned long end, int write, struct page **pages, int *nr);
+static gup_huge_pud_t* gup_huge_pud = NULL;
+
+typedef int (gup_pte_range_t) (pmd_t pmd, unsigned long addr, unsigned long end, int write, struct page **pages, int *nr);
+static gup_pte_range_t* gup_pte_range = NULL;
+
+static struct kprobe _kp_gup_huge_pmd = {
+    .pre_handler = NULL,
+    .post_handler = NULL,
+    .fault_handler = NULL,
+#ifdef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+    .symbol_name = "gup_huge_pmd",
+#endif
+    .addr = (kprobe_opcode_t*)NULL
+};
+
+static struct kprobe _kp_gup_huge_pud = {
+    .pre_handler = NULL,
+    .post_handler = NULL,
+    .fault_handler = NULL,
+#ifdef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+    .symbol_name = "gup_huge_pud",
+#endif
+    .addr = (kprobe_opcode_t*)NULL
+};
+
+static struct kprobe _kp_gup_pte_range = {
+    .pre_handler = NULL,
+    .post_handler = NULL,
+    .fault_handler = NULL,
+#ifdef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+    .symbol_name = "gup_pte_range",
+#endif
+    .addr = (kprobe_opcode_t*)NULL
+};
+
+static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end, int write, struct page **pages, int *nr)
+{
+    unsigned long next;
+    pmd_t *pmdp;
+
+    pmdp = pmd_offset(&pud, addr);
+    do {
+        pmd_t pmd = *pmdp;
+
+        next = pmd_addr_end(addr, end);
+        if (pmd_none(pmd))
+            return 0;
+        if (unlikely(pmd_large(pmd))) {
+            if (!gup_huge_pmd(pmd, addr, next, write, pages, nr))
+                return 0;
+        } else {
+            if (!gup_pte_range(pmd, addr, next, write, pages, nr))
+                return 0;
+        }
+    } while (pmdp++, addr = next, addr != end);
+
+    return 1;
+}
+
+static int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end, int write, struct page **pages, int *nr)
+{
+    unsigned long next;
+    pud_t *pudp;
+
+    pudp = pud_offset(&pgd, addr);
+    do {
+        pud_t pud = *pudp;
+
+        next = pud_addr_end(addr, end);
+        if (pud_none(pud))
+            return 0;
+        if (unlikely(pud_large(pud))) {
+            if (!gup_huge_pud(pud, addr, next, write, pages, nr))
+                return 0;
+        } else {
+            if (!gup_pmd_range(pud, addr, next, write, pages, nr))
+                return 0;
+        }
+    } while (pudp++, addr = next, addr != end);
+
+    return 1;
+}
+
+int vtss_get_user_pages_fast(unsigned long start, int nr_pages, int write, struct page **pages)
+{
+    struct mm_struct *mm = current->mm;
+    unsigned long addr, len, end;
+    unsigned long next;
+    unsigned long flags;
+    pgd_t *pgdp;
+    int nr = 0;
+
+    start &= PAGE_MASK;
+    addr = start;
+    len = (unsigned long) nr_pages << PAGE_SHIFT;
+    end = start + len;
+    if (!access_ok(write ? VERIFY_WRITE : VERIFY_READ, (void __user *)start, len))
+        return 0;
+
+    local_irq_save(flags);
+    pgdp = pgd_offset(mm, addr);
+    do {
+        pgd_t pgd = *pgdp;
+
+        next = pgd_addr_end(addr, end);
+        if (pgd_none(pgd))
+            break;
+        if (!gup_pud_range(pgd, addr, next, write, pages, &nr))
+            break;
+    } while (pgdp++, addr = next, addr != end);
+    local_irq_restore(flags);
+
+    return nr;
+}
+
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+
+typedef int (vtss_get_user_pages_fast_t) (unsigned long start, int nr_pages, int write, struct page **pages);
+static vtss_get_user_pages_fast_t* vtss_get_user_pages_fast = NULL;
+
+static struct kprobe _kp_dummy = {
+    .pre_handler = NULL,
+    .post_handler = NULL,
+    .fault_handler = NULL,
+#ifdef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+    .symbol_name = "__get_user_pages_fast",
+#endif
+    .addr = (kprobe_opcode_t*)NULL
+};
+
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,38) */
+#define vtss_get_user_pages_fast __get_user_pages_fast
+#endif
+
+extern atomic_t vtss_mmap_reg_callcnt;
+
+#ifndef VTSS_AUTOCONF_KMAP_ATOMIC_ONE_ARG
+#ifndef KM_NMI
+#define KM_NMI KM_IRQ0
+#endif
+
+#ifndef in_nmi
+static inline int in_nmi(void) __attribute__ ((always_inline));
+static inline int in_nmi(void)
+{
+    return 0;
+}
+#endif /* in_nmi */
+#endif /* VTSS_AUTOCONF_KMAP_ATOMIC_ONE_ARG */
+
+static int vtss_user_vm_page_unpin(struct user_vm_accessor* this)
+{
+    if (this->m_irq) {
+        if (this->m_maddr != NULL)
+#ifdef VTSS_AUTOCONF_KMAP_ATOMIC_ONE_ARG
+            kunmap_atomic(this->m_maddr);
+#else
+            kunmap_atomic(this->m_maddr, in_nmi() ? KM_NMI : KM_IRQ0);
+#endif
+        this->m_maddr = NULL;
+        if (this->m_page != NULL)
+            put_page(this->m_page);
+        this->m_page = NULL;
+    } else {
+        if (this->m_maddr != NULL)
+            kunmap(this->m_maddr);
+        this->m_maddr = NULL;
+        if (this->m_page != NULL)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,6,0)
+            /* since 3.8 page_cache_release is an alias to put_page */ 
+            page_cache_release(this->m_page);
+#else
+            /* since 4.6 page_cache_release removed from the kernel */
+            put_page(this->m_page);
+#endif
+        this->m_page = NULL;
+    }
+    this->m_page_id = (unsigned long)-1;
+    return 0;
+}
+
+#ifdef VTSS_AUTOCONF_GET_USER_PAGES_TSK_MM_ARGS
+#define vtss_get_user_pages(task, mm, start, nr_pages, write, force, pages, vmas)\
+        get_user_pages(task, mm, start, nr_pages, write, force, pages, vmas)
+#elif defined(VTSS_AUTOCONF_GET_USER_PAGES_TWO_FLAGS)
+#define vtss_get_user_pages(task, mm, start, nr_pages, write, force, pages, vmas)\
+        get_user_pages(start, nr_pages, write, force, pages, vmas)
+#else
+#define vtss_get_user_pages(task, mm, start, nr_pages, write, force, pages, vmas)\
+        get_user_pages(start, nr_pages, (write ? FOLL_WRITE : 0) | (force ? FOLL_FORCE : 0), pages, vmas)
+#endif
+
+static int vtss_user_vm_page_pin(struct user_vm_accessor* this, unsigned long addr)
+{
+    int rc;
+
+    if (this->m_irq) {
+        rc = vtss_get_user_pages_fast(addr, 1, 0, &this->m_page);
+        if (rc != 1) {
+            this->m_page  = NULL;
+            this->m_maddr = NULL;
+            this->m_page_id = (unsigned long)-1;
+            return 1;
+        }
+#ifdef VTSS_AUTOCONF_KMAP_ATOMIC_ONE_ARG
+        this->m_maddr = kmap_atomic(this->m_page);
+#else
+        this->m_maddr = kmap_atomic(this->m_page, in_nmi() ? KM_NMI : KM_IRQ0);
+#endif
+    } else {
+        rc = vtss_get_user_pages(this->m_task, this->m_mm, addr, 1, 0, 1, &this->m_page, &this->m_vma);
+        if (rc != 1) {
+            this->m_page  = NULL;
+            this->m_maddr = NULL;
+            this->m_page_id = (unsigned long)-1;
+            return 1;
+        }
+        this->m_maddr = kmap(this->m_page);
+    }
+    rc = (this->m_maddr != NULL) ? 0 : 2;
+#ifdef VTSS_VMA_CACHE
+    if (!rc) {
+        if (this->m_irq) {
+            mm_segment_t old_fs = get_fs();
+            set_fs(KERNEL_DS);
+            pagefault_disable();
+            VTSS_PROFILE(cpy, rc = __copy_from_user_inatomic(this->m_buffer, this->m_maddr, PAGE_SIZE));
+            pagefault_enable();
+            set_fs(old_fs);
+        } else {
+            VTSS_PROFILE(cpy, copy_from_user_page(this->m_vma, this->m_page, addr & PAGE_MASK, this->m_buffer, this->m_maddr, PAGE_SIZE));
+        }
+        vtss_user_vm_page_unpin(this);
+    }
+#endif
+    this->m_page_id = rc ? (unsigned long)-1 : ((addr & PAGE_MASK) >> PAGE_SHIFT);
+    return rc;
+}
+
+
+#ifdef VTSS_VMA_SEARCH_BOOST
+static void vtss_vma_cache_init(struct user_vm_accessor* this)
+{
+    struct vm_area_struct* vma;
+    int callcnt = atomic_read(&vtss_mmap_reg_callcnt);
+    int is_vdso_found = 0;
+
+    if (!this) return;
+    if (this->mmap_reg_callcnt >= callcnt) return;
+    this->mmap_reg_callcnt = callcnt;
+
+    this->mmap_vdso_start = 0;
+    this->mmap_vdso_end = 0;
+    this->mmap_mms_start = 0;
+    this->mmap_mms_end = 0;
+    this->mmap_stack_start = 0;
+    this->mmap_stack_end = 0;
+
+    if (unlikely(!this->m_mm)){
+         return;
+    }
+/*
+    this->mmap_code_start_addr = this->m_mm->start_code;
+    this->mmap_code_end_addr = this->m_mm->end_code;
+    this->mmap_heap_start_addr = this->m_mm->start_brk;
+    this->mmap_heap_end_addr = this->m_mm->brk;
+
+    //////////////////////////////
+    this->mmap_mms_addr = this->m_mm->brk;
+    this->mmap_vdso_addr = (unsigned long)this->m_mm->context.vdso; //VTSS_USER_SPACE_HIGH;
+    this->mmap_end_addr = (unsigned long)this->m_mm->context.vdso+PAGE_SIZE; //VTSS_USER_SPACE_HIGH;
+
+    vma = find_vma(this->m_mm, (unsigned long)this->m_mm->context.vdso);
+    if (vma){
+        this->mmap_end_addr = vma->vm_end;
+    }
+
+    return;
+*/
+    for (vma = this->m_mm->mmap; vma != NULL; vma = vma->vm_next) {
+            if (vma->vm_mm && vma->vm_start == (long)vma->vm_mm->context.vdso) {
+                is_vdso_found = 1;
+                this->mmap_vdso_start = vma->vm_start;
+                this->mmap_vdso_end = vma->vm_end;
+            } else if (vma->vm_flags & VM_EXEC) {
+                    if (vma->vm_start <= this->m_mm->start_stack && this->m_mm->start_stack < vma->vm_end){
+                        this->mmap_stack_start = vma->vm_start;
+                        this->mmap_stack_end = vma->vm_end;
+                    } else if (vma->vm_start !=this->m_mm->start_code){
+                        if (this->mmap_mms_start > vma->vm_start || this->mmap_mms_start == 0) this->mmap_mms_start = vma->vm_start;
+                        if (this->mmap_mms_end < vma->vm_end) this->mmap_mms_end = vma->vm_end;
+                    }
+            }
+    }
+    if (!is_vdso_found && this->m_mm->context.vdso) {
+        this->mmap_vdso_start = (unsigned long)this->m_mm->context.vdso;
+        this->mmap_vdso_end = (unsigned long)this->m_mm->context.vdso + PAGE_SIZE;
+    }
+
+}
+#endif
+
+static int vtss_user_vm_unlock(struct user_vm_accessor* this)
+{
+    this->m_vma_cache = NULL;
+//    this->m_mm->mmap_cache = this->m_vma_cache; //restore cache
+    vtss_user_vm_page_unpin(this);
+    if (this->m_mm != NULL) {
+        if (!this->m_irq) {
+            up_read(&this->m_mm->mmap_sem);
+            mmput(this->m_mm);
+        }
+        this->m_mm = NULL;
+    }
+    this->m_task = NULL;
+#ifdef VTSS_VMA_TIME_LIMIT
+    this->m_time = 0;
+#endif
+    return 0;
+}
+
+static int vtss_user_vm_trylock(struct user_vm_accessor* this, struct task_struct* task)
+{
+    this->m_task = NULL;
+    this->m_mm   = NULL;
+
+    if (task == NULL || task->mm == NULL)
+        return 1;
+
+    if (!this->m_irq) {
+        struct mm_struct* mm = get_task_mm(task);
+        if (!mm) return 2;
+        if (!down_read_trylock(&mm->mmap_sem)) {
+            mmput(mm);
+            return 2;
+        }
+        this->m_mm = mm;
+    } else {
+        this->m_mm = task->mm;
+    }
+    this->m_task = task;
+#ifdef VTSS_VMA_TIME_LIMIT
+    this->m_time = get_cycles();
+#endif
+    //remove cashed vma addresses.
+#ifdef VTSS_VMA_SEARCH_BOOST
+    vtss_vma_cache_init(this);
+#endif
+    this->m_vma_cache = NULL;
+//    this->m_vma_cache = this->m_mm->mmap_cache; //save cache
+    return 0;
+}
+
+static size_t vtss_user_vm_read(struct user_vm_accessor* this, void* from, void* to, size_t size)
+{
+    size_t cpsize, bytes = 0;
+    unsigned long offset, addr = (unsigned long)from;
+#ifdef VTSS_DEBUG_PROFILE
+    cycles_t start_vma_time = get_cycles();
+#endif
+#ifndef VTSS_VMA_CACHE
+    mm_segment_t old_fs = get_fs();
+
+    if (this->m_irq) {
+        set_fs(KERNEL_DS);
+        pagefault_disable();
+    }
+#endif
+
+    do {
+#ifdef VTSS_VMA_TIME_LIMIT
+        cycles_t access_time = get_cycles();
+#endif
+        unsigned long page_id = (addr & PAGE_MASK) >> PAGE_SHIFT;
+
+        offset = addr & (PAGE_SIZE - 1);
+        cpsize = min((size_t)(PAGE_SIZE - offset), size - bytes);
+        TRACE("addr=0x%p(0x%lx) size=%zu (page=0x%lx, offset=0x%lx)",
+                from, addr, cpsize, page_id, offset);
+#ifdef VTSS_VMA_TIME_LIMIT
+        if ((access_time - this->m_time) > this->m_limit) {
+            TRACE("addr=0x%p(0x%lx) size=%zu (page=0x%lx, offset=0x%lx), time=%llu",
+                    from, addr, cpsize, page_id, offset, (access_time - this->m_time));
+            break; /* Time is over */
+        }
+#endif
+        if (!access_ok(VERIFY_READ, addr, cpsize))
+            break; /* Don't have a read access */
+        if (page_id != this->m_page_id) {
+#ifdef VTSS_DEBUG_PROFILE
+            cycles_t start_pgp_time = get_cycles();
+#endif
+            TRACE("not in cache 0x%lx", this->m_page_id);
+            vtss_user_vm_page_unpin(this);
+            if (vtss_user_vm_page_pin(this, addr)) {
+                vtss_user_vm_page_unpin(this);
+#ifdef VTSS_DEBUG_PROFILE
+                vtss_profile_cnt_pgp++;
+                vtss_profile_clk_pgp += get_cycles() - start_pgp_time;
+#endif
+                TRACE("page lock FAIL");
+                break; /* Cannot get a page for an access */
+            }
+#ifdef VTSS_DEBUG_PROFILE
+            vtss_profile_cnt_pgp++;
+            vtss_profile_clk_pgp += get_cycles() - start_pgp_time;
+#endif
+        }
+#ifdef VTSS_VMA_CACHE
+        memcpy(to, this->m_buffer + offset, cpsize);
+#else
+        if (this->m_irq) {
+            long rc = 0;
+            VTSS_PROFILE(cpy, rc = __copy_from_user_inatomic(to, this->m_maddr + offset, cpsize));
+            if (rc)
+                break;
+        } else {
+            VTSS_PROFILE(cpy, copy_from_user_page(this->m_vma, this->m_page, addr, to, this->m_maddr + offset, cpsize));
+        }
+#endif
+        bytes += cpsize;
+        to    += cpsize;
+        addr  += cpsize;
+    } while (bytes < size);
+
+#ifndef VTSS_VMA_CACHE
+    if (this->m_irq) {
+        pagefault_enable();
+        set_fs(old_fs);
+    }
+#endif
+#ifdef VTSS_DEBUG_PROFILE
+    vtss_profile_cnt_vma++;
+    vtss_profile_clk_vma += get_cycles() - start_vma_time;
+#endif
+    return bytes;
+}
+
+
+/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */
+/*struct vm_area_struct *vtss_find_vma(struct mm_struct *mm, unsigned long addr, struct user_vm_accessor* this)
+{
+    struct vm_area_struct* vma = (this)? this->m_vma_cache : NULL;
+    if (!(vma && vma->vm_end > addr && vma->vm_start <= addr)) {
+        struct rb_node *rb_node;
+        rb_node = mm->mm_rb.rb_node;
+        vma = NULL;
+        while (rb_node) {
+            struct vm_area_struct *vma_tmp;
+            vma_tmp = rb_entry(rb_node, struct vm_area_struct, vm_rb);
+            if (vma_tmp->vm_end > addr) {
+                vma = vma_tmp;
+                if (vma_tmp->vm_start <= addr) break;
+                rb_node = rb_node->rb_left;
+            } else
+            rb_node = rb_node->rb_right;
+        }
+        if (vma) this->m_vma_cache = vma;
+     }
+     return vma;
+}
+*/
+
+#ifdef VTSS_VMA_SEARCH_BOOST
+static int vtss_user_vm_validate(struct user_vm_accessor* this, unsigned long ip)
+{
+#ifdef CONFIG_X86_64
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,16,0)
+    if ((ip >= VSYSCALL_START) && (ip < VSYSCALL_END))
+#else
+    if ((ip & PAGE_MASK) == VSYSCALL_ADDR)
+#endif
+        return 1; /* [vsyscall] */
+    else
+#endif
+
+    if (ip < VTSS_KSTART) {
+        int st = 0;
+        struct vm_area_struct* vma = NULL;
+
+        if (!this || !this->m_mm) return 0;
+
+//        if (this->mmap_stack_start <= ip && ip < this->m_mm->start_stack) return 0; //not used stack area
+//        if (this->m_mm->start_stack <= ip && ip < this->mmap_stack_end) return 1;
+
+        if ((this->m_mm->start_code <= ip  && ip < this->m_mm->end_code) ||
+            (this->mmap_vdso_start <= ip && ip < this->mmap_vdso_end) ||
+//            (this->m_mm->start_stack <= ip && ip < this->mmap_stack_end)  ||
+            (this->mmap_stack_start <= ip && ip < this->mmap_stack_end)  ||
+            (this->mmap_mms_start <= ip && ip < this->mmap_mms_end)){
+            st = 1;
+        } else if (this->m_mm->start_brk <= ip && ip < this->m_mm->brk)/*for java it can be code*/{
+            //java functions can be called without call.
+            st = 2;
+        } else {
+            return 0;
+        }
+
+        vma = find_vma(this->m_mm, ip);
+        if (vma == NULL){
+            return 0;
+        }
+        if (!((vma->vm_flags & VM_EXEC) || (vma->vm_flags & VM_MAYEXEC))) {
+            return 0;
+        }
+        if ((ip >= vma->vm_start) && (ip < vma->vm_end)) {
+            if (st == 1)
+            {
+                ; //TODO: check the code
+            }
+            return 1;
+        }
+        return 0;
+    } else
+        return (ip < PAGE_OFFSET) ? 1 : 0; /* in kernel? */
+}
+#else
+static int vtss_user_vm_validate(struct user_vm_accessor* this, unsigned long ip)
+{
+#ifdef CONFIG_X86_64
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,16,0)
+    if ((ip >= VSYSCALL_START) && (ip < VSYSCALL_END))
+#else
+    if ((ip & PAGE_MASK) == VSYSCALL_ADDR)
+#endif
+        return 1; /* [vsyscall] */
+    else
+#endif
+    if (ip < VTSS_KSTART) {
+        struct vm_area_struct* vma = this->m_mm ? find_vma(this->m_mm, ip) : NULL;
+        return ((vma != NULL) && ((vma->vm_flags & VM_EXEC) || (vma->vm_flags & VM_MAYEXEC)) && (ip >= vma->vm_start) && (ip < vma->vm_end)) ? 1 : 0;
+    } else
+        return (ip < PAGE_OFFSET) ? 1 : 0; /* in kernel? */
+}
+#endif
+
+user_vm_accessor_t* vtss_user_vm_accessor_init(int in_irq, cycles_t limit)
+{
+    user_vm_accessor_t* acc;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+    if (gup_huge_pmd == NULL || gup_huge_pud == NULL || gup_pte_range == NULL)
+        return NULL;
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    if (vtss_get_user_pages_fast == NULL)
+        return NULL;
+#endif
+    acc = (user_vm_accessor_t*)vtss_kmalloc(sizeof(user_vm_accessor_t), in_irq ? GFP_ATOMIC : GFP_KERNEL);
+    if (acc != NULL) {
+        memset(acc, 0, sizeof(user_vm_accessor_t));
+        acc->m_page_id = (unsigned long)-1;
+        acc->m_irq     = in_irq;
+#ifdef VTSS_VMA_TIME_LIMIT
+        acc->m_limit   = limit ? limit : VTSS_VMA_TIME_LIMIT;
+#else
+        acc->m_limit   = limit;
+#endif
+        acc->mmap_reg_callcnt = 0;
+
+        acc->mmap_vdso_start = 0;
+        acc->mmap_vdso_end = 0;
+        acc->mmap_mms_start = 0;
+        acc->mmap_mms_end = 0;
+        acc->mmap_stack_start = 0;
+        acc->mmap_stack_end = 0;
+
+        acc->trylock   = vtss_user_vm_trylock;
+        acc->unlock    = vtss_user_vm_unlock;
+        acc->read      = vtss_user_vm_read;
+        acc->validate  = vtss_user_vm_validate;
+    } else {
+        ERROR("No memory for accessor");
+    }
+    return acc;
+}
+
+void vtss_user_vm_accessor_fini(user_vm_accessor_t* acc)
+{
+    if (acc != NULL) {
+//        acc->unlock(acc);
+        vtss_kfree(acc);
+    }
+}
+
+int vtss_user_vm_init(void)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+    if (gup_huge_pmd == NULL || gup_huge_pud == NULL || gup_pte_range == NULL) {
+#ifndef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+        gup_huge_pmd  =  (gup_huge_pmd_t*)kallsyms_lookup_name("gup_huge_pmd");
+        gup_huge_pud  =  (gup_huge_pud_t*)kallsyms_lookup_name("gup_huge_pud");
+        gup_pte_range = (gup_pte_range_t*)kallsyms_lookup_name("gup_pte_range");
+#else  /* VTSS_AUTOCONF_KPROBE_SYMBOL_NAME */
+        if (!register_kprobe(&_kp_gup_huge_pmd)) {
+            gup_huge_pmd = (gup_huge_pmd_t*)_kp_gup_huge_pmd.addr;
+            TRACE("gup_huge_pmd=0x%p", gup_huge_pmd);
+            unregister_kprobe(&_kp_gup_huge_pmd);
+        }
+        if (!register_kprobe(&_kp_gup_huge_pud)) {
+            gup_huge_pud = (gup_huge_pud_t*)_kp_gup_huge_pud.addr;
+            TRACE("gup_huge_pud=0x%p", gup_huge_pud);
+            unregister_kprobe(&_kp_gup_huge_pud);
+        }
+        if (!register_kprobe(&_kp_gup_pte_range)) {
+            gup_pte_range = (gup_pte_range_t*)_kp_gup_pte_range.addr;
+            TRACE("gup_pte_range=0x%p", gup_pte_range);
+            unregister_kprobe(&_kp_gup_pte_range);
+        }
+#endif /* VTSS_AUTOCONF_KPROBE_SYMBOL_NAME */
+        if (gup_huge_pmd == NULL) {
+            ERROR("Cannot find 'gup_huge_pmd' symbol");
+        }
+        if (gup_huge_pud == NULL) {
+            ERROR("Cannot find 'gup_huge_pud' symbol");
+        }
+        if (gup_pte_range == NULL) {
+            ERROR("Cannot find 'gup_pte_range' symbol");
+        }
+        if (gup_huge_pmd == NULL || gup_huge_pud == NULL || gup_pte_range == NULL) {
+            return -1;
+        }
+    }
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    if (vtss_get_user_pages_fast == NULL) {
+#ifndef VTSS_AUTOCONF_KPROBE_SYMBOL_NAME
+        vtss_get_user_pages_fast = (vtss_get_user_pages_fast_t*)kallsyms_lookup_name("__get_user_pages_fast");
+#else  /* VTSS_AUTOCONF_KPROBE_SYMBOL_NAME */
+        if (!register_kprobe(&_kp_dummy)) {
+            vtss_get_user_pages_fast = (vtss_get_user_pages_fast_t*)_kp_dummy.addr;
+            TRACE("__get_user_pages_fast=0x%p", vtss_get_user_pages_fast);
+            unregister_kprobe(&_kp_dummy);
+        }
+#endif /* VTSS_AUTOCONF_KPROBE_SYMBOL_NAME */
+        if (vtss_get_user_pages_fast == NULL) {
+            ERROR("Cannot find '__get_user_pages_fast' symbol");
+            return -1;
+        }
+    }
+#endif
+    return 0;
+}
+
+void vtss_user_vm_fini(void)
+{
+}
-- 
2.18.0

