From 4f9ba42b60b234e04982d1d6f1bb653ed67580bd Mon Sep 17 00:00:00 2001
From: Miguel Bernal Marin <miguel.bernal.marin@linux.intel.com>
Date: Wed, 29 Aug 2018 06:30:46 -0500
Subject: [PATCH 8015/8021] misc: intel: socwatch: add C source files

---
 drivers/misc/intel/socwatch/sw_collector.c    |  642 ++++++
 drivers/misc/intel/socwatch/sw_driver.c       | 1266 +++++++++++
 drivers/misc/intel/socwatch/sw_file_ops.c     |  332 +++
 drivers/misc/intel/socwatch/sw_hardware_io.c  |  176 ++
 drivers/misc/intel/socwatch/sw_internal.c     |  225 ++
 drivers/misc/intel/socwatch/sw_mem.c          |  299 +++
 drivers/misc/intel/socwatch/sw_ops_provider.c |  886 ++++++++
 .../misc/intel/socwatch/sw_output_buffer.c    |  533 +++++
 drivers/misc/intel/socwatch/sw_reader.c       |  159 ++
 drivers/misc/intel/socwatch/sw_telem.c        |  488 +++++
 .../socwatch/sw_trace_notifier_provider.c     | 1876 +++++++++++++++++
 .../intel/socwatch/sw_tracepoint_handlers.c   |  358 ++++
 12 files changed, 7240 insertions(+)
 create mode 100644 drivers/misc/intel/socwatch/sw_collector.c
 create mode 100644 drivers/misc/intel/socwatch/sw_driver.c
 create mode 100644 drivers/misc/intel/socwatch/sw_file_ops.c
 create mode 100644 drivers/misc/intel/socwatch/sw_hardware_io.c
 create mode 100644 drivers/misc/intel/socwatch/sw_internal.c
 create mode 100644 drivers/misc/intel/socwatch/sw_mem.c
 create mode 100644 drivers/misc/intel/socwatch/sw_ops_provider.c
 create mode 100644 drivers/misc/intel/socwatch/sw_output_buffer.c
 create mode 100644 drivers/misc/intel/socwatch/sw_reader.c
 create mode 100644 drivers/misc/intel/socwatch/sw_telem.c
 create mode 100644 drivers/misc/intel/socwatch/sw_trace_notifier_provider.c
 create mode 100644 drivers/misc/intel/socwatch/sw_tracepoint_handlers.c

diff --git a/drivers/misc/intel/socwatch/sw_collector.c b/drivers/misc/intel/socwatch/sw_collector.c
new file mode 100644
index 000000000000..62ffa92d91f6
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_collector.c
@@ -0,0 +1,642 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include "sw_internal.h"
+#include "sw_structs.h"
+#include "sw_collector.h"
+#include "sw_defines.h"
+#include "sw_mem.h"
+#include "sw_types.h"
+#include "sw_hardware_io.h"
+#include "sw_output_buffer.h"
+
+/* -------------------------------------------------
+ * Local function declarations.
+ * -------------------------------------------------
+ */
+void sw_free_driver_interface_info_i(struct sw_driver_interface_info *info);
+const struct sw_hw_ops **sw_alloc_ops_i(pw_u16_t num_io_descriptors);
+void sw_free_ops_i(const struct sw_hw_ops **ops);
+struct sw_driver_interface_info *sw_copy_driver_interface_info_i(
+      const struct sw_driver_interface_info *info);
+int sw_init_driver_interface_info_i(struct sw_driver_interface_info *info);
+int sw_reset_driver_interface_info_i(struct sw_driver_interface_info *info);
+int sw_init_ops_i(const struct sw_hw_ops **ops, const struct sw_driver_interface_info *info);
+sw_driver_msg_t *sw_alloc_collector_msg_i(const struct sw_driver_interface_info *info, size_t per_msg_payload_size);
+void sw_free_collector_msg_i(sw_driver_msg_t *msg);
+size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info);
+void sw_handle_per_cpu_msg_i(void *info, enum sw_wakeup_action action);
+/* -------------------------------------------------
+ * Variables.
+ * -------------------------------------------------
+ */
+const static struct sw_hw_ops *s_hw_ops = NULL;
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+/*
+ * Driver interface info functions.
+ */
+
+/**
+ * sw_add_driver_info() - Add a collector node to the list called at this
+ *                      "when type".
+ * @head:   The collector node list to add the new node to.
+ * @info:   Driver information to add to the list.
+ *
+ *  This function allocates and links in a "collector node" for each
+ *  collector based on the collector info in the info parameter.
+ *  The function allocates the new node, and links it to a local copy
+ *  of the passed-in driver interface info.  If the collector has an
+ *  init function among its operations, it iterates through the
+ *  descriptors in info, passing each one to the init function.
+ *
+ *  Finally, it allocates and initializes the "collector message" which
+ *  buffers a data sample that this collector gathers during the run.
+ *
+ * Returns:  -PW_ERROR on failure, PW_SUCCESS on success.
+ */
+int sw_add_driver_info(void *list_head,
+                       const struct sw_driver_interface_info *info)
+{
+    SW_LIST_HEAD_VAR(sw_collector_data) *head = list_head;
+    struct sw_collector_data *node = sw_alloc_collector_node();
+    if (!node) {
+        pw_pr_error("ERROR allocating collector node!\n");
+        return -PW_ERROR;
+    }
+
+    node->info = sw_copy_driver_interface_info_i(info);
+    if (!node->info) {
+        pw_pr_error("ERROR allocating or copying driver_interface_info!\n");
+        sw_free_collector_node(node);
+        return -PW_ERROR;
+    }
+    /*
+     * Initialize the collectors in the node's descriptors.
+     */
+    if (sw_init_driver_interface_info_i(node->info)) {
+        pw_pr_error("ERROR initializing a driver_interface_info node!\n");
+        sw_free_collector_node(node);
+        return -PW_ERROR;
+    }
+    /*
+     * Allocate the ops array. We do this one time as an optimization
+     * (we could always just repeatedly call 'sw_get_hw_ops_for()'
+     * during the collection but we want to avoid that overhead)
+     */
+    node->ops = sw_alloc_ops_i(info->num_io_descriptors);
+    if (!node->ops || sw_init_ops_i(node->ops, info)) {
+        pw_pr_error("ERROR initializing the ops array!\n");
+        sw_free_collector_node(node);
+        return -PW_ERROR;
+    }
+    /*
+     * Allocate and initialize the "collector message".
+     */
+    node->per_msg_payload_size = sw_get_payload_size_i(info);
+    pw_pr_debug("Debug: Per msg payload size = %u\n",
+                (unsigned)node->per_msg_payload_size);
+    node->msg = sw_alloc_collector_msg_i(info, node->per_msg_payload_size);
+    if (!node->msg) {
+        pw_pr_error("ERROR allocating space for a collector msg!\n");
+        sw_free_collector_node(node);
+        return -PW_ERROR;
+    }
+    pw_pr_debug("NODE = %p, NODE->MSG = %p\n", node, node->msg);
+    cpumask_clear(&node->cpumask);
+    {
+        /*
+         * For now, use following protocol:
+         * cpu_mask == -2 ==> Collect on ALL CPUs
+         * cpu_mask == -1 ==> Collect on ANY CPU
+         * cpu_mask >= 0 ==> Collect on a specific CPU
+         */
+        if (node->info->cpu_mask >= 0) {
+            /*
+             * Collect data on 'node->info->cpu_mask'
+             */
+            cpumask_set_cpu(node->info->cpu_mask, &node->cpumask);
+            pw_pr_debug("OK: set CPU = %d\n", node->info->cpu_mask);
+        } else if (node->info->cpu_mask == -1) {
+            /*
+             * Collect data on ANY CPU.  Leave empty as a flag to
+             * signify user wishes to collect data on 'ANY' cpu.
+             */
+            pw_pr_debug("OK: set ANY CPU\n");
+        } else {
+            /*
+             * Collect data on ALL cpus.
+             */
+            cpumask_copy(&node->cpumask, cpu_present_mask);
+            pw_pr_debug("OK: set ALL CPUs\n");
+        }
+    }
+    SW_LIST_ADD(head, node, list);
+    return PW_SUCCESS;
+}
+
+const struct sw_hw_ops **sw_alloc_ops_i(pw_u16_t num_io_descriptors)
+{
+    size_t size = num_io_descriptors * sizeof(struct sw_hw_ops *);
+    const struct sw_hw_ops **ops = sw_kmalloc(size, GFP_KERNEL);
+    if (ops) {
+        memset(ops, 0, size);
+    }
+    return ops;
+}
+
+void sw_free_driver_interface_info_i(struct sw_driver_interface_info *info)
+{
+    if (info) {
+        sw_kfree(info);
+    }
+}
+
+void sw_free_ops_i(const struct sw_hw_ops **ops)
+{
+    if (ops) {
+        sw_kfree(ops);
+    }
+}
+
+/**
+ * sw_copy_driver_interface_info_i - Allocate and copy the passed-in "info".
+ *
+ * @info: Information about the metric and collection properties
+ *
+ * Returns: a pointer to the newly allocated sw_driver_interface_info,
+ *          which is a copy of the version passed in via the info pointer.
+ */
+struct sw_driver_interface_info *sw_copy_driver_interface_info_i(
+                           const struct sw_driver_interface_info *info)
+{
+    size_t size;
+    struct sw_driver_interface_info *node = NULL;
+
+    if (!info) {
+        pw_pr_error("ERROR: NULL sw_driver_interface_info in alloc!\n");
+        return node;
+    }
+
+    size = SW_DRIVER_INTERFACE_INFO_HEADER_SIZE() +
+        (info->num_io_descriptors * sizeof(struct sw_driver_io_descriptor));
+    node = (struct sw_driver_interface_info *)sw_kmalloc(size, GFP_KERNEL);
+    if (!node) {
+        pw_pr_error("ERROR allocating driver interface info!\n");
+        return node;
+    }
+    memcpy((char *)node, (const char *)info, size);
+
+    /*
+     * Do debug dump.
+     */
+    pw_pr_debug("DRIVER info has plugin_ID = %d, metric_ID = %d, "
+                "msg_ID = %d\n", node->plugin_id, node->metric_id,
+                node->msg_id);
+
+    return node;
+}
+int sw_init_driver_interface_info_i(struct sw_driver_interface_info *info)
+{
+    /*
+     * Do any initialization here.
+     * For now, only IPC/MMIO descriptors need to be initialized.
+     */
+    int i=0;
+    struct sw_driver_io_descriptor *descriptor = NULL;
+    if (!info) {
+        pw_pr_error("ERROR: no info!\n");
+        return -PW_ERROR;
+    }
+    for (i=0, descriptor=(struct sw_driver_io_descriptor *)info->descriptors; i<info->num_io_descriptors; ++i, ++descriptor) {
+        if (sw_init_driver_io_descriptor(descriptor)) {
+            return -PW_ERROR;
+        }
+    }
+    return PW_SUCCESS;
+}
+
+int sw_reset_driver_interface_info_i(struct sw_driver_interface_info *info)
+{
+    /*
+     * Do any finalization here.
+     * For now, only IPC/MMIO descriptors need to be finalized.
+     */
+    int i=0;
+    struct sw_driver_io_descriptor *descriptor = NULL;
+    if (!info) {
+        pw_pr_error("ERROR: no info!\n");
+        return -PW_ERROR;
+    }
+    for (i=0, descriptor=(struct sw_driver_io_descriptor *)info->descriptors; i<info->num_io_descriptors; ++i, ++descriptor) {
+        if (sw_reset_driver_io_descriptor(descriptor)) {
+            return -PW_ERROR;
+        }
+    }
+    return PW_SUCCESS;
+}
+int sw_init_ops_i(const struct sw_hw_ops **ops, const struct sw_driver_interface_info *info)
+{
+    int i=0;
+    struct sw_driver_io_descriptor *descriptor = NULL;
+    if (!ops || !info) {
+        return -PW_ERROR;
+    }
+    for (i=0, descriptor=(struct sw_driver_io_descriptor *)info->descriptors; i<info->num_io_descriptors; ++i, ++descriptor) {
+        ops[i] = sw_get_hw_ops_for(descriptor->collection_type);
+        if (ops[i] == NULL) {
+            return -PW_ERROR;
+        }
+    }
+    return PW_SUCCESS;
+}
+
+// If this descriptor's collector has an init function, call it passing in
+// this descriptor.  That allows the collector to perform any initialization
+// or registration specific to this metric.
+int sw_init_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor)
+{
+    sw_io_desc_init_func_t init_func = NULL;
+    const struct sw_hw_ops *ops = sw_get_hw_ops_for(descriptor->collection_type);
+    if (ops == NULL) {
+        pw_pr_error("NULL ops found in init_driver_io_desc: type %d\n", descriptor->collection_type);
+        return -PW_ERROR;
+    }
+    init_func = ops->init;
+
+    if (init_func) {
+        int retval = (*init_func)(descriptor);
+        if (retval) {
+            pw_pr_error("(*init) return value for type %d: %d\n",
+                        descriptor->collection_type, retval);
+        }
+        return retval;
+    }
+    return PW_SUCCESS;
+}
+
+/*
+ * If this descriptor's collector has a finalize function, call it passing in this
+ * descriptor. This allows the collector to perform any finalization specific to
+ * this metric.
+ */
+int sw_reset_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor)
+{
+    sw_io_desc_reset_func_t reset_func = NULL;
+    const struct sw_hw_ops *ops = sw_get_hw_ops_for(descriptor->collection_type);
+    if (ops == NULL) {
+        pw_pr_error("NULL ops found in reset_driver_io_desc: type %d\n", descriptor->collection_type);
+        return -PW_ERROR;
+    }
+    pw_pr_debug("calling reset on descriptor of type %d\n", descriptor->collection_type);
+    reset_func = ops->reset;
+
+    if (reset_func) {
+        int retval = (*reset_func)(descriptor);
+        if (retval) {
+            pw_pr_error("(*reset) return value for type %d: %d\n",
+                        descriptor->collection_type, retval);
+        }
+        return retval;
+    }
+    return PW_SUCCESS;
+}
+
+int sw_handle_driver_io_descriptor(char *dst_vals, int cpu,
+                                   const struct sw_driver_io_descriptor *descriptor,
+                                   const struct sw_hw_ops *hw_ops)
+{
+    typedef void (*sw_hardware_io_func_t)(char *, int, const struct sw_driver_io_descriptor *, u16);
+    sw_hardware_io_func_t hardware_io_func = NULL;
+    if (descriptor->collection_command < SW_IO_CMD_READ || descriptor->collection_command > SW_IO_CMD_WRITE) {
+        return -PW_ERROR;
+    }
+    switch (descriptor->collection_command) {
+        case SW_IO_CMD_READ:
+            hardware_io_func = hw_ops->read;
+            break;
+        case SW_IO_CMD_WRITE:
+            hardware_io_func = hw_ops->write;
+            break;
+        default:
+            break;
+    }
+    if (hardware_io_func) {
+        (*hardware_io_func)(dst_vals, cpu, descriptor, descriptor->counter_size_in_bytes);
+    } else {
+        pw_pr_debug("NO ops to satisfy %u operation for collection type %u!\n", descriptor->collection_command, descriptor->collection_type);
+    }
+    return PW_SUCCESS;
+}
+
+sw_driver_msg_t *sw_alloc_collector_msg_i(const struct sw_driver_interface_info *info, size_t per_msg_payload_size)
+{
+    size_t per_msg_size = 0, total_size = 0;
+    sw_driver_msg_t *msg = NULL;
+    if (!info) {
+        return NULL;
+    }
+    per_msg_size = sizeof(struct sw_driver_msg) + per_msg_payload_size;
+    total_size = per_msg_size * num_possible_cpus();
+    msg = (sw_driver_msg_t *)sw_kmalloc(total_size, GFP_KERNEL);
+    if (msg) {
+        int cpu = -1;
+        memset(msg, 0, total_size);
+        for_each_possible_cpu(cpu) {
+            sw_driver_msg_t *__msg = GET_MSG_SLOT_FOR_CPU(msg, cpu, per_msg_payload_size);
+            char *__payload = (char *)__msg + sizeof(struct sw_driver_msg);
+            __msg->cpuidx = (pw_u16_t)cpu;
+            __msg->plugin_id = (pw_u8_t)info->plugin_id;
+            __msg->metric_id = (pw_u8_t)info->metric_id;
+            __msg->msg_id = (pw_u8_t)info->msg_id;
+            __msg->payload_len = per_msg_payload_size;
+            __msg->p_payload = __payload;
+            pw_pr_debug("[%d]: per_msg_payload_size = %zx, msg = %p, payload = %p\n", cpu, per_msg_payload_size, __msg, __payload);
+        }
+    }
+    return msg;
+}
+
+void sw_free_collector_msg_i(sw_driver_msg_t *msg)
+{
+    if (msg) {
+        sw_kfree(msg);
+    }
+}
+
+size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info)
+{
+    size_t size = 0;
+    int i = 0;
+
+    if (info) {
+        for (i = 0; i < info->num_io_descriptors; size += ((struct sw_driver_io_descriptor *)info->descriptors)[i].counter_size_in_bytes, ++i);
+    }
+    return size;
+}
+
+void sw_handle_per_cpu_msg_i(void *info, enum sw_wakeup_action action)
+{
+    /*
+     * Basic algo:
+     * For each descriptor in 'node->info->descriptors'; do:
+     * 1. Perform H/W read; use 'descriptor->collection_type' to determine type of read; use 'descriptor->counter_size_in_bytes'
+     * for read size. Use msg->p_payload[dst_idx] as dst address
+     * 2. Increment dst idx by 'descriptor->counter_size_in_bytes'
+     */
+    struct sw_collector_data *node = (struct sw_collector_data *)info;
+    int cpu = RAW_CPU();
+    u16 num_descriptors = node->info->num_io_descriptors, i=0;
+    struct sw_driver_io_descriptor *descriptors = (struct sw_driver_io_descriptor *)node->info->descriptors;
+    sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu, node->per_msg_payload_size);
+    char *dst_vals = msg->p_payload;
+    const struct sw_hw_ops **ops = node->ops;
+    bool wasAnyWrite = false;
+
+    // msg TSC assigned when msg is written to buffer
+    msg->cpuidx = cpu;
+
+    for (i=0; i<num_descriptors; ++i, dst_vals += descriptors->counter_size_in_bytes, ++descriptors) {
+        if (unlikely(ops[i] == NULL)) {
+            pw_pr_debug("NULL OPS!\n");
+            continue;
+        }
+        if (descriptors->collection_command == SW_IO_CMD_WRITE) {
+            wasAnyWrite = true;
+        }
+        if (sw_handle_driver_io_descriptor(dst_vals, cpu, descriptors, ops[i])) {
+            pw_pr_error("ERROR reading descriptor with type %d\n", descriptors->collection_type);
+        }
+    }
+
+    /*
+     * We produce messages only on READs. Note that SWA prohibits
+     * messages that contain both READ and WRITE descriptors, so it
+     * is enough to check if there was ANY WRITE descriptor in this
+     * message.
+     */
+    if (likely(wasAnyWrite == false)) {
+        if (sw_produce_generic_msg(msg, action)) {
+            pw_pr_warn("WARNING: could NOT produce message!\n");
+        }
+    }
+
+    return;
+}
+
+
+/*
+ * Collector list and node functions.
+ */
+struct sw_collector_data *sw_alloc_collector_node(void)
+{
+    struct sw_collector_data *node = (struct sw_collector_data *)sw_kmalloc(sizeof(struct sw_collector_data), GFP_KERNEL);
+    if (node) {
+        node->per_msg_payload_size = 0x0;
+        node->last_update_jiffies = 0x0;
+        node->info = NULL;
+        node->ops = NULL;
+        node->msg = NULL;
+        SW_LIST_ENTRY_INIT(node, list);
+    }
+    return node;
+}
+
+void sw_free_collector_node(struct sw_collector_data *node)
+{
+    if (!node) {
+        return;
+    }
+    if (node->info) {
+        sw_reset_driver_interface_info_i(node->info);
+        sw_free_driver_interface_info_i(node->info);
+        node->info = NULL;
+    }
+    if (node->ops) {
+        sw_free_ops_i(node->ops);
+        node->ops = NULL;
+    }
+    if (node->msg) {
+        sw_free_collector_msg_i(node->msg);
+        node->msg = NULL;
+    }
+    sw_kfree(node);
+    return;
+}
+
+int sw_handle_collector_node(struct sw_collector_data *node)
+{
+    if (!node || !node->info || !node->ops || !node->msg) {
+        return -PW_ERROR;
+    }
+    pw_pr_debug("Calling SMP_CALL_FUNCTION_MANY!\n");
+    sw_schedule_work(&node->cpumask, &sw_handle_per_cpu_msg, node);
+    return PW_SUCCESS;
+}
+
+int sw_handle_collector_node_on_cpu(struct sw_collector_data *node, int cpu)
+{
+    if (!node || !node->info || !node->ops || !node->msg) {
+        return -PW_ERROR;
+    }
+    /*
+     * Check if this node indicates it should be scheduled
+     * on the given cpu. If so, clear all other CPUs from the
+     * mask and schedule the node.
+     */
+    if (cpumask_test_cpu(cpu, &node->cpumask)) {
+        struct cpumask tmp_mask;
+        cpumask_clear(&tmp_mask);
+        cpumask_set_cpu(cpu, &tmp_mask);
+        pw_pr_debug("Calling SMP_CALL_FUNCTION_MANY!\n");
+        sw_schedule_work(&tmp_mask, &sw_handle_per_cpu_msg, node);
+    }
+    return PW_SUCCESS;
+}
+
+void sw_init_collector_list(void *list_head)
+{
+    SW_LIST_HEAD_VAR(sw_collector_data) *head = list_head;
+    SW_LIST_HEAD_INIT(head);
+}
+
+void sw_destroy_collector_list(void *list_head)
+{
+    SW_LIST_HEAD_VAR(sw_collector_data) *head = list_head;
+    while (!SW_LIST_EMPTY(head)) {
+        struct sw_collector_data *curr = SW_LIST_GET_HEAD_ENTRY(head, sw_collector_data, list);
+        BUG_ON(!curr->info);
+        SW_LIST_UNLINK(curr, list);
+        sw_free_collector_node(curr);
+    }
+}
+
+/**
+ * sw_handle_collector_list - Iterate through the collector list, calling
+ *                            func() upon each element.
+ * @list_head:  The collector list head.
+ * @func:  The function to call for each collector.
+ *
+ * This function is called when one of the "when types" fires, since the
+ * passed-in collector node list is the list of collections to do at that time.
+ *
+ * Returns: PW_SUCCESS on success, -PW_ERROR on error.
+ */
+int sw_handle_collector_list(void *list_head,
+                             int (*func)(struct sw_collector_data *data))
+{
+    SW_LIST_HEAD_VAR(sw_collector_data) *head = list_head;
+    int retVal = PW_SUCCESS;
+    struct sw_collector_data *curr = NULL;
+    if (!head || !func) {
+        return -PW_ERROR;
+    }
+    SW_LIST_FOR_EACH_ENTRY(curr, head, list) {
+        pw_pr_debug("HANDLING\n");
+        if ((*func)(curr)) {
+            retVal = -PW_ERROR;
+        }
+    }
+    return retVal;
+}
+
+int sw_handle_collector_list_on_cpu(void *list_head,
+                                    int (*func)(struct sw_collector_data *data, int cpu),
+                                    int cpu)
+{
+    SW_LIST_HEAD_VAR(sw_collector_data) *head = list_head;
+    int retVal = PW_SUCCESS;
+    struct sw_collector_data *curr = NULL;
+    if (!head || !func) {
+        return -PW_ERROR;
+    }
+    SW_LIST_FOR_EACH_ENTRY(curr, head, list) {
+        pw_pr_debug("HANDLING\n");
+        if ((*func)(curr, cpu)) {
+            retVal = -PW_ERROR;
+        }
+    }
+    return retVal;
+}
+
+void sw_handle_per_cpu_msg(void *info)
+{
+    sw_handle_per_cpu_msg_i(info, SW_WAKEUP_ACTION_DIRECT);
+}
+
+void sw_handle_per_cpu_msg_no_sched(void *info)
+{
+    sw_handle_per_cpu_msg_i(info, SW_WAKEUP_ACTION_TIMER);
+}
+
+void sw_handle_per_cpu_msg_on_cpu(int cpu, void *info)
+{
+    if (unlikely(cpu == RAW_CPU())) {
+        sw_handle_per_cpu_msg_no_sched(info);
+    } else {
+        pw_pr_debug("[%d] is handling for %d\n", RAW_CPU(), cpu);
+        /*
+         * No need to disable preemption -- 'smp_call_function_single' does that for us.
+         */
+        smp_call_function_single(cpu, &sw_handle_per_cpu_msg_no_sched, info, false /* false ==> do NOT wait for function completion */);
+    }
+}
+
+void sw_set_collector_ops(const struct sw_hw_ops *hw_ops)
+{
+    s_hw_ops = hw_ops;
+}
diff --git a/drivers/misc/intel/socwatch/sw_driver.c b/drivers/misc/intel/socwatch/sw_driver.c
new file mode 100644
index 000000000000..51f228d5f51a
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_driver.c
@@ -0,0 +1,1266 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#define MOD_AUTHOR "Gautam Upadhyaya <gautam.upadhyaya@intel.com>"
+#define MOD_DESC "SoC Watch kernel module"
+
+#include "sw_internal.h"
+#include "sw_structs.h"
+#include "sw_defines.h"
+#include "sw_types.h"
+#include "sw_mem.h"
+#include "sw_ioctl.h"
+#include "sw_output_buffer.h"
+#include "sw_hardware_io.h"
+#include "sw_overhead_measurements.h"
+#include "sw_tracepoint_handlers.h"
+#include "sw_collector.h"
+#include "sw_file_ops.h"
+
+/* -------------------------------------------------
+ * Compile time constants.
+ * -------------------------------------------------
+ */
+/*
+ * Number of entries in the 'sw_collector_lists' array
+ */
+#define NUM_COLLECTOR_MODES (SW_WHEN_TYPE_END - SW_WHEN_TYPE_BEGIN + 1)
+#define PW_OUTPUT_BUFFER_SIZE 256 /* Number of output messages in each per-cpu buffer */
+/*
+ * Check if tracepoint/notifier ID is in (user-supplied) mask
+ */
+#define IS_TRACE_NOTIFIER_ID_IN_MASK(id, mask) ( (id) >= 0 && ( ( (mask) >> (id) ) & 0x1 ) )
+
+
+/* -------------------------------------------------
+ *  Local function declarations.
+ * -------------------------------------------------
+ */
+int sw_load_driver_i(void);
+void sw_unload_driver_i(void);
+int sw_init_collector_lists_i(void);
+void sw_destroy_collector_lists_i(void);
+int sw_init_data_structures_i(void);
+void sw_destroy_data_structures_i(void);
+int sw_get_arch_details_i(void);
+void sw_iterate_driver_info_lists_i(void);
+void sw_handle_immediate_request_i(void *request);
+int sw_print_collector_node_i(struct sw_collector_data *data);
+int sw_collection_start_i(void);
+int sw_collection_stop_i(void);
+int sw_collection_poll_i(void);
+size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info);
+sw_driver_msg_t *sw_alloc_collector_msg_i(const struct sw_driver_interface_info *info, size_t per_msg_payload_size);
+static long sw_unlocked_handle_ioctl_i(unsigned int ioctl_num,
+                                       void *p_local_args);
+static long sw_set_driver_infos_i(struct sw_driver_interface_msg __user *remote_msg, int local_len);
+static long sw_handle_cmd_i(sw_driver_collection_cmd_t cmd, u64 __user* remote_out_args);
+static void sw_do_extract_scu_fw_version(void);
+static long sw_get_available_name_id_mappings_i(enum sw_name_id_type type, struct sw_name_info_msg __user* remote_info, size_t local_len);
+static enum sw_driver_collection_cmd sw_get_collection_cmd_i(void);
+static bool sw_should_flush_buffer_i(void);
+
+/* -------------------------------------------------
+ * Data structures.
+ * -------------------------------------------------
+ */
+/*
+ * Structure to hold current CMD state
+ * of the device driver. Constantly evolving, but
+ * that's OK -- this is internal to the driver
+ * and is NOT exported.
+ */
+struct swa_internal_state {
+    sw_driver_collection_cmd_t cmd; // indicates which command was specified last e.g. START, STOP etc.
+    /*
+     * Should we write to our per-cpu output buffers?
+     * YES if we're actively collecting.
+     * NO if we're not.
+     */
+    bool write_to_buffers;
+    /*
+     * Should we "drain/flush" the per-cpu output buffers?
+     * (See "device_read" for an explanation)
+     */
+    bool drain_buffers;
+    // Others...
+};
+
+
+/* -------------------------------------------------
+ * Variables.
+ * -------------------------------------------------
+ */
+static bool do_force_module_scope_for_cpu_frequencies = false;
+module_param(do_force_module_scope_for_cpu_frequencies, bool, S_IRUSR);
+MODULE_PARM_DESC(do_force_module_scope_for_cpu_frequencies, "Toggle module scope for cpu frequencies. Sets \"affected_cpus\" and \"related_cpus\" of cpufreq_policy.");
+
+static unsigned short sw_buffer_num_pages = 16;
+module_param(sw_buffer_num_pages, ushort, S_IRUSR);
+MODULE_PARM_DESC(sw_buffer_num_pages, "Specify number of 4kB pages to use for each per-cpu buffer. MUST be a power of 2! Default value = 16 (64 kB)");
+
+/* TODO: convert from 'list_head' to 'hlist_head' */
+/*
+ * sw_collector_lists is an array of linked lists of "collector nodes"
+ * (sw_collector_data structs).  It is indexed by the sw_when_type_t's.
+ * Each list holds the collectors to "execute" at a specific time,
+ * e.g. the begining of the run, at a poll interval, tracepoint, etc.
+ */
+static SW_DEFINE_LIST_HEAD(sw_collector_lists, sw_collector_data)[NUM_COLLECTOR_MODES];
+static __read_mostly u16 sw_scu_fw_major_minor = 0x0;
+
+static struct swa_internal_state s_internal_state;
+static struct sw_file_ops s_ops = {
+    .ioctl_handler =    &sw_unlocked_handle_ioctl_i,
+    .stop_handler =     &sw_collection_stop_i,
+    .get_current_cmd =  &sw_get_collection_cmd_i,
+    .should_flush =     &sw_should_flush_buffer_i,
+};
+
+/*
+ * For each function that you want to profile,
+ * do the following (e.g. function 'foo'):
+ * **************************************************
+ * DECLARE_OVERHEAD_VARS(foo);
+ * **************************************************
+ * This will declare the two variables required
+ * to keep track of overheads incurred in
+ * calling/servicing 'foo'. Note that the name
+ * that you declare here *MUST* match the function name!
+ */
+
+DECLARE_OVERHEAD_VARS(sw_collection_poll_i); // for POLL
+DECLARE_OVERHEAD_VARS(sw_any_seg_full);
+
+/*
+ * String representation of the various 'SW_WHEN_TYPE_XYZ' enum values.
+ * Debugging ONLY!
+ */
+#if DO_DEBUG_OUTPUT
+static const char *s_when_type_names[] = {
+    "BEGIN",
+    "POLL",
+    "NOTIFIER",
+    "TRACEPOINT",
+    "END"
+};
+#endif // DO_DEBUG_OUTPUT
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+/*
+ * External functions.
+ */
+int sw_process_snapshot(enum sw_when_type when)
+{
+    if (when > SW_WHEN_TYPE_END) {
+        pw_pr_error("invalid snapshot time %d specified!\n", when);
+        return -EINVAL;
+    }
+    if (sw_handle_collector_list(&sw_collector_lists[when], &sw_handle_collector_node)) {
+        pw_pr_error("ERROR: could NOT handle snapshot for time %d!\n", when);
+        return -EIO;
+    }
+    return 0;
+}
+
+int sw_process_snapshot_on_cpu(enum sw_when_type when, int cpu)
+{
+    if (when > SW_WHEN_TYPE_END) {
+        pw_pr_error("invalid snapshot time %d specified!\n", when);
+        return -EINVAL;
+    }
+    if (sw_handle_collector_list_on_cpu(&sw_collector_lists[when], &sw_handle_collector_node_on_cpu, cpu)) {
+        pw_pr_error("ERROR: could NOT handle snapshot for time %d!\n", when);
+        return -EIO;
+    }
+    return 0;
+}
+
+/*
+ * Driver interface info and collector list functions.
+ */
+int sw_print_collector_node_i(struct sw_collector_data *curr)
+{
+    pw_u16_t num_descriptors = 0;
+    sw_io_desc_print_func_t print_func = NULL;
+    struct sw_driver_io_descriptor *descriptor = NULL;
+    struct sw_driver_interface_info *info = NULL;
+    if (!curr) {
+        return -PW_ERROR;
+    }
+    info = curr->info;
+    descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+    pw_pr_debug("cpu-mask = %d, Plugin-ID = %d, Metric-ID = %d, MSG-ID = %d\n",
+                info->cpu_mask, info->plugin_id, info->metric_id, info->msg_id);
+    for (num_descriptors = info->num_io_descriptors; num_descriptors > 0; --num_descriptors, ++descriptor) {
+        const struct sw_hw_ops *ops = sw_get_hw_ops_for(descriptor->collection_type);
+        if (ops == NULL) {
+            return -PW_ERROR;
+        }
+        print_func = ops->print;
+        if (print_func && (*print_func)(descriptor)) {
+            return -PW_ERROR;
+        }
+    }
+    return PW_SUCCESS;
+}
+
+/*
+ * Driver interface info and collector list functions.
+ */
+
+/**
+ * sw_reset_collector_node_i - Call the reset op on all of the descriptors
+ *                             in coll that have one.
+ * @coll: The data structure containing an array of collector descriptors.
+ *
+ * Return: PW_SUCCESS if all of the resets succeeded, -PW_ERROR if any failed.
+ */
+int sw_reset_collector_node_i(struct sw_collector_data *coll)
+{
+    struct sw_driver_io_descriptor *descriptor = NULL;
+    struct sw_driver_interface_info *info = NULL;
+    int num_descriptors;
+    int retcode = PW_SUCCESS;
+
+    if (!coll) {
+        return -PW_ERROR;
+    }
+    info = coll->info;
+
+    descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+    pw_pr_debug("cpu-mask = %d, Plugin-ID = %d, Metric-ID = %d, MSG-ID = %d\n",
+                info->cpu_mask, info->plugin_id, info->metric_id, info->msg_id);
+    for (num_descriptors = info->num_io_descriptors; num_descriptors > 0; --num_descriptors, ++descriptor) {
+        const struct sw_hw_ops *ops = sw_get_hw_ops_for(descriptor->collection_type);
+        if (ops && ops->reset && (*ops->reset)(descriptor)) {
+            retcode = -PW_ERROR;
+        }
+    }
+    return retcode;
+}
+
+int sw_iterate_trace_notifier_list_i(struct sw_trace_notifier_data *node, void *dummy)
+{
+    return sw_handle_collector_list(&node->list, &sw_print_collector_node_i);
+}
+
+void sw_iterate_driver_info_lists_i(void)
+{
+    sw_when_type_t which;
+    for (which = SW_WHEN_TYPE_BEGIN; which <= SW_WHEN_TYPE_END; ++which) {
+        pw_pr_debug("ITERATING list %s\n", s_when_type_names[which]);
+        if (sw_handle_collector_list(&sw_collector_lists[which], &sw_print_collector_node_i)) { // Should NEVER happen!
+            pw_pr_error("WARNING: error occured while printing values!\n");
+        }
+    }
+
+    if (sw_for_each_tracepoint_node(&sw_iterate_trace_notifier_list_i, NULL, false /*return-on-error*/)) {
+        pw_pr_error("WARNING: error occured while printing tracepoint values!\n");
+    }
+    if (sw_for_each_notifier_node(&sw_iterate_trace_notifier_list_i, NULL, false /*return-on-error*/)) {
+        pw_pr_error("WARNING: error occured while printing notifier values!\n");
+    }
+}
+
+void sw_reset_collectors_i(void)
+{
+    sw_when_type_t which;
+    for (which = SW_WHEN_TYPE_BEGIN; which <= SW_WHEN_TYPE_END; ++which) {
+        pw_pr_debug("ITERATING list %s\n", s_when_type_names[which]);
+        if (sw_handle_collector_list(&sw_collector_lists[which], &sw_reset_collector_node_i)) {
+            pw_pr_error("WARNING: error occured while resetting a collector!\n");
+        }
+    }
+}
+
+int sw_init_data_structures_i(void)
+{
+    /*
+     * Find the # CPUs in this system.
+     * Update: use 'num_possible' instead of 'num_present' in case
+     * the cpus aren't numbered contiguously
+     */
+    sw_max_num_cpus = num_possible_cpus();
+
+    /*
+     * Initialize our trace subsys: MUST be called
+     * BEFORE 'sw_init_collector_lists_i()!
+     */
+    if (sw_add_trace_notify()) {
+        sw_destroy_data_structures_i();
+        return -PW_ERROR;
+    }
+    if (sw_init_collector_lists_i()) {
+        sw_destroy_data_structures_i();
+        return -PW_ERROR;
+    }
+    if (sw_init_per_cpu_buffers()) {
+        sw_destroy_data_structures_i();
+        return -PW_ERROR;
+    }
+    if (sw_register_hw_ops()) {
+        sw_destroy_data_structures_i();
+        return -PW_ERROR;
+    }
+    return PW_SUCCESS;
+}
+
+void sw_destroy_data_structures_i(void)
+{
+    sw_free_hw_ops();
+    sw_destroy_per_cpu_buffers();
+    sw_destroy_collector_lists_i();
+    sw_remove_trace_notify();
+}
+
+int sw_get_arch_details_i(void)
+{
+    /*
+     * SCU F/W version (if applicable)
+     */
+    sw_do_extract_scu_fw_version();
+    return PW_SUCCESS;
+}
+
+#define INIT_FLAG ((void *)0)
+#define DESTROY_FLAG ((void *)1)
+
+static int sw_init_destroy_trace_notifier_lists_i(struct sw_trace_notifier_data *node, void *is_init)
+{
+    if (is_init == INIT_FLAG) {
+        sw_init_collector_list(&node->list);
+    } else {
+        sw_destroy_collector_list(&node->list);
+    }
+    node->was_registered = false;
+
+    return PW_SUCCESS;
+}
+
+int sw_init_collector_lists_i(void)
+{
+    int i=0;
+    for (i=0; i<NUM_COLLECTOR_MODES; ++i) {
+        sw_init_collector_list(&sw_collector_lists[i]);
+    }
+    sw_for_each_tracepoint_node(&sw_init_destroy_trace_notifier_lists_i, INIT_FLAG, false /*return-on-error*/);
+    sw_for_each_notifier_node(&sw_init_destroy_trace_notifier_lists_i, INIT_FLAG, false /*return-on-error*/);
+
+    return PW_SUCCESS;
+}
+
+void sw_destroy_collector_lists_i(void)
+{
+    int i=0;
+    for (i=0; i<NUM_COLLECTOR_MODES; ++i) {
+        sw_destroy_collector_list(&sw_collector_lists[i]);
+    }
+    sw_for_each_tracepoint_node(&sw_init_destroy_trace_notifier_lists_i, DESTROY_FLAG, false /*return-on-error*/);
+    sw_for_each_notifier_node(&sw_init_destroy_trace_notifier_lists_i, DESTROY_FLAG, false /*return-on-error*/);
+}
+
+/*
+ * Used for {READ,WRITE}_IMMEDIATE requests.
+ */
+typedef struct sw_immediate_request_info sw_immediate_request_info_t;
+struct sw_immediate_request_info {
+    struct sw_driver_io_descriptor *local_descriptor;
+    char *dst_vals;
+    int *retVal;
+};
+void sw_handle_immediate_request_i(void *request)
+{
+    struct sw_immediate_request_info *info = (struct sw_immediate_request_info *)request;
+    struct sw_driver_io_descriptor *descriptor = info->local_descriptor;
+    char *dst_vals = info->dst_vals;
+    const struct sw_hw_ops *ops = sw_get_hw_ops_for(descriptor->collection_type);
+    if (likely(ops != NULL)) {
+        *(info->retVal) = sw_handle_driver_io_descriptor(dst_vals, RAW_CPU(), descriptor, ops);
+    } else {
+        pw_pr_error("No operations found to satisfy collection type %u!\n", descriptor->collection_type);
+    }
+    return;
+}
+
+static int num_times_polled=0;
+
+int sw_collection_start_i(void)
+{
+    /*
+     * Reset the poll tick counter.
+     */
+    num_times_polled = 0;
+    /*
+     * Update the output buffers.
+     */
+    sw_reset_per_cpu_buffers();
+    /*
+     * Ensure clients don't think we're in 'flush' mode.
+     */
+    s_internal_state.drain_buffers = false;
+    /*
+     * Set the 'command'
+     */
+    s_internal_state.cmd = SW_DRIVER_START_COLLECTION;
+    /*
+     * Clear out the topology list
+     */
+    sw_clear_topology_list();
+    /*
+     * Handle 'START' snapshots, if any.
+     */
+    {
+        if (sw_handle_collector_list(&sw_collector_lists[SW_WHEN_TYPE_BEGIN], &sw_handle_collector_node)) {
+            pw_pr_error("ERROR: could NOT handle START collector list!\n");
+            return -PW_ERROR;
+        }
+    }
+    /*
+     * Register any required tracepoints and notifiers.
+     */
+    {
+        if (sw_register_trace_notifiers()) {
+            pw_pr_error("ERROR registering trace_notifiers!\n");
+            sw_unregister_trace_notifiers();
+            return -PW_ERROR;
+        }
+    }
+    pw_pr_debug("OK, STARTED collection!\n");
+    return PW_SUCCESS;
+}
+
+int sw_collection_stop_i(void)
+{
+    /*
+     * Unregister any registered tracepoints and notifiers.
+     */
+    if (sw_unregister_trace_notifiers()) {
+        pw_pr_warn("Warning: some trace_notifier probe functions could NOT be unregistered!\n");
+    }
+    /*
+     * Handle 'STOP' snapshots, if any.
+     */
+    if (sw_handle_collector_list(&sw_collector_lists[SW_WHEN_TYPE_END], &sw_handle_collector_node)) {
+        pw_pr_error("ERROR: could NOT handle STOP collector list!\n");
+        return -PW_ERROR;
+    }
+    /*
+     * Set the 'command'
+     */
+    s_internal_state.cmd = SW_DRIVER_STOP_COLLECTION;
+    /*
+     * Tell consumers to 'flush' all buffers. We need to
+     * defer this as long as possible because it needs to be
+     * close to the 'wake_up_interruptible', below.
+     */
+    s_internal_state.drain_buffers = true;
+    smp_mb();
+    /*
+     * Wakeup any sleeping readers, and cleanup any
+     * timers in the reader subsys.
+     */
+    sw_cancel_reader();
+    /*
+     * Collect stats on samples produced and dropped.
+     * TODO: call from 'device_read()' instead?
+     */
+    sw_count_samples_produced_dropped();
+#if DO_OVERHEAD_MEASUREMENTS
+    pw_pr_force("DEBUG: there were %llu samples produced and %llu samples dropped in buffer v5!\n", sw_num_samples_produced, sw_num_samples_dropped);
+#endif // DO_OVERHEAD_MEASUREMENTS
+    /*
+     * DEBUG: iterate over collection lists.
+     */
+    sw_iterate_driver_info_lists_i();
+    /*
+     * Shut down any collectors that need shutting down.
+     */
+    sw_reset_collectors_i();
+    /*
+     * Clear out the collector lists.
+     */
+    sw_destroy_collector_lists_i();
+    pw_pr_debug("OK, STOPPED collection!\n");
+#if DO_OVERHEAD_MEASUREMENTS
+    pw_pr_force("There were %d poll ticks!\n", num_times_polled);
+#endif // DO_OVERHEAD_MEASUREMENTS
+    return PW_SUCCESS;
+}
+
+int sw_collection_poll_i(void)
+{
+    /*
+     * Handle 'POLL' timer expirations.
+     */
+    if (SW_LIST_EMPTY(&sw_collector_lists[SW_WHEN_TYPE_POLL])) {
+        pw_pr_debug("DEBUG: EMPTY POLL LIST\n");
+    }
+    ++num_times_polled;
+    return sw_handle_collector_list(&sw_collector_lists[SW_WHEN_TYPE_POLL], &sw_handle_collector_node);
+}
+
+/*
+ * Private data for the 'sw_add_trace_notifier_driver_info_i' function.
+ */
+struct tn_data {
+    struct sw_driver_interface_info *info;
+    u64 mask;
+};
+
+static int sw_add_trace_notifier_driver_info_i(struct sw_trace_notifier_data *node, void *priv)
+{
+    struct tn_data *data = (struct tn_data *)priv;
+    struct sw_driver_interface_info *local_info = data->info;
+    u64 mask = data->mask;
+    int id = sw_get_trace_notifier_id(node);
+    if (IS_TRACE_NOTIFIER_ID_IN_MASK(id, mask)) {
+        pw_pr_debug("TRACEPOINT ID = %d is IN mask 0x%llx\n", id, mask);
+        if (sw_add_driver_info(&node->list, local_info)) {
+            pw_pr_error("WARNING: could NOT add driver info to list!\n");
+            return -PW_ERROR;
+        }
+    }
+    return PW_SUCCESS;
+}
+
+static int sw_post_config_i(const struct sw_hw_ops *op, void *priv)
+{
+    if (!op->available || !(*op->available)()) {
+        /* op not available */
+        return 0;
+    }
+    if (!op->post_config || (*op->post_config)()) {
+        return 0;
+    }
+    return -EIO;
+}
+
+/**
+ * sw_set_driver_infos_i - Process the collection config data passed down
+ *                         from the client.
+ * @remote_msg: The user space address of our ioctl data.
+ * @local_len:  The number of bytes of remote_msg we should copy.
+ *
+ * This function copies the ioctl data from user space to kernel
+ * space.  That data is an array of sw_driver_interface_info structs,
+ * which hold information about tracepoints, notifiers, and collector
+ * configuration info for this collection run..  For each driver_info
+ * struct, it calls the appropriate "add info" (registration/
+ * configuration) function for each of the "when types" (begin, poll,
+ * notifier, tracepoint, end) which should trigger a collection
+ * operation for that collector.
+ *
+ * When this function is done, the data structures corresponding to
+ * collection should be configured and initialized.
+ *
+ *
+ * Returns: PW_SUCCESS on success, or a non-zero on an error.
+ */
+static long sw_set_driver_infos_i(struct sw_driver_interface_msg __user *remote_msg, int local_len)
+{
+    struct sw_driver_interface_info *local_info = NULL;
+    struct sw_driver_interface_msg *local_msg = vmalloc(local_len);
+    pw_u8_t read_triggers = 0x0;
+    pw_u16_t num_infos = 0;
+    sw_when_type_t i = SW_WHEN_TYPE_BEGIN;
+    char *__data = (char *)local_msg->infos;
+    size_t dst_idx = 0;
+
+    if (!local_msg) {
+        pw_pr_error("ERROR allocating space for local message!\n");
+        return -EFAULT;
+    }
+    if (copy_from_user(local_msg, remote_msg, local_len)) {
+        pw_pr_error("ERROR copying message from user space!\n");
+        vfree(local_msg);
+        return -EFAULT;
+    }
+    /*
+     * We aren't allowed to config the driver multiple times between
+     * collections. Clear out any previous config values.
+     */
+    sw_destroy_collector_lists_i();
+
+    /*
+     * Did the user specify a min polling interval?
+     */
+    sw_min_polling_interval_msecs = local_msg->min_polling_interval_msecs;
+    pw_pr_debug("min_polling_interval_msecs = %u\n", sw_min_polling_interval_msecs);
+
+    num_infos = local_msg->num_infos;
+    pw_pr_debug("LOCAL NUM INFOS = %u\n", num_infos);
+    for (; num_infos > 0; --num_infos) {
+        local_info = (struct sw_driver_interface_info *)&__data[dst_idx];
+        dst_idx += (SW_DRIVER_INTERFACE_INFO_HEADER_SIZE() + local_info->num_io_descriptors * sizeof(struct sw_driver_io_descriptor));
+        read_triggers = local_info->trigger_bits;
+        pw_pr_debug("read_triggers = %u, # msrs = %u, new dst_idx = %u\n",
+            (unsigned)read_triggers, (unsigned)local_info->num_io_descriptors, (unsigned)dst_idx);
+        for (i=SW_WHEN_TYPE_BEGIN; i<= SW_WHEN_TYPE_END; ++i, read_triggers >>= 1) {
+            if (read_triggers & 0x1) { // Bit 'i' is set
+                pw_pr_debug("BIT %d is SET!\n", i);
+                if (i == SW_WHEN_TYPE_TRACEPOINT) {
+                    struct tn_data tn_data = {local_info, local_info->tracepoint_id_mask};
+                    pw_pr_debug("TRACEPOINT, MASK = 0x%llx\n", local_info->tracepoint_id_mask);
+                    sw_for_each_tracepoint_node(&sw_add_trace_notifier_driver_info_i, &tn_data, false /*return-on-error*/);
+                } else if (i == SW_WHEN_TYPE_NOTIFIER) {
+                    struct tn_data tn_data = {local_info, local_info->notifier_id_mask};
+                    pw_pr_debug("NOTIFIER, MASK = 0x%llx\n", local_info->notifier_id_mask);
+                    sw_for_each_notifier_node(&sw_add_trace_notifier_driver_info_i, &tn_data, false /*return-on-error*/);
+                } else {
+                    if (sw_add_driver_info(&sw_collector_lists[i], local_info)) {
+                        pw_pr_error("WARNING: could NOT add driver info to list for 'when type' %d!\n", i);
+                    }
+                }
+            }
+        }
+    }
+    if (sw_for_each_hw_op(&sw_post_config_i, NULL, false /*return-on-error*/)) {
+        pw_pr_error("POST-CONFIG error!\n");
+    }
+    vfree(local_msg);
+    memset(&s_internal_state, 0, sizeof(s_internal_state));
+    /*
+     * DEBUG: iterate over collection lists.
+     */
+    sw_iterate_driver_info_lists_i();
+    return PW_SUCCESS;
+}
+
+static long sw_handle_cmd_i(sw_driver_collection_cmd_t cmd, u64 __user* remote_out_args)
+{
+    /*
+     * First, handle the command.
+     */
+    if (cmd < SW_DRIVER_START_COLLECTION || cmd > SW_DRIVER_CANCEL_COLLECTION) {
+        pw_pr_error("ERROR: invalid cmd = %d\n", cmd);
+        return -PW_ERROR;
+    }
+    switch (cmd) {
+        case SW_DRIVER_START_COLLECTION:
+            if (sw_collection_start_i()) {
+                return -PW_ERROR;
+            }
+            break;
+        case SW_DRIVER_STOP_COLLECTION:
+            if (sw_collection_stop_i()) {
+                return -PW_ERROR;
+            }
+            break;
+        default:
+            pw_pr_error("WARNING: unsupported command %d\n", cmd);
+            break;
+    }
+    /*
+     * Then retrieve sample stats.
+     */
+#if DO_COUNT_DROPPED_SAMPLES
+    if (cmd == SW_DRIVER_STOP_COLLECTION) {
+        u64 local_args[2] = {sw_num_samples_produced, sw_num_samples_dropped};
+        if (copy_to_user(remote_out_args, local_args, sizeof(local_args))) {
+            pw_pr_error("couldn't copy collection stats to user space!\n");
+            return -PW_ERROR;
+        }
+    }
+#endif // DO_COUNT_DROPPED_SAMPLES
+    return PW_SUCCESS;
+}
+
+#ifdef SFI_SIG_OEMB
+static int sw_do_parse_sfi_oemb_table(struct sfi_table_header *header)
+{
+#ifdef CONFIG_X86_WANT_INTEL_MID
+    struct sfi_table_oemb *oemb = (struct sfi_table_oemb *)header; // 'struct sfi_table_oemb' defined in 'intel-mid.h'
+    if (!oemb) {
+        pw_pr_error("ERROR: NULL sfi table header!\n");
+        return -PW_ERROR;
+    }
+    sw_scu_fw_major_minor = (oemb->scu_runtime_major_version << 8) | (oemb->scu_runtime_minor_version);
+    pw_pr_debug("DEBUG: major = %u, minor = %u\n", oemb->scu_runtime_major_version, oemb->scu_runtime_minor_version);
+#endif // CONFIG_X86_WANT_INTEL_MID
+    return PW_SUCCESS;
+}
+#endif // SFI_SIG_OEMB
+
+static void sw_do_extract_scu_fw_version(void)
+{
+    sw_scu_fw_major_minor = 0x0;
+#ifdef SFI_SIG_OEMB
+    if (sfi_table_parse(SFI_SIG_OEMB, NULL, NULL, &sw_do_parse_sfi_oemb_table)) {
+        pw_pr_force("WARNING: NO SFI information!\n");
+    }
+#endif // SFI_SIG_OEMB
+}
+
+static int sw_gather_trace_notifier_i(struct sw_trace_notifier_data *node, struct sw_name_info_msg *msg, enum sw_name_id_type type)
+{
+    pw_u16_t *idx = &msg->payload_len;
+    char *buffer = (char *)&msg->pairs[*idx];
+    struct sw_name_id_pair *pair = (struct sw_name_id_pair *)buffer;
+    int id = sw_get_trace_notifier_id(node);
+    struct sw_string_type *str = &pair->name;
+    const char *abstract_name = sw_get_trace_notifier_abstract_name(node);
+
+    if (likely(abstract_name && id >= 0)) {
+        ++msg->num_name_id_pairs;
+        pair->type = type;
+        pair->id = (u16)id;
+        str->len = strlen(abstract_name) + 1; // "+1" for trailing '\0'
+        memcpy(&str->data[0], abstract_name, str->len);
+
+        pw_pr_debug("TP[%d] = %s (%u)\n", sw_get_trace_notifier_id(node), abstract_name, (unsigned)strlen(abstract_name));
+
+        *idx += SW_NAME_ID_HEADER_SIZE() + SW_STRING_TYPE_HEADER_SIZE() + str->len;
+    }
+
+    return PW_SUCCESS;
+}
+
+static int sw_gather_tracepoint_i(struct sw_trace_notifier_data *node, void *priv)
+{
+    return sw_gather_trace_notifier_i(node, (struct sw_name_info_msg *)priv, SW_NAME_TYPE_TRACEPOINT);
+}
+
+static int sw_gather_notifier_i(struct sw_trace_notifier_data *node, void *priv)
+{
+    return sw_gather_trace_notifier_i(node, (struct sw_name_info_msg *)priv, SW_NAME_TYPE_NOTIFIER);
+}
+
+static long sw_get_available_trace_notifiers_i(enum sw_name_id_type type, struct sw_name_info_msg *local_info)
+{
+    long retVal = PW_SUCCESS;
+    if (type == SW_NAME_TYPE_TRACEPOINT) {
+        retVal = sw_for_each_tracepoint_node(&sw_gather_tracepoint_i, local_info, false /*return-on-error*/);
+    } else {
+        retVal = sw_for_each_notifier_node(&sw_gather_notifier_i, local_info, false /*return-on-error*/);
+    }
+    pw_pr_debug("There are %u extracted traces/notifiers for a total of %u bytes!\n", local_info->num_name_id_pairs, local_info->payload_len);
+    return retVal;
+}
+
+static int sw_gather_hw_op_i(const struct sw_hw_ops *op, void *priv)
+{
+    struct sw_name_info_msg *msg = (struct sw_name_info_msg *)priv;
+    pw_u16_t *idx = &msg->payload_len;
+    char *buffer = (char *)&msg->pairs[*idx];
+    struct sw_name_id_pair *pair = (struct sw_name_id_pair *)buffer;
+    struct sw_string_type *str = &pair->name;
+    const char *abstract_name = sw_get_hw_op_abstract_name(op);
+    int id = sw_get_hw_op_id(op);
+
+    pw_pr_debug("Gather Collector[%d] = %s\n", id, abstract_name);
+    if (likely(abstract_name && id >= 0)) {
+        /*
+         * Final check: is this operation available on the target platform?
+         * If 'available' function doesn't exist then YES. Else call 'available'
+         * function to decide.
+         */
+        pw_pr_debug("%s has available = %p\n", abstract_name, op->available);
+        if (!op->available || (*op->available)()) {
+            ++msg->num_name_id_pairs;
+            pair->type = SW_NAME_TYPE_COLLECTOR;
+            pair->id = (u16)id;
+            str->len = strlen(abstract_name) + 1; // "+1" for trailing '\0'
+            memcpy(&str->data[0], abstract_name, str->len);
+
+            *idx += SW_NAME_ID_HEADER_SIZE() + SW_STRING_TYPE_HEADER_SIZE() + str->len;
+        }
+    }
+
+    return PW_SUCCESS;
+}
+
+static long sw_get_available_collectors_i(struct sw_name_info_msg *local_info)
+{
+    return sw_for_each_hw_op(&sw_gather_hw_op_i, local_info, false /*return-on-error*/);
+}
+
+static long sw_get_available_name_id_mappings_i(enum sw_name_id_type type, struct sw_name_info_msg __user* remote_info, size_t local_len)
+{
+    char *buffer = vmalloc(local_len);
+    struct sw_name_info_msg *local_info = NULL;
+    long retVal = PW_SUCCESS;
+    if (!buffer) {
+        pw_pr_error("ERROR: couldn't alloc temp buffer!\n");
+        return -PW_ERROR;
+    }
+    memset(buffer, 0, local_len);
+    local_info = (struct sw_name_info_msg *)buffer;
+
+    if (type == SW_NAME_TYPE_COLLECTOR) {
+        retVal = sw_get_available_collectors_i(local_info);
+    } else {
+        retVal = sw_get_available_trace_notifiers_i(type, local_info);
+    }
+    if (retVal == PW_SUCCESS) {
+        retVal = copy_to_user(remote_info, local_info, local_len);
+        if (retVal) {
+            pw_pr_error("ERROR: couldn't copy tracepoint info to user space!\n");
+        }
+    }
+    vfree(buffer);
+    return retVal;
+}
+
+static long sw_get_topology_changes_i(struct sw_driver_topology_msg __user* remote_msg, size_t local_len)
+{
+    char *buffer = NULL;
+    struct sw_driver_topology_msg *local_msg = NULL;
+    size_t buffer_len = sizeof(struct sw_driver_topology_msg) + sw_num_topology_entries * sizeof(struct sw_driver_topology_change);
+    long retVal = PW_SUCCESS;
+    struct sw_driver_topology_change *dst = NULL;
+    size_t dst_idx = 0;
+    SW_LIST_HEAD_VAR(sw_topology_node) *head = (void *)&sw_topology_list;
+    struct sw_topology_node *tnode = NULL;
+
+    if (local_len < buffer_len) {
+        pw_pr_error("ERROR: insufficient buffer space to encode topology changes! Requires %zu, output space = %zu\n", buffer_len, local_len);
+        return -EIO;
+    }
+
+    buffer = vmalloc(buffer_len);
+    if (!buffer) {
+        pw_pr_error("ERROR: couldn't allocate buffer for topology transfer!\n");
+        return -EIO;
+    }
+    memset(buffer, 0, buffer_len);
+
+    local_msg = (struct sw_driver_topology_msg *)buffer;
+    local_msg->num_entries = sw_num_topology_entries;
+    dst = (struct sw_driver_topology_change *)&local_msg->topology_entries[0];
+    SW_LIST_FOR_EACH_ENTRY(tnode, head, list) {
+        struct sw_driver_topology_change *change = &tnode->change;
+        memcpy(&dst[dst_idx++], change, sizeof(*change));
+    }
+    retVal = copy_to_user(remote_msg, local_msg, buffer_len);
+    if (retVal) {
+        pw_pr_error("ERROR: couldn't copy topology changes to user space!\n");
+    }
+    vfree(buffer);
+    return retVal;
+}
+
+
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+    #define MATCH_IOCTL(num, pred) ( (num) == (pred) || (num) == (pred##32) )
+#else
+    #define MATCH_IOCTL(num, pred) ( (num) == (pred) )
+#endif
+
+static long sw_unlocked_handle_ioctl_i(unsigned int ioctl_num,
+                                       void *p_local_args)
+{
+    struct sw_driver_ioctl_arg local_args;
+    int local_in_len, local_out_len;
+
+    if (!p_local_args) {
+        pw_pr_error("ERROR: NULL p_local_args value?!\n");
+        return -PW_ERROR;
+    }
+
+    /*
+     * (1) Sanity check:
+     * Before doing anything, double check to
+     * make sure this IOCTL was really intended
+     * for us!
+     */
+    if(_IOC_TYPE(ioctl_num) != APWR_IOCTL_MAGIC_NUM){
+        pw_pr_error("ERROR: requested IOCTL TYPE (%d) != APWR_IOCTL_MAGIC_NUM (%d)\n", _IOC_TYPE(ioctl_num), APWR_IOCTL_MAGIC_NUM);
+        return -PW_ERROR;
+    }
+    /*
+     * (2) Extract arg lengths.
+     */
+    local_args = *((struct sw_driver_ioctl_arg *)p_local_args);
+
+    local_in_len = local_args.in_len;
+    local_out_len = local_args.out_len;
+    pw_pr_debug("GU: local_in_len = %d, local_out_len = %d\n", local_in_len, local_out_len);
+    /*
+     * (3) Service individual IOCTL requests.
+     */
+    if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CONFIG)) {
+        pw_pr_debug("PW_IOCTL_CONFIG\n");
+        return sw_set_driver_infos_i((struct sw_driver_interface_msg __user*)local_args.in_arg, local_in_len);
+    }
+    else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CMD)) {
+        sw_driver_collection_cmd_t local_cmd;
+        pw_pr_debug("PW_IOCTL_CMD\n");
+        if (get_user(local_cmd, (sw_driver_collection_cmd_t __user *)local_args.in_arg)) {
+            pw_pr_error("ERROR: could NOT extract cmd value!\n");
+            return -PW_ERROR;
+        }
+        return sw_handle_cmd_i(local_cmd, (u64 __user*)local_args.out_arg);
+    }
+    else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_POLL)) {
+        pw_pr_debug("PW_IOCTL_POLL\n");
+        return DO_PER_CPU_OVERHEAD_FUNC_RET(int , sw_collection_poll_i);
+    }
+    else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_IMMEDIATE_IO)) {
+        struct sw_driver_interface_info *local_info;
+        struct sw_driver_io_descriptor *local_descriptor = NULL;
+        int retVal = PW_SUCCESS;
+        char *src_vals = NULL;
+        char *dst_vals = NULL;
+
+        pw_pr_debug("PW_IOCTL_IMMEDIATE_IO\n");
+        pw_pr_debug("local_in_len = %u\n", local_in_len);
+
+        src_vals = vmalloc(local_in_len);
+        if (!src_vals) {
+            pw_pr_error("ERROR allocating space for immediate IO\n");
+            return -PW_ERROR;
+        }
+        if (local_out_len) {
+            dst_vals = vmalloc(local_out_len);
+            if (!dst_vals) {
+                vfree(src_vals);
+                pw_pr_error("ERROR allocating space for immediate IO\n");
+                return -PW_ERROR;
+            }
+        }
+        if (copy_from_user(src_vals, (char *)local_args.in_arg, local_in_len)) {
+            pw_pr_error("ERROR copying in immediate IO descriptor\n");
+            retVal = -PW_ERROR;
+            goto ret_immediate_io;
+        }
+        local_info = (struct sw_driver_interface_info *)src_vals;
+        pw_pr_debug("OK, asked to perform immediate IO on cpu(s) %d, # descriptors = %d\n",
+            local_info->cpu_mask, local_info->num_io_descriptors);
+        /*
+         * For now, require only a single descriptor.
+         */
+        if (local_info->num_io_descriptors != 1) {
+            pw_pr_error("ERROR: told to perform immediate IO with %d descriptors -- MAX of 1 descriptor allowed!\n",
+            local_info->num_io_descriptors);
+            retVal = -PW_ERROR;
+            goto ret_immediate_io;
+        }
+        local_descriptor = ((struct sw_driver_io_descriptor *)local_info->descriptors);
+        pw_pr_debug("Collection type after %d\n", local_descriptor->collection_type);
+        /*
+         * Check cpu mask for correctness here. For now, we do NOT allow
+         * reading on ALL cpus.
+         */
+        if ((int)local_info->cpu_mask < -1 || (int)local_info->cpu_mask >= (int)sw_max_num_cpus) {
+            pw_pr_error("ERROR: invalid cpu mask %d specified in immediate IO; valid values are: -1, [0 -- %d]!\n",
+            local_info->cpu_mask, sw_max_num_cpus-1);
+            retVal = -PW_ERROR;
+            goto ret_immediate_io;
+        }
+        /*
+         * Check collection type for correctness here
+         */
+        pw_pr_debug("Asked to perform immediate IO with descriptor with type = %d, on cpu = %d\n", local_descriptor->collection_type,
+            local_info->cpu_mask);
+        if (sw_is_valid_hw_op_id(local_descriptor->collection_type) == false) {
+            pw_pr_error("ERROR: invalid collection type %d specified for immediate IO\n", (int)local_descriptor->collection_type);
+            retVal = -PW_ERROR;
+            goto ret_immediate_io;
+        }
+        /*
+         * Check collection cmd for correctness here
+         */
+        if (local_descriptor->collection_command < SW_IO_CMD_READ || local_descriptor->collection_command > SW_IO_CMD_WRITE) {
+            pw_pr_error("ERROR: invalid collection command %d specified for immediate IO\n", local_descriptor->collection_command);
+            retVal = -PW_ERROR;
+            goto ret_immediate_io;
+        }
+        /*
+         * Initialize the descriptor -- 'MMIO' and 'IPC' reads may need
+         * an "ioremap_nocache"
+         */
+        if (sw_init_driver_io_descriptor(local_descriptor)) {
+            pw_pr_error("ERROR initializing immediate IO descriptor\n");
+            retVal = -PW_ERROR;
+            goto ret_immediate_io;
+        }
+        /*
+         * OK, perform the actual IO.
+         */
+        {
+            struct sw_immediate_request_info request_info = {local_descriptor, dst_vals, &retVal};
+            struct cpumask cpumask;
+            cpumask_clear(&cpumask);
+            switch (local_info->cpu_mask) {
+                case -1: // IO on ANY CPU (assume current CPU)
+                    cpumask_set_cpu(RAW_CPU(), &cpumask);
+                    pw_pr_debug("ANY CPU\n");
+                    break;
+                default: // IO on a particular CPU
+                    cpumask_set_cpu(local_info->cpu_mask, &cpumask);
+                    pw_pr_debug("[%d] setting for %d\n", RAW_CPU(), local_info->cpu_mask);
+                    break;
+            }
+            sw_schedule_work(&cpumask, &sw_handle_immediate_request_i, &request_info);
+        }
+        if (retVal != PW_SUCCESS) {
+            pw_pr_error("ERROR performing immediate IO on one (or more) CPUs!\n");
+            goto ret_immediate_io_reset;
+        }
+        /*
+         * OK, all done.
+         */
+        if (local_descriptor->collection_command == SW_IO_CMD_READ) {
+            if (copy_to_user(local_args.out_arg, dst_vals, local_out_len)) {
+                pw_pr_error("ERROR copying %u bytes of value to userspace!\n", local_out_len);
+                retVal = -PW_ERROR;
+                goto ret_immediate_io_reset;
+            }
+            pw_pr_debug("OK, copied %u bytes of value to userspace addr %p!\n", local_out_len, local_args.out_arg);
+        }
+ret_immediate_io_reset:
+        /*
+         * Reset the descriptor -- 'MMIO' and 'IPC' reads may have
+         * performed an "ioremap_nocache" which now needs to be unmapped.
+         */
+        if (sw_reset_driver_io_descriptor(local_descriptor)) {
+            pw_pr_error("ERROR resetting immediate IO descriptor\n");
+            retVal = -PW_ERROR;
+            goto ret_immediate_io;
+        }
+ret_immediate_io:
+        vfree(src_vals);
+        if (dst_vals) {
+            vfree(dst_vals);
+        }
+        return retVal;
+    }
+    else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_SCU_FW_VERSION)) {
+        u32 local_data = (u32)sw_scu_fw_major_minor;
+        if (put_user(local_data, (u32 __user *)local_args.out_arg)) {
+            pw_pr_error("ERROR copying scu fw version to userspace!\n");
+            return -PW_ERROR;
+        }
+        return PW_SUCCESS;
+    }
+    else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_DRIVER_VERSION)) {
+        pw_u64_t local_version = (pw_u64_t)SW_DRIVER_VERSION_MAJOR << 32 | \
+                                 (pw_u64_t)SW_DRIVER_VERSION_MINOR << 16 |
+                                 (pw_u64_t)SW_DRIVER_VERSION_OTHER;
+        if (put_user(local_version, (u64 __user *)local_args.out_arg)) {
+            pw_pr_error("ERROR copying driver version to userspace!\n");
+            return -PW_ERROR;
+        }
+        return PW_SUCCESS;
+    }
+    else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_TRACEPOINTS)) {
+        pw_pr_debug("DEBUG: AVAIL tracepoints! local_out_len = %u\n", local_out_len);
+        return sw_get_available_name_id_mappings_i(SW_NAME_TYPE_TRACEPOINT, (struct sw_name_info_msg __user*)local_args.out_arg, local_out_len);
+    }
+    else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_NOTIFIERS)) {
+        pw_pr_debug("DEBUG: AVAIL tracepoints! local_out_len = %u\n", local_out_len);
+        return sw_get_available_name_id_mappings_i(SW_NAME_TYPE_NOTIFIER, (struct sw_name_info_msg __user*)local_args.out_arg, local_out_len);
+    }
+    else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_COLLECTORS)) {
+        pw_pr_debug("DEBUG: AVAIL tracepoints! local_out_len = %u\n", local_out_len);
+        return sw_get_available_name_id_mappings_i(SW_NAME_TYPE_COLLECTOR, (struct sw_name_info_msg __user*)local_args.out_arg, local_out_len);
+    }
+    else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_TOPOLOGY_CHANGES)) {
+        pw_pr_debug("DEBUG: TOPOLOGY changes! local_out_len = %u\n", local_out_len);
+        return sw_get_topology_changes_i((struct sw_driver_topology_msg __user*)local_args.out_arg, local_out_len);
+    }
+    else {
+        pw_pr_error("ERROR: invalid ioctl num: %u\n", _IOC_NR(ioctl_num));
+    }
+    return -PW_ERROR;
+}
+
+enum sw_driver_collection_cmd sw_get_collection_cmd_i(void)
+{
+    return s_internal_state.cmd;
+};
+
+bool sw_should_flush_buffer_i(void)
+{
+    return s_internal_state.drain_buffers;
+};
+
+
+int sw_load_driver_i(void)
+{
+    /*
+     * Set per-cpu buffer size.
+     * First, Perform sanity checking of per-cpu buffer size.
+     */
+    /*
+     * 1. Num pages MUST be pow-of-2.
+     */
+    {
+        if (sw_buffer_num_pages & (sw_buffer_num_pages - 1)) {
+            pw_pr_error("Invalid value (%u) for number of pages in each per-cpu buffer; MUST be a power of 2!\n", sw_buffer_num_pages);
+            return -PW_ERROR;
+        }
+    }
+    /*
+     * 2. Num pages MUST be <= 16 (i.e. per-cpu buffer size
+     * MUST be <= 64 kB)
+     */
+    {
+        if (sw_buffer_num_pages > 16) {
+            pw_pr_error("Invalid value (%u) for number of pages in each per-cpu buffer; MUST be <= 16!\n", sw_buffer_num_pages);
+            return -PW_ERROR;
+        }
+    }
+    sw_buffer_alloc_size = sw_buffer_num_pages * PAGE_SIZE;
+    /*
+     * Retrieve any arch details here.
+     */
+    if (sw_get_arch_details_i()) {
+        pw_pr_error("ERROR retrieving arch details!\n");
+        return -PW_ERROR;
+    }
+    /*
+     * Check to see if the user wants us to force
+     * software coordination of CPU frequencies.
+     */
+    if (do_force_module_scope_for_cpu_frequencies) {
+        pw_pr_force("DEBUG: FORCING MODULE SCOPE FOR CPU FREQUENCIES!\n");
+        if (sw_set_module_scope_for_cpus()) {
+            pw_pr_force("ERROR setting affected cpus\n");
+            return -PW_ERROR;
+        } else {
+            pw_pr_debug("OK, setting worked\n");
+        }
+    }
+    if (sw_init_data_structures_i()) {
+        pw_pr_error("ERROR initializing data structures!\n");
+        goto err_ret_init_data;
+    }
+    if (sw_register_dev(&s_ops)) {
+        goto err_ret_register_dev;
+    }
+    /*
+     * Retrieve a list of tracepoint structs to use when
+     * registering probe functions.
+     */
+    {
+        if (sw_extract_tracepoints()) {
+            pw_pr_error("ERROR: could NOT retrieve a complete list of valid tracepoint structs!\n");
+            goto err_ret_tracepoint;
+        }
+    }
+    pw_pr_force("-----------------------------------------\n");
+    pw_pr_force("OK: LOADED SoC Watch Driver\n");
+#ifdef CONFIG_X86_WANT_INTEL_MID
+    pw_pr_force("SOC Identifier = %u, Stepping = %u\n", intel_mid_identify_cpu(), intel_mid_soc_stepping());
+#endif // CONFIG_X86_WANT_INTEL_MID
+    pw_pr_force("-----------------------------------------\n");
+    return PW_SUCCESS;
+
+err_ret_tracepoint:
+    sw_unregister_dev();
+err_ret_register_dev:
+    sw_destroy_data_structures_i();
+err_ret_init_data:
+    if (do_force_module_scope_for_cpu_frequencies) {
+        if (sw_reset_module_scope_for_cpus()) {
+            pw_pr_force("ERROR resetting affected cpus\n");
+        } else {
+            pw_pr_debug("OK, resetting worked\n");
+        }
+    }
+    return -PW_ERROR;
+}
+
+void sw_unload_driver_i(void)
+{
+    sw_iterate_driver_info_lists_i();
+
+    sw_unregister_dev();
+
+    sw_destroy_data_structures_i();
+
+    if (do_force_module_scope_for_cpu_frequencies) {
+        if (sw_reset_module_scope_for_cpus()) {
+            pw_pr_force("ERROR resetting affected cpus\n");
+        } else {
+            pw_pr_debug("OK, resetting worked\n");
+        }
+    }
+
+    pw_pr_force("-----------------------------------------\n");
+    pw_pr_force("OK: UNLOADED SoC Watch Driver\n");
+
+    sw_print_trace_notifier_overheads();
+    sw_print_output_buffer_overheads();
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_collection_poll_i, "POLL");
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_any_seg_full, "ANY_SEG_FULL");
+#if DO_TRACK_MEMORY_USAGE
+    {
+        /*
+         * Dump memory stats.
+         */
+        pw_pr_force("TOTAL # BYTES ALLOCED = %llu, CURR # BYTES ALLOCED = %llu, MAX # BYTES ALLOCED = %llu\n",
+           sw_get_total_bytes_alloced(), sw_get_curr_bytes_alloced(), sw_get_max_bytes_alloced());
+        if (unlikely(sw_get_curr_bytes_alloced())) {
+            pw_pr_force("***********************************************************************\n");
+            pw_pr_force("WARNING: possible memory leak: there are %llu bytes still allocated!\n", sw_get_curr_bytes_alloced());
+            pw_pr_force("***********************************************************************\n");
+        }
+    }
+#endif // DO_TRACK_MEMORY_USAGE
+    pw_pr_force("-----------------------------------------\n");
+}
+
+module_init(sw_load_driver_i);
+module_exit(sw_unload_driver_i);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR(MOD_AUTHOR);
+MODULE_DESCRIPTION(MOD_DESC);
diff --git a/drivers/misc/intel/socwatch/sw_file_ops.c b/drivers/misc/intel/socwatch/sw_file_ops.c
new file mode 100644
index 000000000000..f4da19ee74cb
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_file_ops.c
@@ -0,0 +1,332 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include <linux/module.h>  // try_module_get
+#include <linux/fs.h>      // inode
+#include <linux/device.h>  // class_create
+#include <linux/cdev.h>    // cdev_alloc
+#include <linux/version.h> // LINUX_VERSION_CODE
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,12,0)
+    #include <asm/uaccess.h>   // copy_to_user
+#else
+    #include <linux/uaccess.h>   // copy_to_user
+#endif // LINUX_VERSION_CODE
+#include <linux/wait.h>    // wait_event_interruptible
+#include <linux/sched.h>   // TASK_INTERRUPTIBLE
+
+#include "sw_types.h"
+#include "sw_structs.h"
+#include "sw_file_ops.h"
+#include "sw_ioctl.h"
+#include "sw_output_buffer.h"
+
+/* -------------------------------------------------
+ * Compile time constants.
+ * -------------------------------------------------
+ */
+/*
+ * Get current command.
+ */
+#define GET_CMD() ( (*s_file_ops->get_current_cmd)() )
+/*
+ * Check if we're currently collecting data.
+ */
+#define IS_COLLECTING() ({sw_driver_collection_cmd_t __cmd = GET_CMD(); bool __val = (__cmd == SW_DRIVER_START_COLLECTION || __cmd == SW_DRIVER_RESUME_COLLECTION); __val;})
+/*
+ * Check if we're currently paused.
+ */
+#define IS_SLEEPING() ({sw_driver_collection_cmd_t __cmd = GET_CMD(); bool __val = __cmd == SW_DRIVER_PAUSE_COLLECTION; __val;})
+/* -------------------------------------------------
+ * Typedefs
+ * -------------------------------------------------
+ */
+typedef unsigned long sw_bits_t;
+
+
+/* -------------------------------------------------
+ *  Local function declarations.
+ * -------------------------------------------------
+ */
+static int sw_device_open_i(struct inode *inode, struct file *file);
+static int sw_device_release_i(struct inode *inode, struct file *file);
+static ssize_t sw_device_read_i(struct file *file, char __user * buffer, size_t length, loff_t * offset);
+static long sw_device_unlocked_ioctl_i(struct file *filp, unsigned int ioctl_num, unsigned long ioctl_param);
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+    static long sw_device_compat_ioctl_i(struct file *file, unsigned int ioctl_num, unsigned long ioctl_param);
+#endif
+
+/*
+ * File operations exported by the driver.
+ */
+static struct file_operations s_fops = {
+    .open = &sw_device_open_i,
+    .read = &sw_device_read_i,
+    .unlocked_ioctl = &sw_device_unlocked_ioctl_i,
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+    .compat_ioctl = &sw_device_compat_ioctl_i,
+#endif // COMPAT && x64
+    .release = &sw_device_release_i,
+};
+/*
+ * Character device file MAJOR
+ * number -- we're now obtaining
+ * this dynamically.
+ */
+static int apwr_dev_major_num = -1;
+/*
+ * Variables to create the character device file
+ */
+static dev_t apwr_dev;
+static struct cdev *apwr_cdev;
+static struct class *apwr_class = NULL;
+/*
+ * Operations exported by the main driver.
+ */
+static struct sw_file_ops *s_file_ops = NULL;
+/*
+ * Is the device open right now? Used to prevent
+ * concurent access into the same device.
+ */
+#define DEV_IS_OPEN 0 // see if device is in use
+static volatile sw_bits_t dev_status;
+
+/*
+ * File operations.
+ */
+/*
+ * Service an "open(...)" call from user-space.
+ */
+static int sw_device_open_i(struct inode *inode, struct file *file)
+{
+    /*
+     * We don't want to talk to two processes at the same time
+     */
+    if(test_and_set_bit(DEV_IS_OPEN, &dev_status)){
+        // Device is busy
+        return -EBUSY;
+    }
+
+    try_module_get(THIS_MODULE);
+    pw_pr_debug("OK, allowed client open!\n");
+    return PW_SUCCESS;
+}
+
+/*
+ * Service a "close(...)" call from user-space.
+ */
+static int sw_device_release_i(struct inode *inode, struct file *file)
+{
+    /*
+     * Did the client just try to zombie us?
+     */
+    int retVal = PW_SUCCESS;
+    if (IS_COLLECTING()) {
+        pw_pr_error("ERROR: Detected ongoing collection on a device release!\n");
+        retVal = (*s_file_ops->stop_handler)();
+    }
+    module_put(THIS_MODULE);
+    /*
+     * We're now ready for our next caller
+     */
+    clear_bit(DEV_IS_OPEN, &dev_status);
+    return retVal;
+}
+
+static ssize_t sw_device_read_i(struct file *file, char __user * user_buffer, size_t length, loff_t * offset)
+{
+    size_t bytes_read = 0;
+    u32 val = 0;
+
+    if (!user_buffer) {
+        pw_pr_error("ERROR: \"read\" called with an empty user_buffer?!\n");
+        return -PW_ERROR;
+    }
+    do {
+        val = SW_ALL_WRITES_DONE_MASK;
+        if (wait_event_interruptible(sw_reader_queue,
+                                     (sw_any_seg_full(&val, (*s_file_ops->should_flush)())
+                                     || (!IS_COLLECTING() && !IS_SLEEPING())))) {
+            pw_pr_error("wait_event_interruptible error\n");
+            return -ERESTARTSYS;
+        }
+        pw_pr_debug(KERN_INFO "After wait: val = %u\n", val);
+    } while (val == SW_NO_DATA_AVAIL_MASK);
+    /*
+     * Are we done producing/consuming?
+     */
+    if (val == SW_ALL_WRITES_DONE_MASK) {
+        return 0; // "0" ==> EOF
+    }
+    /*
+     * Copy the buffer contents into userspace.
+     */
+    bytes_read = sw_consume_data(val, user_buffer, length); // 'read' returns # of bytes actually read
+    if (unlikely(bytes_read == 0)) {
+        /* Cannot be EOF since that has already been checked above */
+        return -EIO;
+    }
+    return bytes_read;
+}
+
+/*
+ * (1) Handle 32b IOCTLs in 32b kernel-space.
+ * (2) Handle 64b IOCTLs in 64b kernel-space.
+ */
+static long sw_device_unlocked_ioctl_i(struct file *filp, unsigned int ioctl_num, unsigned long ioctl_param)
+{
+    struct sw_driver_ioctl_arg __user *remote_args = (struct sw_driver_ioctl_arg __user*)ioctl_param;
+    struct sw_driver_ioctl_arg local_args;
+    if (copy_from_user(&local_args, remote_args, sizeof(local_args))) {
+        pw_pr_error("ERROR copying ioctl args from userspace\n");
+        return -PW_ERROR;
+    }
+    return (*s_file_ops->ioctl_handler)(ioctl_num, &local_args);
+};
+
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#include <linux/compat.h>
+/*
+ * Helper struct for use in translating
+ * IOCTLs from 32b user programs in 64b
+ * kernels.
+ */
+#pragma pack(push, 1)
+struct sw_driver_ioctl_arg32 {
+    pw_s32_t in_len;
+    pw_s32_t out_len;
+    compat_caddr_t in_arg;
+    compat_caddr_t out_arg;
+};
+#pragma pack(pop)
+
+/*
+ * Handle 32b IOCTLs in 64b kernel-space.
+ */
+static long sw_device_compat_ioctl_i(struct file *file, unsigned int ioctl_num, unsigned long ioctl_param)
+{
+    struct sw_driver_ioctl_arg32 __user *remote_args32 = compat_ptr(ioctl_param);
+    struct sw_driver_ioctl_arg local_args;
+    u32 data;
+
+    if (get_user(local_args.in_len, &remote_args32->in_len)) {
+        return -PW_ERROR;
+    }
+    if (get_user(local_args.out_len, &remote_args32->out_len)) {
+        return -PW_ERROR;
+    }
+    if (get_user(data, &remote_args32->in_arg)) {
+        return -PW_ERROR;
+    }
+    local_args.in_arg = (char *)(unsigned long)data;
+    if (get_user(data, &remote_args32->out_arg)) {
+        return -PW_ERROR;
+    }
+    local_args.out_arg = (char *)(unsigned long)data;
+    return (*s_file_ops->ioctl_handler)(ioctl_num, &local_args);
+}
+#endif
+
+/*
+ * Device creation, deletion operations.
+ */
+int sw_register_dev(struct sw_file_ops *ops)
+{
+    int ret;
+    /*
+     * Ensure we have valid handlers!
+     */
+    if (!ops) {
+        pw_pr_error("NULL file ops?!\n");
+        return -PW_ERROR;
+    }
+
+    /*
+     * Create the character device
+     */
+    ret = alloc_chrdev_region(&apwr_dev, 0, 1, PW_DEVICE_NAME);
+    apwr_dev_major_num = MAJOR(apwr_dev);
+    apwr_class = class_create(THIS_MODULE, "apwr");
+    if (IS_ERR(apwr_class)) {
+        printk(KERN_ERR "Error registering apwr class\n");
+    }
+
+    device_create(apwr_class, NULL, apwr_dev, NULL, PW_DEVICE_NAME);
+    apwr_cdev = cdev_alloc();
+    if (apwr_cdev == NULL) {
+        printk("Error allocating character device\n");
+        return ret;
+    }
+    apwr_cdev->owner = THIS_MODULE;
+    apwr_cdev->ops = &s_fops;
+    if (cdev_add(apwr_cdev, apwr_dev, 1) < 0 )  {
+        printk("Error registering device driver\n");
+        return ret;
+    }
+    s_file_ops = ops;
+
+    return ret;
+}
+
+void sw_unregister_dev(void)
+{
+    /*
+     * Remove the device
+     */
+    unregister_chrdev(apwr_dev_major_num, PW_DEVICE_NAME);
+    device_destroy(apwr_class, apwr_dev);
+    class_destroy(apwr_class);
+    unregister_chrdev_region(apwr_dev, 1);
+    cdev_del(apwr_cdev);
+}
diff --git a/drivers/misc/intel/socwatch/sw_hardware_io.c b/drivers/misc/intel/socwatch/sw_hardware_io.c
new file mode 100644
index 000000000000..fbf1fe719fdb
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_hardware_io.c
@@ -0,0 +1,176 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#include "sw_types.h"
+#include "sw_defines.h"
+#include "sw_ops_provider.h"
+#include "sw_mem.h"
+#include "sw_internal.h"
+#include "sw_hardware_io.h"
+
+
+struct sw_ops_node {
+    const struct sw_hw_ops *op;
+    int id;
+    SW_LIST_ENTRY(list, sw_ops_node);
+};
+
+static SW_DEFINE_LIST_HEAD(s_ops, sw_ops_node) = SW_LIST_HEAD_INITIALIZER(s_ops);
+
+static int s_op_idx = -1;
+
+/*
+ * Function definitions.
+ */
+int sw_get_hw_op_id(const struct sw_hw_ops *ops)
+{
+    if (ops && ops->name) {
+        struct sw_ops_node *node = NULL;
+        SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
+            if (node->op->name && !strcmp(node->op->name, ops->name)) {
+                return node->id;
+            }
+        }
+    }
+    return -1;
+}
+
+const struct sw_hw_ops *sw_get_hw_ops_for(int id)
+{
+    struct sw_ops_node *node = NULL;
+    SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
+        if (node->id == id) {
+            return node->op;
+        }
+    }
+    return NULL;
+}
+
+bool sw_is_valid_hw_op_id(int id)
+{
+    struct sw_ops_node *node = NULL;
+    SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
+        if (node->id == id) {
+            return true;
+        }
+    }
+    return false;
+}
+
+const char *sw_get_hw_op_abstract_name(const struct sw_hw_ops *op)
+{
+    if (op) {
+        return op->name;
+    }
+    return NULL;
+}
+
+int sw_for_each_hw_op(int (*func)(const struct sw_hw_ops *op, void *priv), void *priv, bool return_on_error)
+{
+    int retval = PW_SUCCESS;
+    struct sw_ops_node *node = NULL;
+    if (func) {
+        SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
+            if ((*func)(node->op, priv)) {
+                retval = -EIO;
+                if (return_on_error) {
+                    break;
+                }
+            }
+        }
+    }
+    return retval;
+}
+
+int sw_register_hw_op(const struct sw_hw_ops *op)
+{
+    struct sw_ops_node *node = NULL;
+    if (!op) {
+        pw_pr_error("NULL input node in \"sw_register_hw_op\"");
+        return -EIO;
+    }
+    node = sw_kmalloc(sizeof(struct sw_ops_node), GFP_KERNEL);
+    if (!node) {
+        pw_pr_error("sw_kmalloc error in \"sw_register_hw_op\"");
+        return -ENOMEM;
+    }
+    node->op = op;
+    node->id = ++s_op_idx;
+    SW_LIST_ENTRY_INIT(node, list);
+    SW_LIST_ADD(&s_ops, node, list);
+    return PW_SUCCESS;
+}
+
+int sw_register_hw_ops(void)
+{
+    return sw_register_ops_providers();
+}
+
+void sw_free_hw_ops(void)
+{
+    /*
+     * Free all nodes.
+     */
+    while (!SW_LIST_EMPTY(&s_ops)) {
+        struct sw_ops_node *node = SW_LIST_GET_HEAD_ENTRY(&s_ops, sw_ops_node, list);
+        SW_LIST_UNLINK(node, list);
+        sw_kfree(node);
+    }
+    /*
+     * Call our providers to deallocate resources.
+     */
+    sw_free_ops_providers();
+}
diff --git a/drivers/misc/intel/socwatch/sw_internal.c b/drivers/misc/intel/socwatch/sw_internal.c
new file mode 100644
index 000000000000..72f807512044
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_internal.c
@@ -0,0 +1,225 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include "sw_hardware_io.h"
+#include "sw_mem.h"
+#include "sw_internal.h"
+
+bool sw_check_output_buffer_params(void *buffer, size_t bytes_to_read, size_t buff_size)
+{
+    if (!buffer) {
+        pw_pr_error("ERROR: NULL ptr in sw_consume_data!\n");
+        return false;
+    }
+    if (bytes_to_read != buff_size) {
+        pw_pr_error("Error: bytes_to_read = %zu, required to be %zu\n", bytes_to_read, buff_size);
+        return false;
+    }
+    return true;
+}
+
+unsigned long sw_copy_to_user(void *dst, char *src, size_t bytes_to_copy)
+{
+    return copy_to_user((char __user *)dst, src, bytes_to_copy);
+}
+
+void sw_schedule_work(const struct cpumask *mask, void (*work)(void *), void *data)
+{
+    /*
+     * Did the user ask us to run on 'ANY' CPU?
+     */
+    if (cpumask_empty(mask)) {
+        (*work)(data); // Call on current CPU
+    } else {
+        preempt_disable();
+        {
+            /*
+             * Did the user ask to run on this CPU?
+             */
+            if (cpumask_test_cpu(RAW_CPU(), mask)) {
+                (*work)(data); // Call on current CPU
+            }
+            /*
+             * OK, now check other CPUs.
+             */
+            smp_call_function_many(mask, work, data, true/* Wait for all funcs to complete */);
+        }
+        preempt_enable();
+    }
+}
+
+int sw_get_cpu(unsigned long *flags)
+{
+    local_irq_save(*flags);
+    return get_cpu();
+}
+
+void sw_put_cpu(unsigned long flags)
+{
+    put_cpu();
+    local_irq_restore(flags);
+}
+
+#ifndef CONFIG_NR_CPUS_PER_MODULE
+    #define CONFIG_NR_CPUS_PER_MODULE 2
+#endif // CONFIG_NR_CPUS_PER_MODULE
+
+static void sw_get_cpu_sibling_mask(int cpu, struct cpumask *sibling_mask)
+{
+    unsigned int base = (cpu/CONFIG_NR_CPUS_PER_MODULE) * CONFIG_NR_CPUS_PER_MODULE;
+    unsigned int i;
+
+    cpumask_clear(sibling_mask);
+    for (i=base; i<(base+CONFIG_NR_CPUS_PER_MODULE); ++i) {
+        cpumask_set_cpu(i, sibling_mask);
+    }
+}
+
+struct pw_cpufreq_node {
+    int cpu;
+    struct cpumask cpus, related_cpus;
+    unsigned int shared_type;
+    struct list_head list;
+};
+static struct list_head pw_cpufreq_policy_lists;
+
+int sw_set_module_scope_for_cpus(void)
+{
+    /*
+     * Warning: no support for cpu hotplugging!
+     */
+    int cpu=0;
+    INIT_LIST_HEAD(&pw_cpufreq_policy_lists);
+    for_each_online_cpu(cpu) {
+        struct cpumask sibling_mask;
+        struct pw_cpufreq_node *node = NULL;
+        struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+
+        if (!policy) {
+            continue;
+        }
+        /*
+         * Get siblings for this cpu.
+         */
+        sw_get_cpu_sibling_mask(cpu, &sibling_mask);
+        /*
+         * Check if affected_cpus already contains sibling_mask
+         */
+        if (cpumask_subset(&sibling_mask, policy->cpus)) {
+            /*
+             * 'sibling_mask' is already a subset of affected_cpus -- nothing
+             * to do on this CPU.
+             */
+            cpufreq_cpu_put(policy);
+            continue;
+        }
+
+        node = sw_kmalloc(sizeof(*node), GFP_ATOMIC);
+        if (node) {
+            cpumask_clear(&node->cpus); cpumask_clear(&node->related_cpus);
+
+            node->cpu = cpu;
+            cpumask_copy(&node->cpus, policy->cpus);
+            cpumask_copy(&node->related_cpus, policy->related_cpus);
+            node->shared_type = policy->shared_type;
+        }
+
+        policy->shared_type = CPUFREQ_SHARED_TYPE_ALL;
+        /*
+         * Set siblings. Don't worry about online/offline, that's
+         * handled below.
+         */
+        cpumask_copy(policy->cpus, &sibling_mask);
+        /*
+         * Ensure 'related_cpus' is a superset of 'cpus'
+         */
+        cpumask_or(policy->related_cpus, policy->related_cpus, policy->cpus);
+        /*
+         * Ensure 'cpus' only contains online cpus.
+         */
+        cpumask_and(policy->cpus, policy->cpus, cpu_online_mask);
+
+        cpufreq_cpu_put(policy);
+
+        if (node) {
+            INIT_LIST_HEAD(&node->list);
+            list_add_tail(&node->list, &pw_cpufreq_policy_lists);
+        }
+    }
+    return PW_SUCCESS;
+}
+
+int sw_reset_module_scope_for_cpus(void)
+{
+    struct list_head *head = &pw_cpufreq_policy_lists;
+    while (!list_empty(head)) {
+        struct pw_cpufreq_node *node = list_first_entry(head, struct pw_cpufreq_node, list);
+        int cpu = node->cpu;
+        struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+        if (!policy) {
+            continue;
+        }
+        policy->shared_type = node->shared_type;
+        cpumask_copy(policy->related_cpus, &node->related_cpus);
+        cpumask_copy(policy->cpus, &node->cpus);
+
+        cpufreq_cpu_put(policy);
+
+        pw_pr_debug("OK, reset cpufreq_policy for cpu %d\n", cpu);
+        list_del(&node->list);
+        sw_kfree(node);
+    }
+    return PW_SUCCESS;
+}
diff --git a/drivers/misc/intel/socwatch/sw_mem.c b/drivers/misc/intel/socwatch/sw_mem.c
new file mode 100644
index 000000000000..083192b0c39f
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_mem.c
@@ -0,0 +1,299 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include <linux/slab.h>
+
+#include "sw_defines.h"
+#include "sw_lock_defs.h"
+#include "sw_mem.h"
+
+/*
+ * How do we behave if we ever
+ * get an allocation error?
+ * (a) Setting to '1' REFUSES ANY FURTHER
+ * allocation requests.
+ * (b) Setting to '0' treats each
+ * allocation request as separate, and
+ * handles them on an on-demand basis
+ */
+#define DO_MEM_PANIC_ON_ALLOC_ERROR 0
+
+#if DO_MEM_PANIC_ON_ALLOC_ERROR
+/*
+ * If we ever run into memory allocation errors then
+ * stop (and drop) everything.
+ */
+static atomic_t pw_mem_should_panic = ATOMIC_INIT(0);
+/*
+ * Macro to check if PANIC is on.
+ */
+#define MEM_PANIC() do{ atomic_set(&pw_mem_should_panic, 1); smp_mb(); }while(0)
+#define SHOULD_TRACE() ({bool __tmp = false; smp_mb(); __tmp = (atomic_read(&pw_mem_should_panic) == 0); __tmp;})
+
+#else // if !DO_MEM_PANIC_ON_ALLOC_ERROR
+
+#define MEM_PANIC()
+#define SHOULD_TRACE() (true)
+
+#endif
+
+/*
+ * Variables to track memory usage.
+ */
+/*
+ * TOTAL num bytes allocated.
+ */
+static u64 total_num_bytes_alloced = 0;
+/*
+ * Num of allocated bytes that have
+ * not yet been freed.
+ */
+static u64 curr_num_bytes_alloced = 0;
+/*
+ * Max # of allocated bytes that
+ * have not been freed at any point
+ * in time.
+ */
+static u64 max_num_bytes_alloced = 0;
+
+u64 sw_get_total_bytes_alloced(void)
+{
+    return total_num_bytes_alloced;
+};
+
+u64 sw_get_max_bytes_alloced(void)
+{
+    return max_num_bytes_alloced;
+};
+
+u64 sw_get_curr_bytes_alloced(void)
+{
+    return curr_num_bytes_alloced;
+};
+
+/*
+ * Allocate free pages.
+ * TODO: add memory tracker?
+ */
+unsigned long sw_allocate_pages(unsigned int flags, unsigned int alloc_size_in_bytes)
+{
+    return __get_free_pages((gfp_t)flags, get_order(alloc_size_in_bytes));
+}
+/*
+ * Free up previously allocated pages.
+ * TODO: add memory tracker?
+ */
+void sw_release_pages(unsigned long addr, unsigned int alloc_size_in_bytes)
+{
+    free_pages(addr, get_order(alloc_size_in_bytes));
+}
+
+#if DO_TRACK_MEMORY_USAGE
+
+/*
+ * Lock to guard access to memory
+ * debugging stats.
+ */
+static SW_DEFINE_SPINLOCK(sw_kmalloc_lock);
+
+/*
+ * Helper macros to print out
+ * mem debugging stats.
+ */
+#define TOTAL_NUM_BYTES_ALLOCED() total_num_bytes_alloced
+#define CURR_NUM_BYTES_ALLOCED() curr_num_bytes_alloced
+#define MAX_NUM_BYTES_ALLOCED() max_num_bytes_alloced
+
+/*
+ * MAGIC number based memory tracker. Relies on
+ * storing (a) a MAGIC marker and (b) the requested
+ * size WITHIN the allocated block of memory. Standard
+ * malloc-tracking stuff, really.
+ *
+ * Overview:
+ * (1) ALLOCATION:
+ * When asked to allocate a block of 'X' bytes, allocate
+ * 'X' + 8 bytes. Then, in the FIRST 4 bytes, write the
+ * requested size. In the NEXT 4 bytes, write a special
+ * (i.e. MAGIC) number to let our deallocator know that
+ * this block of memory was allocated using this technique.
+ * Also, keep track of the number of bytes allocated.
+ *
+ * (2) DEALLOCATION:
+ * When given an object to deallocate, we first check
+ * the MAGIC number by decrementing the pointer by
+ * 4 bytes and reading the (integer) stored there.
+ * After ensuring the pointer was, in fact, allocated
+ * by us, we then read the size of the allocated
+ * block (again, by decrementing the pointer by 4
+ * bytes and reading the integer size). We
+ * use this size argument to decrement # of bytes
+ * allocated.
+ */
+#define PW_MEM_MAGIC 0xdeadbeef
+
+#define PW_ADD_MAGIC(x) ({char *__tmp1 = (char *)(x); *((int *)__tmp1) = PW_MEM_MAGIC; __tmp1 += sizeof(int); __tmp1;})
+#define PW_ADD_SIZE(x,s) ({char *__tmp1 = (char *)(x); *((int *)__tmp1) = (s); __tmp1 += sizeof(int); __tmp1;})
+#define PW_ADD_STAMP(x,s) PW_ADD_MAGIC(PW_ADD_SIZE((x), (s)))
+
+#define PW_IS_MAGIC(x) ({int *__tmp1 = (int *)((char *)(x) - sizeof(int)); *__tmp1 == PW_MEM_MAGIC;})
+#define PW_REMOVE_STAMP(x) ({char *__tmp1 = (char *)(x); __tmp1 -= sizeof(int) * 2; __tmp1;})
+#define PW_GET_SIZE(x) (*((int *)(x)))
+
+void *sw_kmalloc(size_t size, unsigned flags)
+{
+    size_t act_size = 0;
+    void *retVal = NULL;
+    /*
+     * No point in allocating if
+     * we were unable to allocate
+     * previously!
+     */
+    {
+        if (!SHOULD_TRACE()) {
+            return NULL;
+        }
+    }
+    /*
+     * (1) Allocate requested block.
+     */
+    act_size = size + sizeof(int) * 2;
+    retVal = kmalloc(act_size, (gfp_t)flags);
+    if (!retVal) {
+        /*
+         * Panic if we couldn't allocate
+         * requested memory.
+         */
+        printk(KERN_INFO "ERROR: could NOT allocate memory!\n");
+        MEM_PANIC();
+        return NULL;
+    }
+    /*
+     * (2) Update memory usage stats.
+     */
+    LOCK(sw_kmalloc_lock);
+    {
+        total_num_bytes_alloced += size;
+        curr_num_bytes_alloced += size;
+        if (curr_num_bytes_alloced > max_num_bytes_alloced)
+            max_num_bytes_alloced = curr_num_bytes_alloced;
+    }
+    UNLOCK(sw_kmalloc_lock);
+    /*
+     * (3) And finally, add the 'size'
+     * and 'magic' stamps.
+     */
+    return PW_ADD_STAMP(retVal, size);
+};
+
+void sw_kfree(const void *obj)
+{
+    void *tmp = NULL;
+    size_t size = 0;
+
+    /*
+     * (1) Check if this block was allocated
+     * by us.
+     */
+    if (!PW_IS_MAGIC(obj)) {
+        printk(KERN_INFO "ERROR: %p is NOT a PW_MAGIC ptr!\n", obj);
+        return;
+    }
+    /*
+     * (2) Strip the magic num...
+     */
+    tmp = PW_REMOVE_STAMP(obj);
+    /*
+     * ...and retrieve size of block.
+     */
+    size = PW_GET_SIZE(tmp);
+    /*
+     * (3) Update memory usage stats.
+     */
+    LOCK(sw_kmalloc_lock);
+    {
+        curr_num_bytes_alloced -= size;
+    }
+    UNLOCK(sw_kmalloc_lock);
+    /*
+     * And finally, free the block.
+     */
+    kfree(tmp);
+};
+
+#else // !DO_TRACK_MEMORY_USAGE
+
+void *sw_kmalloc(size_t size, unsigned flags)
+{
+    void *ret = NULL;
+
+    if (SHOULD_TRACE()) {
+        if (!(ret = kmalloc(size, (gfp_t)flags))) {
+            /*
+             * Panic if we couldn't allocate
+             * requested memory.
+             */
+            MEM_PANIC();
+        }
+    }
+    return ret;
+};
+
+void sw_kfree(const void *mem)
+{
+    kfree(mem);
+};
+
+#endif // DO_TRACK_MEMORY_USAGE
diff --git a/drivers/misc/intel/socwatch/sw_ops_provider.c b/drivers/misc/intel/socwatch/sw_ops_provider.c
new file mode 100644
index 000000000000..b65da1e2716c
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_ops_provider.c
@@ -0,0 +1,886 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/pci.h> // "pci_get_domain_bus_and_slot"
+#include <linux/delay.h> // "udelay"
+#include <asm/msr.h>
+#ifdef CONFIG_RPMSG_IPC
+    #include <asm/intel_mid_rpmsg.h>
+#endif // CONFIG_RPMSG_IPC
+
+#include "sw_types.h"
+#include "sw_defines.h"
+#include "sw_hardware_io.h"
+#include "sw_telem.h"
+#include "sw_ops_provider.h"
+
+/*
+ * Compile time constants.
+ */
+/*
+ * Should we be doing 'direct' PCI reads and writes?
+ * '1' ==> YES, call "pci_{read,write}_config_dword()" directly
+ * '0' ==> NO, Use the "intel_mid_msgbus_{read32,write32}_raw()" API (defined in 'intel_mid_pcihelpers.c')
+ */
+#define DO_DIRECT_PCI_READ_WRITE 0
+#if !DO_ANDROID || !defined(CONFIG_X86_WANT_INTEL_MID)
+    /*
+     * 'intel_mid_pcihelpers.h' is probably not present -- force
+     * direct PCI calls in this case.
+     */
+    #undef DO_DIRECT_PCI_READ_WRITE
+    #define DO_DIRECT_PCI_READ_WRITE  1
+#endif
+#if !DO_DIRECT_PCI_READ_WRITE
+    #include <asm/intel_mid_pcihelpers.h>
+#endif
+
+#define SW_PCI_MSG_CTRL_REG 0x000000D0
+#define SW_PCI_MSG_DATA_REG 0x000000D4
+
+/*
+ *  NUM_RETRY & USEC_DELAY are used in PCH Mailbox (sw_read_pch_mailbox_info_i).
+ *  Tested on KBL + SPT-LP. May need to revisit.
+ */
+#define NUM_RETRY  100
+#define USEC_DELAY 100
+
+#define EXTCNF_CTRL 0xF00 // offset for hw semaphore.
+#define FWSM_CTRL 0x5B54 // offset for fw semaphore
+#define GBE_CTRL_OFFSET 0x34 // GBE LPM offset
+
+#define IS_HW_SEMAPHORE_SET(data) (data & (pw_u64_t)(0x1 << 6))
+#define IS_FW_SEMAPHORE_SET(data) (data & (pw_u64_t)0x1)
+
+/*
+ * Local data structures.
+ */
+/*
+ * TODO: separate into H/W and S/W IO?
+ */
+typedef enum sw_io_type {
+    SW_IO_MSR        = 0,
+    SW_IO_IPC        = 1,
+    SW_IO_MMIO       = 2,
+    SW_IO_PCI        = 3,
+    SW_IO_CONFIGDB   = 4,
+    SW_IO_TRACE_ARGS = 5,
+    SW_IO_WAKEUP     = 6,
+    SW_IO_SOCPERF    = 7,
+    SW_IO_PROC_NAME  = 8,
+    SW_IO_IRQ_NAME   = 9,
+    SW_IO_WAKELOCK   =10,
+    SW_IO_TELEM      =11,
+    SW_IO_PCH_MAILBOX=12,
+    SW_IO_MAILBOX    =13,
+    SW_IO_MAX        =14,
+} sw_io_type_t;
+
+/*
+ * "io_remapped" values for HW and FW semaphores
+ */
+static struct {
+    volatile void *hw_semaphore;
+    volatile void *fw_semaphore;
+} s_gbe_semaphore = {0,0};
+
+/*
+ * Function declarations.
+ */
+/*
+ * Exported by the SOCPERF driver.
+ */
+extern void SOCPERF_Read_Data2(void *data_buffer);
+
+/*
+ * Init functions.
+ */
+int sw_ipc_mmio_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor);
+int sw_pch_mailbox_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor);
+int sw_mailbox_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor);
+
+/*
+ * Read functions.
+ */
+void sw_read_msr_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_ipc_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_mmio_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_pch_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_pci_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_configdb_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_socperf_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+
+/*
+ * Write functions.
+ */
+void sw_write_msr_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_ipc_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_mmio_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_pci_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_configdb_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_trace_args_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_wakeup_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_socperf_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+
+/*
+ * Print functions.
+ */
+int sw_print_msr_io_descriptor(const struct sw_driver_io_descriptor *descriptor);
+
+/*
+ * Reset functions -- equal but opposite of init.
+ */
+int sw_ipc_mmio_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor);
+int sw_pch_mailbox_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor);
+int sw_mailbox_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor);
+
+/*
+ * Available functions.
+ */
+bool sw_socperf_available_i(void);
+
+/*
+ * Helper functions.
+ */
+u32 sw_platform_configdb_read32(u32 address);
+u32 sw_platform_pci_read32(u32 bus, u32 device, u32 function, u32 ctrl_offset, u32 ctrl_value, u32 data_offset);
+u64 sw_platform_pci_read64(u32 bus, u32 device, u32 function, u32 ctrl_offset, u32 ctrl_value, u32 data_offset);
+bool sw_platform_pci_write32(u32 bus, u32 device, u32 function, u32 write_offset, u32 data_value);
+
+/*
+ * Table of collector operations.
+ */
+static const struct sw_hw_ops s_hw_ops[] = {
+    [SW_IO_MSR] = {
+        .name = "MSR",
+        .init = NULL,
+        .read = &sw_read_msr_info_i,
+        .write = &sw_write_msr_info_i,
+        .print = &sw_print_msr_io_descriptor,
+        .reset = NULL,
+        .available = NULL
+    },
+    [SW_IO_IPC] = {
+        .name = "IPC",
+        .init = &sw_ipc_mmio_descriptor_init_func_i,
+        .read = &sw_read_ipc_info_i,
+        .reset = &sw_ipc_mmio_descriptor_reset_func_i,
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_MMIO] = {
+        .name = "MMIO",
+        .init = &sw_ipc_mmio_descriptor_init_func_i,
+        .read = &sw_read_mmio_info_i,
+        .write = &sw_write_mmio_info_i,
+        .reset = &sw_ipc_mmio_descriptor_reset_func_i,
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_PCI] = {
+        .name = "PCI",
+        .read = &sw_read_pci_info_i,
+        .write = &sw_write_pci_info_i,
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_CONFIGDB] = {
+        .name = "CONFIGDB",
+        .read = &sw_read_configdb_info_i,
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_WAKEUP] = {
+        .name = "WAKEUP",
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_SOCPERF] = {
+        .name = "SOCPERF",
+        .read = &sw_read_socperf_info_i,
+        .available = &sw_socperf_available_i,
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_PROC_NAME] = {
+        .name = "PROC-NAME",
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_IRQ_NAME] = {
+        .name = "IRQ-NAME",
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_WAKELOCK] = {
+        .name = "WAKELOCK",
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_TELEM] = {
+        .name = "TELEM",
+        .init = &sw_telem_init_func,
+        .read = &sw_read_telem_info,
+        .reset = &sw_reset_telem,
+        .available = &sw_telem_available,
+        .post_config = &sw_telem_post_config,
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_PCH_MAILBOX] = {
+        .name = "PCH-MAILBOX",
+        .init = &sw_pch_mailbox_descriptor_init_func_i,
+        .read = &sw_read_pch_mailbox_info_i,
+        .reset = &sw_pch_mailbox_descriptor_reset_func_i,
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_MAILBOX] = {
+        .name = "MAILBOX",
+        .init = &sw_mailbox_descriptor_init_func_i,
+        .read = &sw_read_mailbox_info_i,
+        .write = &sw_write_mailbox_info_i,
+        .reset = &sw_mailbox_descriptor_reset_func_i,
+        /* Other fields are don't care (will be set to NULL) */
+    },
+    [SW_IO_MAX] = {
+        .name = NULL,
+        /* Other fields are don't care (will be set to NULL) */
+    }
+};
+
+/*
+ * Function definitions.
+ */
+int sw_ipc_mmio_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor)
+{
+    // Perform any required 'io_remap' calls here
+    struct sw_driver_ipc_mmio_io_descriptor *__ipc_mmio = NULL;
+    u64 data_address = 0;
+    if (!descriptor) { // Should NEVER happen
+        return -PW_ERROR;
+    }
+    if (descriptor->collection_type == SW_IO_IPC) {
+        __ipc_mmio = &descriptor->ipc_descriptor;
+    } else {
+        __ipc_mmio = &descriptor->mmio_descriptor;
+    }
+    pw_pr_debug("cmd = %u, sub-cmd = %u, data_addr = 0x%llx\n", __ipc_mmio->command, __ipc_mmio->sub_command, __ipc_mmio->data_address);
+    data_address = __ipc_mmio->data_address;
+    /*
+    if (__ipc_mmio->command || __ipc_mmio->sub_command) {
+        __ipc_mmio->ipc_command = ((pw_u32_t)__ipc_mmio->sub_command << 12) | (pw_u32_t)__ipc_mmio->command;
+    }
+    */
+    if (data_address) {
+        __ipc_mmio->data_remapped_address = (pw_u64_t)(unsigned long)ioremap_nocache((unsigned long)data_address, descriptor->counter_size_in_bytes);
+        if ((void *)(unsigned long)__ipc_mmio->data_remapped_address == NULL) {
+            return -EIO;
+        }
+        pw_pr_debug("mapped addr 0x%llx\n", __ipc_mmio->data_remapped_address);
+        if (__ipc_mmio->is_gbe) {
+            if (!s_gbe_semaphore.hw_semaphore || !s_gbe_semaphore.fw_semaphore) {
+                pw_pr_debug("Initializing GBE semaphore\n");
+                if (data_address >= GBE_CTRL_OFFSET) {
+                    u64 hw_addr = (data_address - GBE_CTRL_OFFSET) + EXTCNF_CTRL;
+                    u64 fw_addr = (data_address - GBE_CTRL_OFFSET) + FWSM_CTRL;
+                    s_gbe_semaphore.hw_semaphore = (volatile void *)ioremap_nocache((unsigned long)hw_addr, descriptor->counter_size_in_bytes);
+                    s_gbe_semaphore.fw_semaphore = (volatile void *)ioremap_nocache((unsigned long)fw_addr, descriptor->counter_size_in_bytes);
+                    if (s_gbe_semaphore.hw_semaphore == NULL || s_gbe_semaphore.fw_semaphore == NULL) {
+                        pw_pr_error("couldn't mmap hw/fw semaphores for GBE MMIO op!\n");
+                        return -EIO;
+                    }
+                    pw_pr_debug("GBE has hw_sem = 0x%llx, fw_sem = 0x%llx, size = %u\n",
+                            s_gbe_semaphore.hw_semaphore, s_gbe_semaphore.fw_semaphore, descriptor->counter_size_in_bytes);
+                }
+            }
+        }
+    }
+    return PW_SUCCESS;
+}
+
+int sw_pch_mailbox_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor)
+{
+    // Perform any required 'io_remap' calls here
+    struct sw_driver_pch_mailbox_io_descriptor *__pch_mailbox = NULL;
+    if (!descriptor) { // Should NEVER happen
+        return -PW_ERROR;
+    }
+    __pch_mailbox = &descriptor->pch_mailbox_descriptor;
+    pw_pr_debug("cmd = %u, sub-cmd = %u, data_addr = 0x%llx\n", __pch_mailbox->command, __pch_mailbox->sub_command, __pch_mailbox->data_address);
+    if (__pch_mailbox->mtpmc_address) {
+        __pch_mailbox->mtpmc_remapped_address = (pw_u64_t)(unsigned long)ioremap_nocache((unsigned long)__pch_mailbox->mtpmc_address, descriptor->counter_size_in_bytes);
+        if ((void *)(unsigned long)__pch_mailbox->mtpmc_remapped_address == NULL) {
+            return -PW_ERROR;
+        }
+        pw_pr_debug("mtpmc_mapped addr 0x%llx\n", __pch_mailbox->mtpmc_remapped_address);
+    }
+    if (__pch_mailbox->msg_full_sts_address) {
+        __pch_mailbox->msg_full_sts_remapped_address = (pw_u64_t)(unsigned long)ioremap_nocache((unsigned long)__pch_mailbox->msg_full_sts_address, descriptor->counter_size_in_bytes);
+        if ((void *)(unsigned long)__pch_mailbox->msg_full_sts_remapped_address == NULL) {
+            return -PW_ERROR;
+        }
+        pw_pr_debug("msg_full_sts_mapped addr 0x%llx\n", __pch_mailbox->msg_full_sts_address);
+    }
+    if (__pch_mailbox->mfpmc_address) {
+        __pch_mailbox->mfpmc_remapped_address = (pw_u64_t)(unsigned long)ioremap_nocache((unsigned long)__pch_mailbox->mfpmc_address, descriptor->counter_size_in_bytes);
+        if ((void *)(unsigned long)__pch_mailbox->mfpmc_remapped_address == NULL) {
+            return -PW_ERROR;
+        }
+        pw_pr_debug("mfpmc_mapped addr 0x%llx\n", __pch_mailbox->mfpmc_remapped_address);
+    }
+    return PW_SUCCESS;
+}
+
+int sw_mailbox_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor)
+{
+    // Perform any required 'io_remap' calls here
+    struct sw_driver_mailbox_io_descriptor *__mailbox = NULL;
+    if (!descriptor) { // Should NEVER happen
+        return -PW_ERROR;
+    }
+    __mailbox = &descriptor->mailbox_descriptor;
+
+    pw_pr_debug("type = %u, interface_address = 0x%llx, data_address = 0x%llx\n", __mailbox->is_msr_type, __mailbox->interface_address, __mailbox->data_address);
+
+    if (!__mailbox->is_msr_type) {
+        if (__mailbox->interface_address) {
+            __mailbox->interface_remapped_address = (pw_u64_t)(unsigned long)ioremap_nocache((unsigned long)__mailbox->interface_address, descriptor->counter_size_in_bytes);
+            if ((void *)(unsigned long)__mailbox->interface_remapped_address == NULL) {
+                pw_pr_error("Couldn't iomap interface_address = 0x%llx\n", __mailbox->interface_address);
+                return -PW_ERROR;
+            }
+        }
+        if (__mailbox->data_address) {
+            __mailbox->data_remapped_address = (pw_u64_t)(unsigned long)ioremap_nocache((unsigned long)__mailbox->data_address, descriptor->counter_size_in_bytes);
+            if ((void *)(unsigned long)__mailbox->data_remapped_address == NULL) {
+                pw_pr_error("Couldn't iomap data_address = 0x%llx\n", __mailbox->data_address);
+                return -PW_ERROR;
+            }
+        }
+        pw_pr_debug("OK, mapped addr 0x%llx, 0x%llx\n", __mailbox->interface_remapped_address, __mailbox->data_remapped_address);
+    }
+    return PW_SUCCESS;
+}
+
+void sw_read_msr_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptors, u16 counter_size_in_bytes)
+{
+    u64 address = descriptors->msr_descriptor.address;
+    u32 l=0, h=0;
+
+    if (likely(cpu == RAW_CPU())) {
+        rdmsr_safe((unsigned long)address, &l, &h);
+    } else {
+        rdmsr_safe_on_cpu(cpu, (unsigned long)address, &l, &h);
+    }
+    switch (counter_size_in_bytes) {
+    case 4:
+        *((u32 *)dst_vals) = l;
+        break;
+    case 8:
+        *((u64 *)dst_vals) = ((u64)h << 32) | l;
+        break;
+    default:
+        break;
+    }
+    return;
+}
+
+#ifdef CONFIG_RPMSG_IPC
+#    define SW_DO_IPC(cmd, sub_cmd) rpmsg_send_generic_simple_command(cmd, sub_cmd)
+#else
+#    define SW_DO_IPC(cmd, sub_cmd) (-ENODEV)
+#endif // CONFIG_RPMSG_IPC
+
+void sw_read_ipc_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptors, u16 counter_size_in_bytes)
+{
+    u16 cmd = descriptors->ipc_descriptor.command, sub_cmd = descriptors->ipc_descriptor.sub_command;
+    unsigned long remapped_address = (unsigned long)descriptors->ipc_descriptor.data_remapped_address;
+
+    if (cmd || sub_cmd) {
+        pw_pr_debug("EXECUTING IPC Cmd = %u, %u\n", cmd, sub_cmd);
+        if (SW_DO_IPC(cmd, sub_cmd)) {
+            pw_pr_error("ERROR running IPC command(s)\n");
+            return;
+        }
+    }
+
+    if (remapped_address) {
+        // memcpy(&value, (void *)remapped_address, counter_size_in_bytes);
+        pw_pr_debug("COPYING MMIO size %u\n", counter_size_in_bytes);
+        memcpy(dst_vals, (void *)remapped_address, counter_size_in_bytes);
+    }
+    pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
+}
+
+static void sw_read_gbe_mmio_info_i(char *dst_vals, const struct sw_driver_io_descriptor *descriptors, u16 counter_size_in_bytes)
+{
+    u32 hw_val=0, fw_val=0;
+    unsigned long remapped_address = (unsigned long)descriptors->mmio_descriptor.data_remapped_address;
+    u64 write_value = descriptors->write_value;
+
+    memset(dst_vals, 0, counter_size_in_bytes);
+
+    pw_pr_debug("hw_sem = 0x%llx, fw_sem = 0x%llx, addr = 0x%lx, dst_vals = 0x%lx, size = %u\n",
+            s_gbe_semaphore.hw_semaphore, s_gbe_semaphore.fw_semaphore, remapped_address, (unsigned long)dst_vals, counter_size_in_bytes);
+    if (!s_gbe_semaphore.hw_semaphore || !s_gbe_semaphore.fw_semaphore || !remapped_address) {
+        return;
+    }
+
+    memcpy_fromio(&hw_val, s_gbe_semaphore.hw_semaphore, sizeof(hw_val));
+    memcpy_fromio(&fw_val, s_gbe_semaphore.fw_semaphore, sizeof(fw_val));
+    pw_pr_debug("HW_VAL = 0x%lx, FW_VAL = 0x%lx\n", (unsigned long)hw_val, (unsigned long)fw_val);
+    if (!IS_HW_SEMAPHORE_SET(hw_val) && !IS_FW_SEMAPHORE_SET(fw_val)) {
+        memcpy_toio((void *)remapped_address, &write_value, 4 /* counter_size_in_bytes*/);
+        memcpy_fromio(dst_vals, (void *)remapped_address, counter_size_in_bytes);
+    }
+}
+void sw_read_mmio_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptors, u16 counter_size_in_bytes)
+{
+    unsigned long remapped_address = (unsigned long)descriptors->mmio_descriptor.data_remapped_address;
+    if (descriptors->mmio_descriptor.is_gbe) {
+        /* MMIO for GBE requires a mailbox-like operation */
+        sw_read_gbe_mmio_info_i(dst_vals, descriptors, counter_size_in_bytes);
+    } else {
+        if (remapped_address) {
+            memcpy_fromio(dst_vals, (void *)remapped_address, counter_size_in_bytes);
+        }
+    }
+    pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
+}
+
+void sw_read_pch_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes)
+{
+    /*
+     * TODO: spinlock?
+     */
+    const struct sw_driver_pch_mailbox_io_descriptor *pch_mailbox = &descriptor->pch_mailbox_descriptor;
+    u32 address = pch_mailbox->data_address;
+    u64 mtpmc_remapped_address = pch_mailbox->mtpmc_remapped_address;
+    u64 msg_full_sts_remapped_address = pch_mailbox->msg_full_sts_remapped_address;
+    u64 mfpmc_remapped_address = pch_mailbox->mfpmc_remapped_address;
+
+    /*
+     * write address of desired device counter to request from PMC (shift and add 2 to format device offset)
+     */
+    if (mtpmc_remapped_address) {
+        int iter = 0;
+        u32 written_val = 0;
+        u32 write_value = (address << 16) + 2; /* shift and add 2 to format device offset */
+        memcpy_toio((volatile void *)(unsigned long)mtpmc_remapped_address, &write_value, 4 /*counter_size_in_bytes*/);
+        /*
+         * Check if address has been written using a while loop in order to wait for the PMC to consume that address
+         * and to introduce sufficient delay so that the message full status bit has time to flip. This should ensure
+         * all is ready when begin the wait loop for it to turn 0, which indicates the value is available to be read.
+         * (This fixes problem where values being read were huge.)
+         */
+        do {
+            memcpy_fromio(&written_val, (volatile void *)(unsigned long)mtpmc_remapped_address, 4 /*counter_size_in_bytes*/);
+            pw_pr_debug("DEBUG: written_val = 0x%x, address = 0x%x\n", written_val, address);
+            udelay(USEC_DELAY);
+        } while ((written_val >> 16) != address && ++iter < NUM_RETRY);
+    }
+
+
+    /*
+     * wait for PMC to set status indicating that device counter is available for read.
+     */
+    if (msg_full_sts_remapped_address) {
+        u32 status_wait = 0;
+        int iter = 0;
+        do {
+            memcpy_fromio(&status_wait, (volatile void *)(unsigned long)msg_full_sts_remapped_address, 4 /*counter_size_in_bytes*/);
+            pw_pr_debug("DEBUG: status_wait = 0x%x\n", status_wait);
+            udelay(USEC_DELAY);
+        } while ((status_wait & 0x01000000) >> 24 && ++iter < NUM_RETRY);
+    }
+
+    /*
+     * read device counter
+     */
+    if (mfpmc_remapped_address) {
+        memcpy_fromio(dst_vals, (volatile void *)(unsigned long)mfpmc_remapped_address, 4 /*counter_size_in_bytes*/);
+        pw_pr_debug("DEBUG: read value = 0x%x\n", *((pw_u32_t *)dst_vals));
+    }
+}
+
+void sw_read_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes)
+{
+    /*
+     * TODO: spinlock?
+     */
+    const struct sw_driver_mailbox_io_descriptor *mailbox = &descriptor->mailbox_descriptor;
+    unsigned long interface_address = mailbox->interface_address;
+    unsigned long interface_remapped_address = mailbox->interface_remapped_address;
+    unsigned long data_address = mailbox->data_address;
+    size_t iter = 0;
+
+    if (mailbox->is_msr_type) {
+        u64 command = 0;
+        rdmsrl_safe(interface_address, &command);
+        command &= mailbox->command_mask;
+        command |= mailbox->command | (u64)0x1 << mailbox->run_busy_bit;
+        wrmsrl_safe(interface_address, command);
+        do {
+            rdmsrl_safe(interface_address, &command);
+        } while ((command & ((u64)0x1 << mailbox->run_busy_bit)) && ++iter < 100);
+        rdmsrl_safe(data_address, &command);
+        *((u64 *)dst_vals) = command;
+    } else {
+        u32 command = 0;
+        memcpy_fromio(&command, (volatile void *)(unsigned long)interface_remapped_address, sizeof(command));
+        command &= mailbox->command_mask;
+        command |= (u32)mailbox->command | (u32)0x1 << mailbox->run_busy_bit;
+        memcpy_toio((volatile void *)(unsigned long)interface_remapped_address, &command, sizeof(command));
+        do {
+            memcpy_fromio(&command, (volatile void *)(unsigned long)interface_remapped_address, sizeof(command));
+        } while ((command & ((u32)0x1 << mailbox->run_busy_bit)) && ++iter < 100);
+        memcpy_toio((volatile void *)(unsigned long)mailbox->data_remapped_address, &command, 4 /* counter size in bytes */);
+        *((u32 *)dst_vals) = command;
+    }
+}
+
+void sw_read_pci_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptors, u16 counter_size_in_bytes)
+{
+    u32 bus = descriptors->pci_descriptor.bus, device = descriptors->pci_descriptor.device;
+    u32 function = descriptors->pci_descriptor.function, offset = descriptors->pci_descriptor.offset;
+    u32 data32 = 0;
+    u64 data64 = 0;
+    switch (counter_size_in_bytes) {
+        case 4:
+            data32 = sw_platform_pci_read32(bus, device, function, 0 /* CTRL-OFFSET */, 0 /* CTRL-DATA, don't care */, offset /* DATA-OFFSET */);
+            *((u32 *)dst_vals) = data32;
+            break;
+        case 8:
+            data64 = sw_platform_pci_read64(bus, device, function, 0 /* CTRL-OFFSET */, 0 /* CTRL-DATA, don't care */, offset /* DATA-OFFSET */);
+            *((u64 *)dst_vals) = data64;
+            break;
+        default:
+            pw_pr_error("ERROR: invalid read size = %u\n", counter_size_in_bytes);
+            return;
+        }
+    return;
+}
+void sw_read_configdb_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptors, u16 counter_size_in_bytes)
+{
+    {
+        pw_u32_t address = descriptors->configdb_descriptor.address;
+        u32 data = sw_platform_configdb_read32(address);
+        pw_pr_debug( "ADDRESS = 0x%x, CPU = %d, dst_vals = %p, counter size = %u, data = %u\n", address, cpu, dst_vals, counter_size_in_bytes, data);
+        /*
+         * 'counter_size_in_bytes' is ignored, for now.
+         */
+        *((u32 *)dst_vals) = data;
+    }
+    return;
+}
+void sw_read_socperf_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptors, u16 counter_size_in_bytes)
+{
+#if DO_SOCPERF
+    u64 *socperf_buffer = (u64 *)dst_vals;
+
+    memset(socperf_buffer, 0, counter_size_in_bytes);
+    SOCPERF_Read_Data2(socperf_buffer);
+#endif // DO_SOCPERF
+    return;
+}
+
+/**
+ * Decide if the socperf interface is available for use
+ * @returns     true if available
+ */
+bool sw_socperf_available_i(void)
+{
+    bool retVal = false;
+#if DO_SOCPERF
+    retVal = true;
+#endif
+    return retVal;
+}
+
+
+/**
+ * sw_platform_configdb_read32 - for reading PCI space through config registers
+ *                               of the platform.
+ * @address: An address in the PCI space
+ *
+ * Returns: the value read from address.
+ */
+u32 sw_platform_configdb_read32(u32 address)
+{
+    u32 read_value = 0;
+#if DO_DIRECT_PCI_READ_WRITE
+    return sw_platform_pci_read32(0/*bus*/, 0/*device*/, 0/*function*/, SW_PCI_MSG_CTRL_REG/*ctrl-offset*/, address/*ctrl-value*/, SW_PCI_MSG_DATA_REG/*data-offset*/);
+#else // !DO_DIRECT_PCI_READ_WRITE
+    read_value = intel_mid_msgbus_read32_raw(address);
+    pw_pr_debug("address = %u, value = %u\n", address, read_value);
+#endif // if DO_DIRECT_PCI_READ_WRITE
+    return read_value;
+}
+
+u32 sw_platform_pci_read32(u32 bus, u32 device, u32 function, u32 write_offset, u32 write_value, u32 read_offset)
+{
+    u32 read_value = 0;
+    struct pci_dev *pci_root = pci_get_domain_bus_and_slot(0, bus, PCI_DEVFN(device, function)); // 0, PCI_DEVFN(0, 0));
+    if (!pci_root) {
+        return 0; /* Application will verify the data */
+    }
+    if (write_offset) {
+        pci_write_config_dword(pci_root, write_offset, write_value); // SW_PCI_MSG_CTRL_REG, address);
+    }
+    pci_read_config_dword(pci_root, read_offset, &read_value); // SW_PCI_MSG_DATA_REG, &read_value);
+    return read_value;
+}
+
+u64 sw_platform_pci_read64(u32 bus, u32 device, u32 function, u32 write_offset, u32 write_value, u32 read_offset)
+{
+    u32 lo = sw_platform_pci_read32(bus, device, function, 0 /* CTRL-OFFSET */, 0 /* CTRL-DATA, don't care */, read_offset /* DATA-OFFSET */);
+    u32 hi = sw_platform_pci_read32(bus, device, function, 0 /* CTRL-OFFSET */, 0 /* CTRL-DATA, don't care */, read_offset + 4 /* DATA-OFFSET */);
+    return ((u64)hi << 32) | lo;
+}
+
+void sw_write_msr_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes)
+{
+    u64 write_value = descriptor->write_value;
+    u64 address = descriptor->msr_descriptor.address;
+    pw_pr_debug("ADDRESS = 0x%llx, CPU = %d, counter size = %u, value = %llu\n", address, cpu, counter_size_in_bytes, write_value);
+    if (likely(cpu == RAW_CPU())) {
+        wrmsrl_safe((unsigned long)address, write_value);
+    } else {
+        u32 l = write_value & 0xffffffff, h = (write_value >> 32) & 0xffffffff;
+        wrmsr_safe_on_cpu(cpu, (u32)address, l, h);
+    }
+    return;
+};
+
+void sw_write_mmio_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes)
+{
+    unsigned long remapped_address = (unsigned long)descriptor->mmio_descriptor.data_remapped_address;
+    u64 write_value = descriptor->write_value;
+
+    if (remapped_address) {
+        memcpy_toio((void *)remapped_address, &write_value, counter_size_in_bytes);
+    }
+    pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
+};
+
+void sw_write_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes)
+{
+    /*
+     * TODO: spinlock?
+     */
+    const struct sw_driver_mailbox_io_descriptor *mailbox = &descriptor->mailbox_descriptor;
+    unsigned long interface_address = mailbox->interface_address;
+    unsigned long interface_remapped_address = mailbox->interface_remapped_address;
+    unsigned long data_address = mailbox->data_address;
+    u64 data = descriptor->write_value;
+    size_t iter = 0;
+
+    if (mailbox->is_msr_type) {
+        u64 command = 0;
+        rdmsrl_safe(interface_address, &command);
+        command &= mailbox->command_mask;
+        command |= mailbox->command | (u64)0x1 << mailbox->run_busy_bit;
+        wrmsrl_safe(data_address, data);
+        wrmsrl_safe(interface_address, command);
+        do {
+            rdmsrl_safe(interface_address, &command);
+        } while ((command & ((u64)0x1 << mailbox->run_busy_bit)) && ++iter < 100);
+    } else {
+        u32 command = 0;
+        memcpy_fromio(&command, (volatile void *)(unsigned long)interface_remapped_address, sizeof(command));
+        command &= mailbox->command_mask;
+        command |= (u32)mailbox->command | (u32)0x1 << mailbox->run_busy_bit;
+        memcpy_toio((volatile void *)(unsigned long)mailbox->data_remapped_address, &data, sizeof(data));
+        memcpy_toio((volatile void *)(unsigned long)interface_remapped_address, &command, sizeof(command));
+        do {
+            memcpy_fromio(&command, (volatile void *)(unsigned long)interface_remapped_address, sizeof(command));
+        } while ((command & ((u32)0x1 << mailbox->run_busy_bit)) && ++iter < 100);
+    }
+}
+
+void sw_write_pci_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes)
+{
+    u32 bus = descriptor->pci_descriptor.bus, device = descriptor->pci_descriptor.device;
+    u32 function = descriptor->pci_descriptor.function, offset = descriptor->pci_descriptor.offset;
+    u32 write_value = (u32)descriptor->write_value;
+    /*
+     * 'counter_size_in_bytes' is ignored for now.
+     */
+    if (!sw_platform_pci_write32(bus, device, function, offset, write_value)) {
+        pw_pr_error("ERROR writing to PCI B/D/F/O %u/%u/%u/%u\n", bus, device, function, offset);
+    } else {
+        pw_pr_debug("OK, successfully wrote to PCI B/D/F/O %u/%u/%u/%u\n", bus, device, function, offset);
+    }
+    return;
+};
+
+/*
+ * Write to PCI space via config registers.
+ */
+bool sw_platform_pci_write32(u32 bus, u32 device, u32 function, u32 write_offset, u32 data_value)
+{
+    struct pci_dev *pci_root = pci_get_domain_bus_and_slot(0, bus, PCI_DEVFN(device, function)); // 0, PCI_DEVFN(0, 0));
+    if (!pci_root) {
+        return false;
+    }
+
+    pci_write_config_dword(pci_root, write_offset, data_value);
+
+    return true;
+};
+
+int sw_print_msr_io_descriptor(const struct sw_driver_io_descriptor *descriptor)
+{
+    if (!descriptor) {
+        return -PW_ERROR;
+    }
+    pw_pr_debug("MSR address = 0x%llx\n", descriptor->msr_descriptor.address);
+    return PW_SUCCESS;
+}
+
+int sw_ipc_mmio_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor)
+{
+    /* Unmap previously mapped memory here */
+    struct sw_driver_ipc_mmio_io_descriptor *__ipc_mmio = NULL;
+    if (!descriptor) { // Should NEVER happen
+        return -PW_ERROR;
+    }
+    if (descriptor->collection_type == SW_IO_IPC) {
+        __ipc_mmio = (struct sw_driver_ipc_mmio_io_descriptor *)&descriptor->ipc_descriptor;
+    } else {
+        __ipc_mmio = (struct sw_driver_ipc_mmio_io_descriptor *)&descriptor->mmio_descriptor;
+    }
+    if (__ipc_mmio->data_remapped_address) {
+        pw_pr_debug("unmapping addr 0x%llx\n", __ipc_mmio->data_remapped_address);
+        iounmap((volatile void *)(unsigned long)__ipc_mmio->data_remapped_address);
+        __ipc_mmio->data_remapped_address = 0;
+    }
+    /* Uninitialize the GBE, if it wasn't already done */
+    if (s_gbe_semaphore.hw_semaphore || s_gbe_semaphore.fw_semaphore) {
+        pw_pr_debug("Uninitializing gbe!\n");
+        if (s_gbe_semaphore.hw_semaphore) {
+            iounmap(s_gbe_semaphore.hw_semaphore);
+        }
+        if (s_gbe_semaphore.fw_semaphore) {
+            iounmap(s_gbe_semaphore.fw_semaphore);
+        }
+        memset(&s_gbe_semaphore, 0, sizeof(s_gbe_semaphore));
+    }
+    return PW_SUCCESS;
+}
+
+int sw_pch_mailbox_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor)
+{
+    /* Unmap previously mapped memory here */
+    struct sw_driver_pch_mailbox_io_descriptor *__pch_mailbox = NULL;
+    if (!descriptor) { // Should NEVER happen
+        return -PW_ERROR;
+    }
+    __pch_mailbox = (struct sw_driver_pch_mailbox_io_descriptor *)&descriptor->pch_mailbox_descriptor;
+    if (__pch_mailbox->mtpmc_remapped_address) {
+        pw_pr_debug("unmapping addr 0x%llx\n", __pch_mailbox->mtpmc_remapped_address);
+        iounmap((volatile void *)(unsigned long)__pch_mailbox->mtpmc_remapped_address);
+        __pch_mailbox->mtpmc_remapped_address = 0;
+    }
+    if (__pch_mailbox->msg_full_sts_remapped_address) {
+        pw_pr_debug("unmapping addr 0x%llx\n", __pch_mailbox->msg_full_sts_remapped_address);
+        iounmap((volatile void *)(unsigned long)__pch_mailbox->msg_full_sts_remapped_address);
+        __pch_mailbox->msg_full_sts_remapped_address = 0;
+    }
+    if (__pch_mailbox->mfpmc_remapped_address) {
+        pw_pr_debug("unmapping addr 0x%llx\n", __pch_mailbox->mfpmc_remapped_address);
+        iounmap((volatile void *)(unsigned long)__pch_mailbox->mfpmc_remapped_address);
+        __pch_mailbox->mfpmc_remapped_address = 0;
+    }
+    return PW_SUCCESS;
+}
+
+int sw_mailbox_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor)
+{
+    /* Unmap previously mapped memory here */
+    struct sw_driver_mailbox_io_descriptor *__mailbox = NULL;
+    if (!descriptor) { // Should NEVER happen
+        return -PW_ERROR;
+    }
+    __mailbox = (struct sw_driver_mailbox_io_descriptor *)&descriptor->mailbox_descriptor;
+    if (!__mailbox->is_msr_type) {
+        if (__mailbox->interface_remapped_address) {
+            pw_pr_debug("unmapping addr 0x%llx\n", __mailbox->interface_remapped_address);
+            iounmap((volatile void *)(unsigned long)__mailbox->interface_remapped_address);
+            __mailbox->interface_remapped_address = 0;
+        }
+        if (__mailbox->data_remapped_address) {
+            pw_pr_debug("unmapping addr 0x%llx\n", __mailbox->data_remapped_address);
+            iounmap((volatile void *)(unsigned long)__mailbox->data_remapped_address);
+            __mailbox->data_remapped_address = 0;
+        }
+    }
+    return PW_SUCCESS;
+}
+
+#define NUM_HW_OPS SW_ARRAY_SIZE(s_hw_ops)
+#define FOR_EACH_HW_OP(idx, op) for (idx=0; idx<NUM_HW_OPS && (op=&s_hw_ops[idx]); ++idx)
+
+int sw_register_ops_providers(void)
+{
+    size_t idx=0;
+    const struct sw_hw_ops *op = NULL;
+    FOR_EACH_HW_OP(idx, op) {
+        if (op->name && sw_register_hw_op(op)) {
+            pw_pr_error("ERROR registering provider %s\n", op->name);
+            return -EIO;
+        }
+    }
+    return PW_SUCCESS;
+}
+
+void sw_free_ops_providers(void)
+{
+    // NOP
+}
diff --git a/drivers/misc/intel/socwatch/sw_output_buffer.c b/drivers/misc/intel/socwatch/sw_output_buffer.c
new file mode 100644
index 000000000000..9e4037e89a93
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_output_buffer.c
@@ -0,0 +1,533 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#include "sw_internal.h"
+#include "sw_output_buffer.h"
+#include "sw_defines.h"
+#include "sw_mem.h"
+#include "sw_lock_defs.h"
+#include "sw_overhead_measurements.h"
+
+/* -------------------------------------------------
+ * Compile time constants and macros.
+ * -------------------------------------------------
+ */
+#define NUM_SEGS_PER_BUFFER 2 /* MUST be pow 2! */
+#define NUM_SEGS_PER_BUFFER_MASK (NUM_SEGS_PER_BUFFER - 1)
+/*
+ * The size of the 'buffer' data array in each segment.
+ */
+#define SW_SEG_DATA_SIZE (sw_buffer_alloc_size)
+/*
+ * Min size of per-cpu output buffers.
+ */
+#define SW_MIN_SEG_SIZE_BYTES (1 << 10) /* 1kB */
+#define SW_MIN_OUTPUT_BUFFER_SIZE (SW_MIN_SEG_SIZE_BYTES * NUM_SEGS_PER_BUFFER)
+/*
+ * A symbolic constant for an empty buffer index.
+ */
+#define EMPTY_SEG (-1)
+/*
+ * How much space is available in a given segment?
+ */
+#define EMPTY_TSC ((u64)-1)
+#define SEG_IS_FULL(seg) ({bool __full = false; \
+        smp_mb(); \
+        __full = ((seg)->is_full != EMPTY_TSC); \
+        __full;})
+#define SEG_SET_FULL(seg, tsc) do { \
+    (seg)->is_full = (tsc); \
+    smp_mb(); \
+} while(0)
+#define SEG_SET_EMPTY(seg) do { \
+    barrier(); \
+    (seg)->bytes_written = 0; \
+    SEG_SET_FULL(seg, EMPTY_TSC); \
+    /*smp_mb(); */ \
+} while(0)
+#define SPACE_AVAIL(seg) (SW_SEG_DATA_SIZE - (seg)->bytes_written )
+#define SEG_IS_EMPTY(seg) (SPACE_AVAIL(seg) == SW_SEG_DATA_SIZE)
+
+#define GET_OUTPUT_BUFFER(cpu) &per_cpu_output_buffers[(cpu)]
+/*
+ * Convenience macro: iterate over each segment in a per-cpu output buffer.
+ */
+#define for_each_segment(i) for (i=0; i<NUM_SEGS_PER_BUFFER; ++i)
+#define for_each_seg(buffer, seg) for (int i=0; i<NUM_SEGS_PER_BUFFER && (seg=(buffer)->segments[i]); ++i)
+/*
+ * How many buffers are we using?
+ */
+#define GET_NUM_OUTPUT_BUFFERS() (sw_max_num_cpus + 1)
+/*
+ * Convenience macro: iterate over each per-cpu output buffer.
+ */
+#define for_each_output_buffer(i) for (i=0; i<GET_NUM_OUTPUT_BUFFERS(); ++i)
+
+/* -------------------------------------------------
+ * Local data structures.
+ * -------------------------------------------------
+ */
+typedef struct sw_data_buffer sw_data_buffer_t;
+typedef struct sw_output_buffer sw_output_buffer_t;
+struct sw_data_buffer {
+    u64 is_full;
+    u32 bytes_written;
+    char *buffer;
+} __attribute__((packed));
+#define SW_SEG_HEADER_SIZE() (sizeof(struct sw_data_buffer) - sizeof(char *))
+
+struct sw_output_buffer {
+    sw_data_buffer_t buffers[NUM_SEGS_PER_BUFFER];
+    int buff_index;
+    u32 produced_samples;
+    u32 dropped_samples;
+    int last_seg_read;
+    unsigned int mem_alloc_size;
+    unsigned long free_pages;
+} ____cacheline_aligned_in_smp;
+
+/* -------------------------------------------------
+ * Function declarations.
+ * -------------------------------------------------
+ */
+extern u64 sw_timestamp(void);
+
+/* -------------------------------------------------
+ * Variable definitions.
+ * -------------------------------------------------
+ */
+u64 sw_num_samples_produced = 0, sw_num_samples_dropped = 0;
+int sw_max_num_cpus = -1;
+
+DECLARE_OVERHEAD_VARS(sw_produce_generic_msg_i);
+/*
+ * Per-cpu output buffers.
+ */
+sw_output_buffer_t *per_cpu_output_buffers = NULL;
+/*
+ * Variables for book keeping.
+ */
+volatile int sw_last_cpu_read = -1;
+volatile s32 sw_last_mask = -1;
+/*
+ * Lock for the polled buffer.
+ */
+SW_DECLARE_SPINLOCK(sw_polled_lock);
+/*
+ * Buffer allocation size.
+ */
+unsigned long sw_buffer_alloc_size = (1 << 16); // 64 KB
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+
+static char *reserve_seg_space_i(size_t size, int cpu, bool *should_wakeup, u64 *reservation_tsc)
+{
+    sw_output_buffer_t *buffer = GET_OUTPUT_BUFFER(cpu);
+    int i=0;
+    int buff_index = buffer->buff_index;
+    char *dst = NULL;
+
+    if (buff_index < 0 || buff_index >= NUM_SEGS_PER_BUFFER) {
+        goto prod_seg_done;
+    }
+    for_each_segment(i) {
+        sw_data_buffer_t *seg = &buffer->buffers[buff_index];
+        if (SEG_IS_FULL(seg) == false) {
+            if (SPACE_AVAIL(seg) >= size) {
+                *reservation_tsc = sw_timestamp();
+                dst = &seg->buffer[seg->bytes_written];
+                seg->bytes_written += size;
+                smp_mb();
+                buffer->buff_index = buff_index;
+                buffer->produced_samples++;
+                goto prod_seg_done;
+            }
+            SEG_SET_FULL(seg, sw_timestamp());
+        }
+        buff_index = CIRCULAR_INC(buff_index, NUM_SEGS_PER_BUFFER_MASK);
+        *should_wakeup = true;
+    }
+prod_seg_done:
+    if (!dst) {
+        buffer->dropped_samples++;
+    }
+    return dst;
+};
+
+int produce_polled_msg(struct sw_driver_msg *msg, enum sw_wakeup_action action)
+{
+    int cpu = GET_POLLED_CPU();
+    bool should_wakeup = false;
+    int retVal = PW_SUCCESS;
+
+    if (!msg) {
+        return -PW_ERROR;
+    }
+    pw_pr_debug("POLLED! cpu = %d\n", cpu);
+    LOCK(sw_polled_lock);
+    {
+        size_t size = SW_DRIVER_MSG_HEADER_SIZE() + msg->payload_len;
+        char *dst = reserve_seg_space_i(size, cpu, &should_wakeup, &msg->tsc);
+        if (dst) {
+            /*
+             * Assign a special CPU number to this CPU.
+             * This is OK, because messages enqueued in this buffer
+             * are always CPU agnostic (otherwise they would
+             * be invoked from within a preempt_disable()d context
+             * in 'sw_handle_collector_node_i()', which ensures they
+             * will be enqueued within the 'sw_produce_generic_msg_on_cpu()'
+             * function).
+             */
+            msg->cpuidx = cpu;
+            memcpy(dst, msg, SW_DRIVER_MSG_HEADER_SIZE()); dst += SW_DRIVER_MSG_HEADER_SIZE();
+            memcpy(dst, msg->p_payload, msg->payload_len);
+        } else {
+            pw_pr_debug("NO space in polled msg!\n");
+            retVal = -PW_ERROR;
+        }
+    }
+    UNLOCK(sw_polled_lock);
+    if (unlikely(should_wakeup)) {
+        sw_wakeup_reader(action);
+    }
+    return retVal;
+};
+
+static int sw_produce_generic_msg_i(struct sw_driver_msg *msg, enum sw_wakeup_action action)
+{
+    int retval = PW_SUCCESS;
+    bool should_wakeup = false;
+    int cpu = -1;
+    unsigned long flags = 0;
+
+    if (!msg) {
+        pw_pr_error("ERROR: CANNOT produce a NULL msg!\n");
+        return -PW_ERROR;
+    }
+
+#ifdef CONFIG_PREEMPT_COUNT
+    if (!in_atomic()) {
+        return produce_polled_msg(msg, action);
+    }
+#endif
+
+    cpu = sw_get_cpu(&flags);
+    {
+
+        size_t size = msg->payload_len + SW_DRIVER_MSG_HEADER_SIZE();
+        char *dst = reserve_seg_space_i(size, cpu, &should_wakeup, &msg->tsc);
+        if (likely(dst)) {
+            memcpy(dst, msg, SW_DRIVER_MSG_HEADER_SIZE()); dst += SW_DRIVER_MSG_HEADER_SIZE();
+            memcpy(dst, msg->p_payload, msg->payload_len);
+        } else {
+            retval = -PW_ERROR;
+        }
+    }
+    sw_put_cpu(flags);
+
+    if (unlikely(should_wakeup)) {
+        sw_wakeup_reader(action);
+    }
+
+    return retval;
+};
+
+int sw_produce_generic_msg(struct sw_driver_msg *msg, enum sw_wakeup_action action)
+{
+    return DO_PER_CPU_OVERHEAD_FUNC_RET(int, sw_produce_generic_msg_i, msg, action);
+};
+
+int sw_init_per_cpu_buffers_i(unsigned long per_cpu_mem_size)
+{
+    int cpu = -1;
+    per_cpu_output_buffers = (sw_output_buffer_t *)sw_kmalloc(sizeof(sw_output_buffer_t) * GET_NUM_OUTPUT_BUFFERS(), GFP_KERNEL | __GFP_ZERO);
+    if (per_cpu_output_buffers == NULL) {
+        pw_pr_error("ERROR allocating space for per-cpu output buffers!\n");
+        sw_destroy_per_cpu_buffers();
+        return -PW_ERROR;
+    }
+    for_each_output_buffer(cpu) {
+        sw_output_buffer_t *buffer = &per_cpu_output_buffers[cpu];
+        char *buff = NULL;
+        int i=0;
+        buffer->mem_alloc_size = per_cpu_mem_size;
+        buffer->free_pages = sw_allocate_pages(GFP_KERNEL | __GFP_ZERO, (unsigned int)per_cpu_mem_size);
+        if (buffer->free_pages == 0) {
+            pw_pr_error("ERROR allocating pages for buffer [%d]!\n", cpu);
+            sw_destroy_per_cpu_buffers();
+            return -PW_ERROR;
+        }
+        buff = (char *)buffer->free_pages;
+        for_each_segment(i) {
+            buffer->buffers[i].buffer = (char *)buff;
+            buff += SW_SEG_DATA_SIZE;
+        }
+    }
+    pw_pr_debug("PER_CPU_MEM_SIZE = %lu, order = %u\n", per_cpu_mem_size, get_order(per_cpu_mem_size));
+    return PW_SUCCESS;
+};
+
+int sw_init_per_cpu_buffers(void)
+{
+    unsigned int per_cpu_mem_size = sw_get_output_buffer_size();
+
+    pw_pr_debug("Buffer alloc size = %ld\n", sw_buffer_alloc_size);
+
+    if (GET_NUM_OUTPUT_BUFFERS() <= 0) {
+        pw_pr_error("ERROR: max # output buffers= %d\n", GET_NUM_OUTPUT_BUFFERS());
+        return -PW_ERROR;
+    }
+
+    pw_pr_debug("DEBUG: sw_max_num_cpus = %d, num output buffers = %d\n", sw_max_num_cpus, GET_NUM_OUTPUT_BUFFERS());
+
+    /*
+     * Try to allocate per-cpu buffers. If allocation fails, decrease buffer size and retry.
+     * Stop trying if size drops below 2KB (which means 1KB for each buffer).
+     */
+    while (per_cpu_mem_size >= SW_MIN_OUTPUT_BUFFER_SIZE && sw_init_per_cpu_buffers_i(per_cpu_mem_size)) {
+        pw_pr_debug("WARNING: couldn't allocate per-cpu buffers with size %u -- trying smaller size!\n", per_cpu_mem_size);
+        sw_buffer_alloc_size >>= 1;
+        per_cpu_mem_size = sw_get_output_buffer_size();
+    }
+
+    if (unlikely(per_cpu_output_buffers == NULL)) {
+        pw_pr_error("ERROR: couldn't allocate space for per-cpu output buffers!\n");
+        return -PW_ERROR;
+    }
+    /*
+     * Initialize our locks.
+     */
+    SW_INIT_SPINLOCK(sw_polled_lock);
+
+    pw_pr_debug("OK, allocated per-cpu buffers with size = %lu\n", per_cpu_mem_size);
+
+    if (sw_init_reader_queue()) {
+        pw_pr_error("ERROR initializing reader subsys\n");
+        return -PW_ERROR;
+    }
+
+    return PW_SUCCESS;
+};
+
+void sw_destroy_per_cpu_buffers(void)
+{
+    int cpu = -1;
+
+    /*
+     * Perform lock finalization.
+     */
+    SW_DESTROY_SPINLOCK(sw_polled_lock);
+
+    if (per_cpu_output_buffers != NULL) {
+        for_each_output_buffer(cpu) {
+            sw_output_buffer_t *buffer = &per_cpu_output_buffers[cpu];
+            if (buffer->free_pages != 0) {
+                sw_release_pages(buffer->free_pages, buffer->mem_alloc_size);
+                buffer->free_pages = 0;
+            }
+        }
+        sw_kfree(per_cpu_output_buffers);
+        per_cpu_output_buffers = NULL;
+    }
+};
+
+void sw_reset_per_cpu_buffers(void)
+{
+    int cpu = 0, i=0;
+    for_each_output_buffer(cpu) {
+        sw_output_buffer_t *buffer = GET_OUTPUT_BUFFER(cpu);
+        buffer->buff_index = buffer->dropped_samples = buffer->produced_samples = 0;
+        buffer->last_seg_read = -1;
+
+        for_each_segment(i) {
+            sw_data_buffer_t *seg = &buffer->buffers[i];
+            memset(seg->buffer, 0, SW_SEG_DATA_SIZE);
+            SEG_SET_EMPTY(seg);
+        }
+    }
+    sw_last_cpu_read = -1;
+    sw_last_mask = -1;
+    pw_pr_debug("OK, reset per-cpu output buffers!\n");
+};
+
+bool sw_any_seg_full(u32 *val, bool is_flush_mode)
+{
+    int num_visited = 0, i = 0;
+
+    if (!val) {
+        pw_pr_error("ERROR: NULL ptrs in sw_any_seg_full!\n");
+        return false;
+    }
+
+    *val = SW_NO_DATA_AVAIL_MASK;
+    pw_pr_debug(KERN_INFO "Checking for full seg: val = %u, flush = %s\n", *val, GET_BOOL_STRING(is_flush_mode));
+    for_each_output_buffer(num_visited) {
+        int min_seg = EMPTY_SEG, non_empty_seg = EMPTY_SEG;
+        u64 min_tsc = EMPTY_TSC;
+        sw_output_buffer_t *buffer = NULL;
+        if (++sw_last_cpu_read >= GET_NUM_OUTPUT_BUFFERS()) {
+            sw_last_cpu_read = 0;
+        }
+        buffer = GET_OUTPUT_BUFFER(sw_last_cpu_read);
+        for_each_segment(i) {
+            sw_data_buffer_t *seg = &buffer->buffers[i];
+            u64 seg_tsc = seg->is_full;
+            if (SEG_IS_EMPTY(seg)) {
+                continue;
+            }
+            non_empty_seg = i;
+            if (seg_tsc < min_tsc) {
+                /*
+                 * Can only happen if seg was full, provided 'EMPTY_TSC' is set to "(u64)-1"
+                 */
+                min_tsc = seg_tsc;
+                min_seg = i;
+            }
+        }
+        if (min_seg != EMPTY_SEG) {
+            *val = (sw_last_cpu_read & 0xffff) << 16 | (min_seg & 0xffff);
+            return true;
+        } else if (is_flush_mode && non_empty_seg != EMPTY_SEG) {
+            *val = (sw_last_cpu_read & 0xffff) << 16 | (non_empty_seg & 0xffff);
+            return true;
+        }
+    }
+    /*
+     * Reaches here only if there's no data to be read.
+     */
+    if (is_flush_mode) {
+        /*
+         * We've drained all buffers and need to tell the userspace application there
+         * isn't any data. Unfortunately, we can't just return a 'zero' value for the
+         * mask (because that could also indicate that segment # 0 of cpu #0 has data).
+         */
+        *val = SW_ALL_WRITES_DONE_MASK;
+        return true;
+    }
+    return false;
+};
+
+/*
+ * Has semantics of 'copy_to_user()' -- returns # of bytes that could NOT be copied
+ * (On success ==> returns 0).
+ */
+size_t sw_consume_data(u32 mask, void *buffer, size_t bytes_to_read)
+{
+    int which_cpu = -1, which_seg = -1;
+    unsigned long bytes_not_copied = 0;
+    sw_output_buffer_t *buff = NULL;
+    sw_data_buffer_t *seg = NULL;
+    size_t bytes_read = 0;
+
+    if (!sw_check_output_buffer_params(buffer, bytes_to_read, SW_SEG_DATA_SIZE)) {
+        pw_pr_error("ERROR: invalid params to \"sw_consume_data\"!\n");
+        return -PW_ERROR;
+    }
+
+    which_cpu = mask >> 16; which_seg = mask & 0xffff;
+    pw_pr_debug("CONSUME: cpu = %d, seg = %d\n", which_cpu, which_seg);
+    if (which_seg >= NUM_SEGS_PER_BUFFER) {
+        pw_pr_error("Error: which_seg (%d) >= NUM_SEGS_PER_BUFFER (%d)\n", which_seg, NUM_SEGS_PER_BUFFER);
+        return bytes_to_read;
+    }
+    /*
+     * OK to access unlocked; either the segment is FULL, or no collection
+     * is ongoing. In either case, we're GUARANTEED no producer is touching
+     * this segment.
+     */
+    buff = GET_OUTPUT_BUFFER(which_cpu);
+    seg = &buff->buffers[which_seg];
+
+    bytes_not_copied = sw_copy_to_user(buffer, seg->buffer, seg->bytes_written); // dst, src
+
+    // bytes_not_copied = copy_to_user(buffer, seg->buffer, seg->bytes_written); // dst,src
+    if (likely(bytes_not_copied == 0)) {
+      bytes_read = seg->bytes_written;
+    } else {
+        pw_pr_error("Warning: couldn't copy %lu bytes\n", bytes_not_copied);
+        bytes_read = 0;
+    }
+    SEG_SET_EMPTY(seg);
+    return bytes_read;
+}
+
+unsigned int sw_get_output_buffer_size(void)
+{
+    return (sw_buffer_alloc_size * NUM_SEGS_PER_BUFFER);
+};
+
+void sw_count_samples_produced_dropped(void)
+{
+    int cpu = 0;
+    sw_num_samples_produced = sw_num_samples_dropped = 0;
+    if (per_cpu_output_buffers == NULL) {
+        return;
+    }
+    for_each_output_buffer(cpu) {
+        sw_output_buffer_t *buff = GET_OUTPUT_BUFFER(cpu);
+        sw_num_samples_dropped += buff->dropped_samples;
+        sw_num_samples_produced += buff->produced_samples;
+    }
+};
+
+void sw_print_output_buffer_overheads(void)
+{
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_produce_generic_msg_i, "PRODUCE_GENERIC_MSG");
+    sw_print_reader_stats();
+};
diff --git a/drivers/misc/intel/socwatch/sw_reader.c b/drivers/misc/intel/socwatch/sw_reader.c
new file mode 100644
index 000000000000..22df7298d829
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_reader.c
@@ -0,0 +1,159 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2017 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include "sw_internal.h"
+#include "sw_output_buffer.h"
+#include "sw_defines.h"
+
+#define SW_BUFFER_CLEANUP_TIMER_DELAY_NSEC 1000000 /* delay buffer cleanup by 10^6 nsec i.e. 1 msec */
+
+/*
+ * The alarm queue.
+ */
+wait_queue_head_t sw_reader_queue;
+/*
+ * Reader wakeup timer.
+ */
+static struct hrtimer s_reader_wakeup_timer;
+/*
+ * Variable to track # timer fires.
+ */
+static int s_num_timer_fires = 0;
+
+/*
+ * The alarm callback.
+ */
+static enum hrtimer_restart sw_wakeup_callback_i(struct hrtimer *timer)
+{
+    ++s_num_timer_fires;
+    wake_up_interruptible(&sw_reader_queue);
+    return HRTIMER_NORESTART;
+}
+
+/*
+ * Init reader queue.
+ */
+int sw_init_reader_queue(void)
+{
+    init_waitqueue_head(&sw_reader_queue);
+    /*
+     * Also init wakeup timer (used in low-overhead mode).
+     */
+    hrtimer_init(&s_reader_wakeup_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+    s_reader_wakeup_timer.function = &sw_wakeup_callback_i;
+
+    return PW_SUCCESS;
+}
+/*
+ * Destroy reader queue.
+ */
+void sw_destroy_reader_queue(void)
+{
+    /* NOP */
+}
+/*
+ * Wakeup client waiting for a full buffer.
+ */
+void sw_wakeup_reader(enum sw_wakeup_action action)
+{
+    if (!waitqueue_active(&sw_reader_queue)) {
+        return;
+    }
+    /*
+     * Direct mode?
+     */
+    switch (action) {
+        case SW_WAKEUP_ACTION_DIRECT:
+            wake_up_interruptible(&sw_reader_queue);
+            break;
+        case SW_WAKEUP_ACTION_TIMER:
+            if (!hrtimer_active(&s_reader_wakeup_timer)) {
+                ktime_t ktime = ns_to_ktime(SW_BUFFER_CLEANUP_TIMER_DELAY_NSEC);
+                // TODO: possible race here -- introduce locks?
+                hrtimer_start(&s_reader_wakeup_timer, ktime, HRTIMER_MODE_REL);
+            }
+            break;
+        default:
+            break;
+    }
+    return;
+}
+/*
+ * Wakeup client waiting for a full buffer, and
+ * cancel any timers initialized by the reader
+ * subsys.
+ */
+void sw_cancel_reader(void)
+{
+    /*
+     * Cancel pending wakeup timer (used in low-overhead mode).
+     */
+    if (hrtimer_active(&s_reader_wakeup_timer)) {
+        hrtimer_cancel(&s_reader_wakeup_timer);
+    }
+    /*
+     * There might be a reader thread blocked on a read: wake
+     * it up to give it a chance to respond to changed
+     * conditions.
+     */
+    sw_wakeup_reader(SW_WAKEUP_ACTION_DIRECT);
+}
+
+void sw_print_reader_stats(void)
+{
+#if DO_OVERHEAD_MEASUREMENTS
+    printk(KERN_INFO "# reader queue timer fires = %d\n", s_num_timer_fires);
+#endif // OVERHEAD
+}
diff --git a/drivers/misc/intel/socwatch/sw_telem.c b/drivers/misc/intel/socwatch/sw_telem.c
new file mode 100644
index 000000000000..492699c5e48b
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_telem.c
@@ -0,0 +1,488 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/compiler.h>     /* Definition of __weak */
+#include <linux/version.h>      /* LINUX_VERSION_CODE */
+#include "sw_defines.h"         /* PW_ERROR, etc. */
+#include "sw_kernel_defines.h"  /* pw_pr_debug */
+#include "sw_mem.h"             /* sw_kmalloc/free */
+#include "sw_lock_defs.h"       /* Various lock-related definitions */
+#include "sw_telem.h"           /* Signatures of fn's exported from here. */
+
+/*
+ * These functions and data structures are exported by the Telemetry
+ * driver.  However, that file may not be available in the kernel for
+ * which this driver is being built, so we re-define many of the same
+ * things here.
+ */
+/**
+ * struct telemetry_evtlog - The "event log" returned by the kernel's
+ *                           full-read telemetry driver.
+ * @telem_evtid:   The 16-bit event ID.
+ * @telem_evtlog:  The actual telemetry data.
+ */
+struct telemetry_evtlog {
+    u32 telem_evtid;    /* Event ID of a data item. */
+    u64 telem_evtlog;   /* Counter data */
+};
+
+struct telemetry_evtconfig {
+    u32 *evtmap;    /* Array of Event-IDs to Enable */
+    u8 num_evts;    /* Number of Events (<29) in evtmap */
+    u8 period;      /* Sampling period */
+};
+
+#define MAX_TELEM_EVENTS 28  /* Max telem events per unit */
+
+/* The enable bit is set when programming events, but is returned
+ * cleared for queried events requests.
+ */
+#define TELEM_EVENT_ENABLE 0x8000 /* Enabled when Event ID HIGH bit */
+
+/*
+ * Sampling Period values.
+ * The sampling period is encoded in an 7-bit value, where
+ *    Period = (Value * 16^Exponent) usec where:
+ *        bits[6:3] -> Value;
+ *        bits [0:2]-> Exponent;
+ * Here are some of the calculated possible values:
+ * | Value  Val+Exp  | Value | Exponent | Period (usec) | Period (msec) |
+ * |-----------------+-------+----------+---------------+---------------|
+ * | 0xA = 000 1+010 |     1 |        2 |           256 |         0.256 |
+ * | 0x12= 001 0+010 |     2 |        2 |           512 |         0.512 |
+ * | 0x22= 010 0+010 |     4 |        2 |          1024 |         1.024 |
+ * | 0xB = 000 1+011 |     1 |        3 |          4096 |         4.096 |
+ * | 0x13= 001 0+011 |     2 |        3 |          8192 |         8.192 |
+ * | 0x1B= 001 1+011 |     3 |        3 |         12288 |        12.288 |
+ * | 0x0C= 000 1+100 |     1 |        4 |         65536 |        65.536 |
+ * | 0x0D= 000 1+101 |     1 |        5 |       1048576 |      1048.576 |
+ */
+#define TELEM_SAMPLING_1MS 0x22  /* Approximately 1 ms */
+#define TELEM_SAMPLING_1S  0x0D  /* Approximately 1 s */
+
+/* These functions make up the main APIs of the telemetry driver.  We
+ * define all of them with weak linkage so that we can still compile
+ * and load into kernels which don't have a telemetry driver.
+ */
+extern int __weak telemetry_raw_read_eventlog(enum telemetry_unit telem_unit,
+                                              struct telemetry_evtlog *evtlog,
+                                              int evcount);
+extern int __weak telemetry_reset(void);
+extern int __weak telemetry_reset_events(void);
+extern int __weak telemetry_get_sampling_period(u8 *punit_min,
+                                                u8 *punit_max,
+                                                u8 *pmc_min,
+                                                u8 *pmc_max);
+extern int __weak telemetry_set_sampling_period(u8 punit_period,
+                                                u8   pmc_period);
+extern int __weak telemetry_get_eventconfig(struct telemetry_evtconfig *punit_config,
+                                            struct telemetry_evtconfig *pmc_config,
+                                            int  punit_len,
+                                            int  pmc_len);
+extern int __weak telemetry_add_events(u8 num_punit_evts, u8 num_pmc_evts,
+                                       u32 *punit_evtmap, u32 *pmc_evtmap);
+
+extern int __weak telemetry_update_events(struct telemetry_evtconfig punit_config,
+                                          struct telemetry_evtconfig pmc_config);
+
+/*
+ * Some telemetry IDs have multiple instances, indexed by cpu ID.  We
+ * implement these by defining two types of IDs: 'regular' and 'scaled'.
+ * For Telemetry IDs with a single instance (the majority of them), the
+ * index into the system's telemetry table is stored in the
+ * sw_driver_io_descriptor.idx.  At read time, the driver gets the telemetry
+ * "slot" from sw_driver_io_descriptor.idx, and reads that data.  This case
+ * is illustrated by telem_desc_A in the illustration below, where idx 2
+ * indicates that telem_data[2] contains the telem data for this descriptor.
+ *
+ *   telem_desc_A                            telem_data
+ *    scale_op: X                              |..|[0]
+ *    idx     : 2 --------------------         |..|[1]
+ *                                    \------->|..|[2]
+ *                     Scaled_IDs              |..|[3]
+ *   telem_desc_B     CPU#0 1 2 3       ------>|..|[4]
+ *    scale_op: /     [0]|.|.|.|.|     /
+ *    idx     : 1---->[1]|4|4|5|5|    /
+ *                        +----------/
+ *
+ * Descriptors with scaled IDs contain a scale operation (scale_op) and
+ * value.  They use a 'scaled_ids' table, which is indexed by descriptor
+ * number and CPU id, and stores the telem_data index.  So in the
+ * illustration above, CPU 0 reading from telem_desc_B would fetch row 1
+ * (from telem_desc_B.idx == 1), and column [0] yielding element 4, so
+ * that's the telemetry ID it looks up in the telemetry data.
+ *
+ * The scaled_ids table is populated at telemetry ID initialization time
+ *
+ */
+static unsigned char *sw_telem_scaled_ids = NULL; /* Allocate on demand */
+static unsigned int   sw_telem_rows_alloced = 0; /* Rows currently allocated */
+static unsigned int   sw_telem_rows_avail   = 0; /* Available rows */
+
+extern int sw_max_num_cpus;     /* SoC Watch's copy of cpu count. */
+
+/* Macro for identifying telemetry IDs with either per-cpu, or per-module
+ * instances.  These IDs need to be 'scaled' as per scale_op and scale_val.
+ */
+#define IS_SCALED_ID(td) ((td)->scale_op != TELEM_OP_NONE)
+/*
+ * Event map that is populated with user-supplied IDs
+ */
+static u32 s_event_map[2][MAX_TELEM_EVENTS];
+/*
+ * Index into event map(s)
+ */
+static size_t s_unit_idx[2] = {0, 0};
+/*
+ * Used to decide if telemetry values need refreshing
+ */
+static size_t s_unit_iters[2] = {0, 0};
+/*
+ * Spinlock to guard updates to the 'iters' values.
+ */
+SW_DEFINE_SPINLOCK(sw_telem_lock);
+/*
+ * Macro to determine if socwatch telemetry system has been configured
+ */
+#define SW_TELEM_CONFIGURED() (s_unit_idx[0] > 0 || s_unit_idx[1] > 0)
+
+/**
+ * telemetry_available - Determine if telemetry driver is present
+ *
+ * Returns: 1 if telemetry driver is present, 0 if not.
+ */
+static int telemetry_available(void)
+{
+    int retval = 0;
+    struct telemetry_evtconfig punit_evtconfig;
+    struct telemetry_evtconfig pmc_evtconfig;
+    u32 punit_event_map[MAX_TELEM_EVENTS];
+    u32 pmc_event_map[MAX_TELEM_EVENTS];
+
+
+    /* The symbol below is weak.  We return 1 if we have a definition
+     * for this telemetry-driver-supplied symbol, or 0 if only the
+     * weak definition exists. This test will suffice to detect if
+     * the telemetry driver is loaded.
+     */
+    if (telemetry_get_eventconfig == 0) {
+            return 0;
+    }
+    /* OK, the telemetry driver is loaded. But it's possible it
+     * hasn't been configured properly. To check that, retrieve
+     * the number of events currently configured. This should never
+     * be zero since the telemetry driver reserves some SSRAM slots
+     * for its own use
+     */
+    memset(&punit_evtconfig, 0, sizeof(punit_evtconfig));
+    memset(&pmc_evtconfig, 0, sizeof(pmc_evtconfig));
+
+    punit_evtconfig.evtmap = (u32*) &punit_event_map;
+    pmc_evtconfig.evtmap = (u32*) &pmc_event_map;
+
+    retval = telemetry_get_eventconfig(&punit_evtconfig, &pmc_evtconfig,
+                                        MAX_TELEM_EVENTS, MAX_TELEM_EVENTS);
+    return retval == 0 && punit_evtconfig.num_evts > 0 && pmc_evtconfig.num_evts > 0;
+}
+
+/**
+ * sw_get_instance_row -- Get the address of a 'row' of instance IDs.
+ * @rownum: The row number of the Instance ID table, whose address to return.
+ * Returns: The address of the appropriate row, or NULL if rownum is bad.
+ */
+static unsigned char *sw_get_instance_row_addr(unsigned int rownum)
+{
+    if (rownum >= (sw_telem_rows_alloced - sw_telem_rows_avail)) {
+        pw_pr_error("ERROR: Cannot retrieve row Instance ID row %d\n",
+                    rownum);
+        return NULL;
+    }
+    return &sw_telem_scaled_ids[rownum * sw_max_num_cpus];
+}
+
+/**
+ * sw_free_telem_scaled_id_table - Free the allocated slots.
+ * Returns: Nothing
+ *
+ * Admittedly, a more symmetrical function name would be nice.
+ */
+static void sw_telem_release_scaled_ids(void)
+{
+    sw_telem_rows_alloced = 0;
+    sw_telem_rows_avail   = 0;
+    if (sw_telem_scaled_ids) {
+        sw_kfree(sw_telem_scaled_ids);
+    }
+    sw_telem_scaled_ids = NULL;
+}
+
+/**
+ * sw_telem_init_func - Set up the telemetry unit to retrieve a data item
+ *                        (e.g. counter).
+ * @descriptor:  The IO descriptor containing the unit and ID
+ *                        of the telemetry info to gather.
+ *
+ * Because we don't (currently) control all of the counters, we
+ * economize by seeing if it's already being collected before allocate
+ * a slot for it.
+ *
+ * Returns: PW_SUCCESS  if the telem collector can collect the requested data.
+ *         -PW_ERROR   if the the addition of that item fails.
+ */
+int sw_telem_init_func(struct sw_driver_io_descriptor *descriptor)
+{
+    struct sw_driver_telem_io_descriptor *td = &(descriptor->telem_descriptor);
+    u8  unit = td->unit;  /* Telemetry unit to use. */
+    u32 id; /* Event ID we want telemetry to track. */
+    size_t idx;  /* Index into telemetry data array of event ID to gather. */
+    const char *unit_str = unit == TELEM_PUNIT? "PUNIT" : "PMC";
+    size_t *unit_idx = &s_unit_idx[unit];
+
+    if (!telemetry_available()) {
+        return -ENXIO;
+    }
+
+    id = (u32)(td->id);
+
+    /* Check if we've already added this ID */
+    for (idx=0; idx<*unit_idx && idx<MAX_TELEM_EVENTS; ++idx) {
+        if (s_event_map[unit][idx] == id) {
+            /* Invariant: idx contains the index of the new data item. */
+            /* Save the index for later fast lookup. */
+            td->idx = (u16)idx;
+            return 0;
+        }
+    }
+
+    if (*unit_idx >= MAX_TELEM_EVENTS) {
+        pw_pr_error("Too many events %s units requested; max of %u available!\n", unit_str, MAX_TELEM_EVENTS);
+        return -E2BIG;
+    }
+    s_event_map[unit][(*unit_idx)++] = id;
+    /* Invariant: idx contains the index of the new data item. */
+    /* Save the index for later fast lookup. */
+    td->idx = (u16)idx;
+    pw_pr_debug("OK, added id = 0x%x to unit %s at entry %zu; retrieved = 0x%x\n", id, unit_str, *unit_idx-1, s_event_map[unit][*unit_idx-1]);
+
+    return 0;
+}
+
+
+/**
+ * sw_read_telem_info - Read a metric's data from the telemetry driver.
+ * @dest:               Destination (storage for the read data)
+ * @cpu:                Which CPU to read from (not used)
+ * @descriptor:         The descriptor containing the data ID to read
+ * @data_size_in_bytes: The # of bytes in the result (always 8)
+ *
+ * Returns: Nothing, but stores SW_TELEM_READ_FAIL_VALUE to dest if the read fails.
+ */
+void sw_read_telem_info(char *dest, int cpu,
+                          const sw_driver_io_descriptor_t *descriptor,
+                          u16 data_size_in_bytes)
+{
+    int  len;
+    u64 *data_dest = (u64 *)dest;
+    int retry_count;
+    const struct sw_driver_telem_io_descriptor *td = &(descriptor->telem_descriptor);
+    unsigned int idx;
+    u8 unit = td->unit;
+    bool needs_refresh = false;
+
+#define TELEM_PKT_SIZE 16  /* sizeof(struct telemetry_evtlog) + padding */
+    static struct telemetry_evtlog events[MAX_TELEM_EVENTS];
+
+    // Get the event index
+    if (IS_SCALED_ID(td)) {
+        unsigned char *scaled_ids;
+        scaled_ids = sw_get_instance_row_addr(td->idx);
+        if (scaled_ids == NULL) {
+            pw_pr_error("Sw_read_telem_info_i: Illegal row index: *%p = %d",
+                        &td->idx, td->idx);
+            *data_dest = SW_TELEM_READ_FAIL_VALUE;
+            return;  /* Don't set the dest/data buffer. */
+        }
+        idx = scaled_ids[RAW_CPU()]; /* Get per-cpu entry */
+    } else {
+        idx = td->idx;
+    }
+
+    /*
+     * Check if we need to refresh the list of values
+     */
+    LOCK(sw_telem_lock);
+    {
+        if (s_unit_iters[unit] == 0) {
+            needs_refresh = true;
+        }
+        if (++s_unit_iters[unit] == s_unit_idx[unit]) {
+            s_unit_iters[unit] = 0;
+        }
+    }
+    UNLOCK(sw_telem_lock);
+
+    /*
+     * Because of the enormous overhead of reading telemetry data from
+     * the current kernel driver, failure to read the data is not
+     * unheard of.  As such, 3 times, should the read fail.  Once we
+     * get a higher-performance read routine, we should be able to
+     * eliminate this retry (or maybe decrease it.)
+     */
+    retry_count = 3;
+    while (needs_refresh && retry_count--) {
+        len = telemetry_raw_read_eventlog(unit,
+                                          events,
+                                          sizeof(events) / TELEM_PKT_SIZE);
+
+        if ((len < 0) || (len < idx)) {
+            pw_pr_error("sw_read_telem_info_i: read failed: len=%d\n", len);
+        } else {
+            break;
+        }
+    }
+
+    if (retry_count) {
+        // TODO: Resolve if we should return something other than
+        //       SW_TELEM_READ_FAIL_VALUE, if the actual data happens to be that.
+        *data_dest = events[idx].telem_evtlog;
+    } else {
+        *data_dest = SW_TELEM_READ_FAIL_VALUE;
+    }
+}
+
+/**
+ * sw_reset_telem - Stop collecting telemetry info.
+ * @descriptor: Unused in this function
+ *
+ * Stop collecting anything extra, and give the driver back to
+ * debugfs.  Because this driver increases the sampling rate, the
+ * kernel's telemetry driver can't succesfully reset the driver unless
+ * we first drop the rate back down to a much slower rate.  This is a
+ * temporary measure, since the reset operation will then reset the
+ * sampling interval to whatever the GMIN driver wants.
+ *
+ * Return: PW_SUCCESS.
+ */
+int sw_reset_telem(const struct sw_driver_io_descriptor *descriptor)
+{
+    if (telemetry_available() && SW_TELEM_CONFIGURED()) {
+        telemetry_set_sampling_period(TELEM_SAMPLING_1S,
+                                      TELEM_SAMPLING_1S);
+        telemetry_reset_events();
+        sw_telem_release_scaled_ids();
+        memset(s_unit_idx, 0, sizeof(s_unit_idx));
+        memset(s_unit_iters, 0, sizeof(s_unit_iters));
+    }
+    return PW_SUCCESS;
+}
+
+/**
+ * sw_available_telem -- Decide if the telemetry subsystem is available for use
+ */
+bool sw_telem_available(void)
+{
+    return telemetry_available();
+};
+
+bool sw_telem_post_config(void)
+{
+    bool retval = true;
+    size_t i=0;
+    struct telemetry_evtconfig punit_evtconfig;
+    struct telemetry_evtconfig pmc_evtconfig;
+
+    if (!SW_TELEM_CONFIGURED()) {
+        return true;
+    }
+
+    memset(&punit_evtconfig, 0, sizeof(punit_evtconfig));
+    memset(&pmc_evtconfig, 0, sizeof(pmc_evtconfig));
+
+    telemetry_set_sampling_period(TELEM_SAMPLING_1S, TELEM_SAMPLING_1S);
+
+    punit_evtconfig.period = TELEM_SAMPLING_1S;
+    pmc_evtconfig.period = TELEM_SAMPLING_1S;
+
+    /* Punit */
+    punit_evtconfig.evtmap = (u32 *)&s_event_map[TELEM_PUNIT];
+    punit_evtconfig.num_evts = s_unit_idx[TELEM_PUNIT];
+    /* PMC */
+    pmc_evtconfig.evtmap = (u32 *)&s_event_map[TELEM_PMC];
+    pmc_evtconfig.num_evts = s_unit_idx[TELEM_PMC];
+
+    for (i=0; i<punit_evtconfig.num_evts; ++i) {
+        pw_pr_debug("PUNIT[%zu] = 0x%x\n", i, punit_evtconfig.evtmap[i]);
+    }
+    for (i=0; i<pmc_evtconfig.num_evts; ++i) {
+        pw_pr_debug("PMC[%zu] = 0x%x\n", i, pmc_evtconfig.evtmap[i]);
+    }
+
+    /*
+     * OK, everything done. Now update
+     */
+    if (telemetry_update_events(punit_evtconfig, pmc_evtconfig)) {
+        pw_pr_error("telemetry_update_events error");
+        retval = false;
+    } else {
+        pw_pr_debug("OK, telemetry_update_events success\n");
+    }
+
+    telemetry_set_sampling_period(TELEM_SAMPLING_1MS, TELEM_SAMPLING_1MS);
+
+    return retval;
+}
diff --git a/drivers/misc/intel/socwatch/sw_trace_notifier_provider.c b/drivers/misc/intel/socwatch/sw_trace_notifier_provider.c
new file mode 100644
index 000000000000..77c534123633
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_trace_notifier_provider.c
@@ -0,0 +1,1876 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include <linux/version.h> // "LINUX_VERSION_CODE"
+#include <linux/hrtimer.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,11,0)
+    #include <asm/cputime.h>
+#else
+    #include <linux/sched/cputime.h>
+#endif
+#include <asm/hardirq.h>
+#include <asm/local.h>
+
+#include <trace/events/power.h>
+#include <trace/events/irq.h>
+#include <trace/events/timer.h>
+#include <trace/events/power.h>
+#include <trace/events/sched.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+    #include <asm/trace/irq_vectors.h> // for the various APIC vector tracepoints (e.g. "thermal_apic", "local_timer" etc.)
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+struct pool_workqueue; // Forward declaration to avoid compiler warnings
+struct cpu_workqueue_struct; // Forward declaration to avoid compiler warnings
+#include <trace/events/workqueue.h>
+#include <linux/suspend.h> // for 'pm_notifier'
+#include <linux/cpufreq.h> // for "cpufreq_notifier"
+#include <linux/cpu.h> // for 'CPU_UP_PREPARE' etc
+
+#include "sw_collector.h"
+#include "sw_overhead_measurements.h"
+#include "sw_tracepoint_handlers.h"
+#include "sw_output_buffer.h"
+#include "sw_mem.h"
+#include "sw_trace_notifier_provider.h"
+
+/* -------------------------------------------------
+ * Compile time constants and useful macros.
+ * -------------------------------------------------
+ */
+#ifndef __get_cpu_var
+    /*
+     * Kernels >= 3.19 don't include a definition
+     * of '__get_cpu_var'. Create one now.
+     */
+    #define __get_cpu_var(var) *this_cpu_ptr(&var)
+#endif // __get_cpu_var
+
+#define BEGIN_LOCAL_IRQ_STATS_READ(p) do{	\
+    p = &__get_cpu_var(irq_stat);
+
+#define END_LOCAL_IRQ_STATS_READ(p)		\
+    }while(0)
+/*
+ * CAS{32,64}
+ */
+#define CAS32(p, o, n) ( cmpxchg((p), (o), (n)) == (o) )
+#define CAS64(p, o, n) ( cmpxchg64((p), (o), (n)) == (o) )
+/*
+ * Timer start pid accessor macros
+ */
+#ifdef CONFIG_TIMER_STATS
+    #define GET_TIMER_THREAD_ID(t) ( (t)->start_pid ) /* 'start_pid' is actually the thread ID of the thread that initialized the timer */
+#else
+    #define GET_TIMER_THREAD_ID(t) ( -1 )
+#endif // CONFIG_TIMER_STATS
+/*
+ * Tracepoint probe register/unregister functions and
+ * helper macros.
+ */
+#ifdef CONFIG_TRACEPOINTS
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,35)
+    #define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe) WARN_ON(register_trace_##name(probe))
+    #define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe) unregister_trace_##name(probe)
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(3,15,0)
+    #define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe) WARN_ON(register_trace_##name(probe, NULL))
+    #define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe) unregister_trace_##name(probe, NULL)
+#else
+    #define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe) WARN_ON(tracepoint_probe_register(node->tp, probe, NULL))
+    #define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe) tracepoint_probe_unregister(node->tp, probe, NULL)
+#endif
+#else // CONFIG_TRACEPOINTS
+    #define DO_REGISTER_SW_TRACEPOINT_PROBE(...) /* NOP */
+    #define DO_UNREGISTER_SW_TRACEPOINT_PROBE(...) /* NOP */
+#endif // CONFIG_TRACEPOINTS
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,35)
+    #define _DEFINE_PROBE_FUNCTION(name, ...) static void name(__VA_ARGS__)
+#else
+    #define _DEFINE_PROBE_FUNCTION(name, ...) static void name(void *ignore, __VA_ARGS__)
+#endif
+#define DEFINE_PROBE_FUNCTION(x) _DEFINE_PROBE_FUNCTION(x)
+
+/*
+ * Tracepoint probe function parameters.
+ * These tracepoint signatures depend on kernel version.
+ */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36)
+    #define PROBE_TPS_PARAMS sw_probe_power_start_i, unsigned int type, unsigned int state
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    #define PROBE_TPS_PARAMS sw_probe_power_start_i, unsigned int type, unsigned int state, unsigned int cpu_id
+#else
+    #define PROBE_TPS_PARAMS sw_probe_cpu_idle_i, unsigned int state, unsigned int cpu_id
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    #define PROBE_TPF_PARAMS sw_probe_power_frequency_i, unsigned int type, unsigned int state
+#else
+    #define PROBE_TPF_PARAMS sw_probe_cpu_frequency_i, unsigned int new_freq, unsigned int cpu
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,35)
+    #define PROBE_SCHED_WAKEUP_PARAMS sw_probe_sched_wakeup_i, struct rq *rq, struct task_struct *task, int success
+#else
+    #define PROBE_SCHED_WAKEUP_PARAMS sw_probe_sched_wakeup_i, struct task_struct *task, int success
+#endif
+
+#if DO_ANDROID
+    #if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+        #define PROBE_WAKE_LOCK_PARAMS sw_probe_wake_lock_i, struct wake_lock *lock
+        #define PROBE_WAKE_UNLOCK_PARAMS sw_probe_wake_unlock_i, struct wake_unlock *unlock
+    #else
+        #define PROBE_WAKE_LOCK_PARAMS sw_probe_wakeup_source_activate_i, const char *name, unsigned int state
+        #define PROBE_WAKE_UNLOCK_PARAMS sw_probe_wakeup_source_deactivate_i, const char *name, unsigned int state
+    #endif // version
+#endif // DO_ANDROID
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,35)
+    #define PROBE_WORKQUEUE_PARAMS sw_probe_workqueue_execution_i, struct task_struct *wq_thread, struct work_struct *work
+#else
+    #define PROBE_WORKQUEUE_PARAMS sw_probe_workqueue_execute_start_i, struct work_struct *work
+#endif
+
+#define PROBE_SCHED_SWITCH_PARAMS sw_probe_sched_switch_i, struct task_struct *prev, struct task_struct *next
+/*
+ * These tracepoint signatures are independent of kernel version.
+ */
+#define PROBE_IRQ_PARAMS sw_probe_irq_handler_entry_i, int irq, struct irqaction *action
+#define PROBE_TIMER_ARGS sw_probe_timer_expire_entry_i, struct timer_list *t
+#define PROBE_HRTIMER_PARAMS sw_probe_hrtimer_expire_entry_i, struct hrtimer *hrt, ktime_t *now
+#define PROBE_PROCESS_FORK_PARAMS sw_probe_sched_process_fork_i, struct task_struct *parent, struct task_struct *child
+#define PROBE_SCHED_PROCESS_EXIT_PARAMS sw_probe_sched_process_exit_i, struct task_struct *task
+#define PROBE_THERMAL_APIC_ENTRY_PARAMS sw_probe_thermal_apic_entry_i, int vector
+#define PROBE_THERMAL_APIC_EXIT_PARAMS sw_probe_thermal_apic_exit_i, int vector
+
+#define IS_VALID_WAKEUP_EVENT(cpu) ({ \
+        bool *per_cpu_event = &per_cpu(sw_is_valid_wakeup_event, (cpu)); \
+        bool old_value = CAS32(per_cpu_event, true, sw_wakeup_event_flag); \
+        old_value; \
+        })
+#define SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu) (IS_VALID_WAKEUP_EVENT(cpu))
+#define RESET_VALID_WAKEUP_EVENT_COUNTER(cpu) (per_cpu(sw_is_valid_wakeup_event, (cpu)) = true)
+
+#define NUM_TRACEPOINT_NODES SW_ARRAY_SIZE(s_trace_collector_lists)
+#define NUM_VALID_TRACEPOINTS (NUM_TRACEPOINT_NODES - 1) /* "-1" for IPI */
+#define FOR_EACH_TRACEPOINT_NODE(idx, node) for (idx=0; idx<NUM_TRACEPOINT_NODES && (node=&s_trace_collector_lists[idx]); ++idx)
+
+#define FOR_EACH_NOTIFIER_NODE(idx, node) for (idx=0; idx<SW_ARRAY_SIZE(s_notifier_collector_lists) && (node = &s_notifier_collector_lists[idx]); ++idx)
+/*
+ * Use these macros if all tracepoint ID numbers ARE contiguous from 0 -- max tracepoint ID #
+ */
+#if 0
+#define IS_VALID_TRACE_NOTIFIER_ID(id) ( (id) >= 0 && (id) < SW_ARRAY_SIZE(s_trace_collector_lists) )
+#define GET_COLLECTOR_TRACE_NODE(id) (&s_trace_collector_lists[id])
+#define FOR_EACH_trace_notifier_id(idx) for (idx=0; idx < SW_ARRAY_SIZE(s_trace_collector_lists); ++idx)
+#endif // if 0
+/*
+ * Use these macros if all tracepoint ID numbers are NOT contiguous from 0 -- max tracepoint ID #
+ */
+#define GET_COLLECTOR_TRACE_NODE(idx) ({int __idx=0; struct sw_trace_notifier_data *__node=NULL, *__retVal=NULL; \
+        FOR_EACH_TRACEPOINT_NODE(__idx, __node) { \
+            if ((idx) == GET_TRACE_NOTIFIER_ID(__node)) { \
+                __retVal = __node; break; \
+            } \
+        } \
+        __retVal;})
+#define IS_VALID_TRACE_NOTIFIER_ID(idx) (GET_COLLECTOR_TRACE_NODE(idx) != NULL)
+
+#define GET_COLLECTOR_NOTIFIER_NODE(idx) ({int __idx=0; struct sw_trace_notifier_data *__node=NULL, *__retVal=NULL; \
+        FOR_EACH_NOTIFIER_NODE(__idx, __node) { \
+            if ((idx) == GET_TRACE_NOTIFIER_ID(__node)) { \
+                __retVal = __node; break; \
+            } \
+        } \
+        __retVal;})
+#define IS_VALID_NOTIFIER_ID(idx) (GET_COLLECTOR_NOTIFIER_NODE(idx) != NULL)
+
+/* -------------------------------------------------
+ * Local function declarations.
+ * -------------------------------------------------
+ */
+/*
+ * The tracepoint registration functions.
+ */
+int sw_register_trace_cpu_idle_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_cpu_idle_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_cpu_frequency_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_cpu_frequency_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_wakeup_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_wakeup_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_process_fork_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_process_fork_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_process_exit_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_process_exit_i(struct sw_trace_notifier_data *node);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+    int sw_register_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node);
+    int sw_unregister_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node);
+    int sw_register_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node);
+    int sw_unregister_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node);
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+#if DO_ANDROID
+    #if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+        int sw_register_trace_wake_lock_i(struct sw_trace_notifier_data *node);
+        int sw_unregister_trace_wake_lock_i(struct sw_trace_notifier_data *node);
+        int sw_register_trace_wake_unlock_i(struct sw_trace_notifier_data *node);
+        int sw_unregister_trace_wake_unlock_i(struct sw_trace_notifier_data *node);
+    #else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+        int sw_register_trace_wakeup_source_activate_i(struct sw_trace_notifier_data *node);
+        int sw_unregister_trace_wakeup_source_activate_i(struct sw_trace_notifier_data *node);
+        int sw_register_trace_wakeup_source_deactivate_i(struct sw_trace_notifier_data *node);
+        int sw_unregister_trace_wakeup_source_deactivate_i(struct sw_trace_notifier_data *node);
+    #endif // LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+#endif // DO_ANDROID
+int sw_register_trace_workqueue_execution_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_workqueue_execution_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_switch_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_switch_i(struct sw_trace_notifier_data *node);
+int sw_register_pm_notifier_i(struct sw_trace_notifier_data *node);
+int sw_unregister_pm_notifier_i(struct sw_trace_notifier_data *node);
+int sw_register_cpufreq_notifier_i(struct sw_trace_notifier_data *node);
+int sw_unregister_cpufreq_notifier_i(struct sw_trace_notifier_data *node);
+int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node);
+int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node);
+void sw_handle_sched_wakeup_i(struct sw_collector_data *node, int source_cpu, int target_cpu);
+void sw_handle_timer_wakeup_helper_i(struct sw_collector_data *curr, struct sw_trace_notifier_data *node,
+                                     pid_t tid);
+void sw_handle_apic_timer_wakeup_i(struct sw_collector_data *node);
+void sw_handle_workqueue_wakeup_helper_i(int cpu, struct sw_collector_data *node);
+void sw_handle_sched_switch_helper_i(void);
+void sw_tps_apic_i(int cpu);
+void sw_tps_tps_i(int cpu);
+void sw_tps_wakeup_i(int cpu);
+void sw_tps_i(void);
+void sw_tpf_i(int cpu, struct sw_trace_notifier_data *node);
+void sw_process_fork_exit_helper_i(struct sw_collector_data *node, struct task_struct *task, bool is_fork);
+void sw_produce_wakelock_msg_i(int cpu, struct sw_collector_data *node, const char *name,
+                               int type, u64 timeout, int pid, int tid, const char *proc_name);
+u64 sw_my_local_arch_irq_stats_cpu_i(void);
+
+/*
+ * The tracepoint probes.
+ */
+/*
+ * The tracepoint handlers.
+ */
+void sw_handle_trace_notifier_i(struct sw_trace_notifier_data *node);
+void sw_handle_trace_notifier_on_cpu_i(int cpu, struct sw_trace_notifier_data *node);
+void sw_handle_reset_messages_i(struct sw_trace_notifier_data *node);
+
+/* -------------------------------------------------
+ * Variable definitions.
+ * -------------------------------------------------
+ */
+/*
+ * For overhead measurements.
+ */
+DECLARE_OVERHEAD_VARS(sw_handle_timer_wakeup_helper_i); // for the "timer_expire" family of probes
+DECLARE_OVERHEAD_VARS(sw_handle_irq_wakeup_i); // for IRQ wakeups
+DECLARE_OVERHEAD_VARS(sw_handle_sched_wakeup_i); // for SCHED
+DECLARE_OVERHEAD_VARS(sw_tps_i); // for TPS
+DECLARE_OVERHEAD_VARS(sw_tpf_i); // for TPF
+DECLARE_OVERHEAD_VARS(sw_process_fork_exit_helper_i);
+#if DO_ANDROID
+    DECLARE_OVERHEAD_VARS(sw_handle_wakelock_i); // for wake lock/unlock
+#endif // DO_ANDROID
+DECLARE_OVERHEAD_VARS(sw_handle_workqueue_wakeup_helper_i);
+DECLARE_OVERHEAD_VARS(sw_handle_sched_switch_helper_i);
+/*
+ * Per-cpu wakeup counters.
+ * Used to decide which wakeup event is the first to occur after a
+ * core wakes up from a C-state.
+ * Set to 'true' in TPS probe
+ */
+static DEFINE_PER_CPU(bool, sw_is_valid_wakeup_event) = {true};
+/*
+ * Per-cpu counts of the number of times the local APIC fired.
+ * We need a separate count because some apic timer fires don't seem
+ * to result in hrtimer/timer expires
+ */
+static DEFINE_PER_CPU(u64, sw_num_local_apic_timer_inters) = 0;
+/*
+ * Flag value to use to decide if the event is a valid wakeup event.
+ * Set to 'false' in TPS probe.
+ */
+static bool sw_wakeup_event_flag = true;
+/*
+ * Scheduler-based polling emulation.
+ */
+DEFINE_PER_CPU(unsigned long, sw_pcpu_polling_jiff);
+pw_u16_t sw_min_polling_interval_msecs;
+
+/*
+ * IDs for supported tracepoints.
+ */
+enum sw_trace_id {
+    SW_TRACE_ID_CPU_IDLE,
+    SW_TRACE_ID_CPU_FREQUENCY,
+    SW_TRACE_ID_IRQ_HANDLER_ENTRY,
+    SW_TRACE_ID_TIMER_EXPIRE_ENTRY,
+    SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY,
+    SW_TRACE_ID_SCHED_WAKEUP,
+    SW_TRACE_ID_IPI,
+    SW_TRACE_ID_SCHED_PROCESS_FORK,
+    SW_TRACE_ID_SCHED_PROCESS_EXIT,
+    SW_TRACE_ID_THERMAL_APIC_ENTRY,
+    SW_TRACE_ID_THERMAL_APIC_EXIT,
+    SW_TRACE_ID_WAKE_LOCK,
+    SW_TRACE_ID_WAKE_UNLOCK,
+    SW_TRACE_ID_WORKQUEUE_EXECUTE_START,
+    SW_TRACE_ID_SCHED_SWITCH,
+};
+/*
+ * IDs for supported notifiers.
+ */
+enum sw_notifier_id {
+    SW_NOTIFIER_ID_SUSPEND, // TODO: change name?
+    SW_NOTIFIER_ID_SUSPEND_ENTER,
+    SW_NOTIFIER_ID_SUSPEND_EXIT,
+    SW_NOTIFIER_ID_HIBERNATE,
+    SW_NOTIFIER_ID_HIBERNATE_ENTER,
+    SW_NOTIFIER_ID_HIBERNATE_EXIT,
+    SW_NOTIFIER_ID_COUNTER_RESET,
+    SW_NOTIFIER_ID_CPUFREQ,
+    SW_NOTIFIER_ID_HOTCPU,
+};
+/*
+ * Names for supported tracepoints. A tracepoint
+ * 'name' consists of two strings: a "kernel" string
+ * that is used to locate the tracepoint within the kernel
+ * and an "abstract" string, that is used by Ring-3 to
+ * specify which tracepoints to use during a collection.
+ */
+static const struct sw_trace_notifier_name s_trace_names[] = {
+    [SW_TRACE_ID_CPU_IDLE] = {"cpu_idle", "CPU-IDLE"},
+    [SW_TRACE_ID_CPU_FREQUENCY] = {"cpu_frequency", "CPU-FREQUENCY"},
+    [SW_TRACE_ID_IRQ_HANDLER_ENTRY] = {"irq_handler_entry", "IRQ-ENTRY"},
+    [SW_TRACE_ID_TIMER_EXPIRE_ENTRY] = {"timer_expire_entry", "TIMER-ENTRY"},
+    [SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY] = {"hrtimer_expire_entry", "HRTIMER-ENTRY"},
+    [SW_TRACE_ID_SCHED_WAKEUP] = {"sched_wakeup", "SCHED-WAKEUP"},
+    [SW_TRACE_ID_IPI] = {NULL, "IPI"},
+    [SW_TRACE_ID_SCHED_PROCESS_FORK] = {"sched_process_fork", "PROCESS-FORK"},
+    [SW_TRACE_ID_SCHED_PROCESS_EXIT] = {"sched_process_exit", "PROCESS-EXIT"},
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+        [SW_TRACE_ID_THERMAL_APIC_ENTRY] = {"thermal_apic_entry", "THERMAL-THROTTLE-ENTRY"},
+        [SW_TRACE_ID_THERMAL_APIC_EXIT] = {"thermal_apic_exit", "THERMAL-THROTTLE-EXIT"},
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+#if DO_ANDROID
+    #if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+            [SW_TRACE_ID_WAKE_LOCK] = {"wake_lock", "WAKE-LOCK"},
+            [SW_TRACE_ID_WAKE_UNLOCK] = {"wake_unlock", "WAKE-UNLOCK"},
+    #else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+            [SW_TRACE_ID_WAKE_LOCK] = {"wakeup_source_activate", "WAKE-LOCK"},
+            [SW_TRACE_ID_WAKE_UNLOCK] = {"wakeup_source_deactivate", "WAKE-UNLOCK"},
+    #endif
+#endif
+    [SW_TRACE_ID_WORKQUEUE_EXECUTE_START] = {"workqueue_execute_start", "WORKQUEUE-START"},
+    [SW_TRACE_ID_SCHED_SWITCH] = {"sched_switch", "CONTEXT-SWITCH"},
+};
+
+/*
+ * Names for supported notifiers. A notifier
+ * 'name' consists of two strings: an unused "kernel" string
+ * and an "abstract" string, that is used by Ring-3 to
+ * specify which notifiers to use during a collection.
+ */
+static const struct sw_trace_notifier_name s_notifier_names[] = {
+    [SW_NOTIFIER_ID_SUSPEND] = {"suspend_notifier" /* don't care */, "SUSPEND-NOTIFIER"},
+    [SW_NOTIFIER_ID_SUSPEND_ENTER] = {NULL, "SUSPEND-ENTER"},
+    [SW_NOTIFIER_ID_SUSPEND_EXIT] = {NULL, "SUSPEND-EXIT"},
+    [SW_NOTIFIER_ID_HIBERNATE] = {"hibernate_notifier" /* don't care */, "HIBERNATE-NOTIFIER"},
+    [SW_NOTIFIER_ID_HIBERNATE_ENTER] = {NULL, "HIBERNATE-ENTER"},
+    [SW_NOTIFIER_ID_HIBERNATE_EXIT] = {NULL, "HIBERNATE-EXIT"},
+    [SW_NOTIFIER_ID_COUNTER_RESET] = {NULL, "COUNTER-RESET"},
+    [SW_NOTIFIER_ID_CPUFREQ] = {"cpufreq_notifier" /* don't care */, "CPUFREQ-NOTIFIER"},
+    [SW_NOTIFIER_ID_HOTCPU] = {"hotcpu_notifier" /* don't care */, "HOTCPU-NOTIFIER"},
+};
+
+#ifdef CONFIG_TRACEPOINTS
+/*
+ * A list of supported tracepoints.
+ */
+static struct sw_trace_notifier_data s_trace_collector_lists[] = {
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_CPU_IDLE], &sw_register_trace_cpu_idle_i, &sw_unregister_trace_cpu_idle_i, NULL},
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_CPU_FREQUENCY], &sw_register_trace_cpu_frequency_i, &sw_unregister_trace_cpu_frequency_i, NULL},
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_IRQ_HANDLER_ENTRY], &sw_register_trace_irq_handler_entry_i, &sw_unregister_trace_irq_handler_entry_i, NULL},
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_TIMER_EXPIRE_ENTRY], &sw_register_trace_timer_expire_entry_i, &sw_unregister_trace_timer_expire_entry_i, NULL},
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY], &sw_register_trace_hrtimer_expire_entry_i, &sw_unregister_trace_hrtimer_expire_entry_i, NULL},
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_SCHED_WAKEUP], &sw_register_trace_sched_wakeup_i, &sw_unregister_trace_sched_wakeup_i, NULL},
+    /* Placeholder for IPI -- no tracepoints associated with it! */
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_IPI], NULL, NULL, NULL},
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_SCHED_PROCESS_FORK], &sw_register_trace_sched_process_fork_i, &sw_unregister_trace_sched_process_fork_i, NULL},
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_SCHED_PROCESS_EXIT], &sw_register_trace_sched_process_exit_i, &sw_unregister_trace_sched_process_exit_i, NULL},
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+        /*
+         * For thermal throttling.
+         * We probably only need one of either 'entry' or 'exit'. Use
+         * both, until we decide which one to keep. Note that
+         * tracepoint IDs for these, and subsequent tracepoints
+         * (e.g. 'wake_lock') will change once we've picked which
+         * one to use.
+         */
+        {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_THERMAL_APIC_ENTRY], &sw_register_trace_thermal_apic_entry_i, &sw_unregister_trace_thermal_apic_entry_i, NULL},
+        {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_THERMAL_APIC_EXIT], &sw_register_trace_thermal_apic_exit_i, &sw_unregister_trace_thermal_apic_exit_i, NULL},
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+    /* Wakelocks have multiple tracepoints, depending on kernel version */
+#if DO_ANDROID
+    #if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+            {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_LOCK], &sw_register_trace_wake_lock_i, &sw_unregister_trace_wake_lock_i, NULL},
+            {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_UNLOCK], &sw_register_trace_wake_unlock_i, &sw_unregister_trace_wake_unlock_i, NULL},
+    #else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+            {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_LOCK], &sw_register_trace_wakeup_source_activate_i, &sw_unregister_trace_wakeup_source_activate_i, NULL},
+            {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_UNLOCK], &sw_register_trace_wakeup_source_deactivate_i, &sw_unregister_trace_wakeup_source_deactivate_i, NULL},
+    #endif // LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)
+#endif // DO_ANDROID
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WORKQUEUE_EXECUTE_START], &sw_register_trace_workqueue_execution_i, &sw_unregister_trace_workqueue_execution_i, NULL},
+    {SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_SCHED_SWITCH], &sw_register_trace_sched_switch_i, &sw_unregister_trace_sched_switch_i, NULL},
+};
+/*
+ * List of supported notifiers.
+ */
+static struct sw_trace_notifier_data s_notifier_collector_lists[] = {
+    {SW_TRACE_COLLECTOR_NOTIFIER, &s_notifier_names[SW_NOTIFIER_ID_SUSPEND], &sw_register_pm_notifier_i, &sw_unregister_pm_notifier_i, NULL, true /* always register */},
+    /* Placeholder for suspend enter/exit -- these will be called from within the pm notifier */
+    {SW_TRACE_COLLECTOR_NOTIFIER, &s_notifier_names[SW_NOTIFIER_ID_SUSPEND_ENTER], NULL, NULL, NULL},
+    {SW_TRACE_COLLECTOR_NOTIFIER, &s_notifier_names[SW_NOTIFIER_ID_SUSPEND_EXIT], NULL, NULL, NULL},
+    /* Placeholder for hibernate enter/exit -- these will be called from within the pm notifier */
+    {SW_TRACE_COLLECTOR_NOTIFIER, &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE], NULL, NULL, NULL},
+    {SW_TRACE_COLLECTOR_NOTIFIER, &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE_ENTER], NULL, NULL, NULL},
+    {SW_TRACE_COLLECTOR_NOTIFIER, &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE_EXIT], NULL, NULL, NULL},
+    {SW_TRACE_COLLECTOR_NOTIFIER, &s_notifier_names[SW_NOTIFIER_ID_COUNTER_RESET], NULL, NULL, NULL},
+    {SW_TRACE_COLLECTOR_NOTIFIER, &s_notifier_names[SW_NOTIFIER_ID_CPUFREQ], &sw_register_cpufreq_notifier_i, &sw_unregister_cpufreq_notifier_i},
+};
+/*
+ * Special entry for CPU notifier (i.e. "hotplug" notifier)
+ * We don't want these to be visible to the user.
+ */
+static struct sw_trace_notifier_data s_hotplug_notifier_data = {
+    SW_TRACE_COLLECTOR_NOTIFIER, &s_notifier_names[SW_NOTIFIER_ID_HOTCPU], &sw_register_hotcpu_notifier_i, &sw_unregister_hotcpu_notifier_i, NULL, true /* always register */
+};
+
+#else // !CONFIG_TRACEPOINTS
+/*
+ * A list of supported tracepoints.
+ */
+static struct sw_trace_notifier_data s_trace_collector_lists[] = { /* EMPTY */ };
+/*
+ * List of supported notifiers.
+ */
+static struct sw_trace_notifier_data s_notifier_collector_lists[] = { /* EMPTY */ };
+
+#endif // CONFIG_TRACEPOINTS
+
+
+/*
+ * Macros to retrieve tracepoint and notifier IDs.
+ */
+#define GET_TRACE_ID_FROM_NODE(node) ( (node)->name - s_trace_names )
+#define GET_NOTIFIER_ID_FROM_NODE(node) ( (node)->name - s_notifier_names )
+
+#define GET_TRACE_NOTIFIER_ID(node) (int)( ( (node)->type == SW_TRACE_COLLECTOR_TRACEPOINT) ? GET_TRACE_ID_FROM_NODE(node) : GET_NOTIFIER_ID_FROM_NODE(node) )
+
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+/*
+ * Retrieve a TSC value
+ */
+static inline u64 sw_tscval(void)
+{
+    unsigned int low, high;
+    asm volatile("rdtsc" : "=a" (low), "=d" (high));
+    return low | ((unsigned long long)high) << 32;
+};
+u64 sw_timestamp(void)
+{
+    struct timespec ts;
+    getnstimeofday(&ts);
+    return (ts.tv_sec * 1000000000ULL + ts.tv_nsec);
+}
+/*
+ * Basically the same as arch/x86/kernel/irq.c --> "arch_irq_stat_cpu(cpu)"
+ */
+u64 sw_my_local_arch_irq_stats_cpu_i(void)
+{
+    u64 sum = 0;
+    irq_cpustat_t *stats;
+#ifdef __arm__
+    int i=0;
+#endif
+    BEGIN_LOCAL_IRQ_STATS_READ(stats);
+    {
+#ifndef __arm__
+        sum += stats->__nmi_count;
+// #ifdef CONFIG_X86_LOCAL_APIC
+        sum += stats->apic_timer_irqs;
+// #endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 34)
+        sum += stats->x86_platform_ipis;
+#endif // 2,6,34
+        sum += stats->apic_perf_irqs;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,5,0)
+        sum += stats->apic_irq_work_irqs;
+#endif // 3,5,0
+#ifdef CONFIG_SMP
+        sum += stats->irq_call_count;
+        sum += stats->irq_resched_count;
+        sum += stats->irq_tlb_count;
+#endif
+#ifdef CONFIG_X86_THERMAL_VECTOR
+        sum += stats->irq_thermal_count;
+#endif
+        sum += stats->irq_spurious_count; // should NEVER be non-zero!!!
+#else
+        sum += stats->__softirq_pending;
+#ifdef CONFIG_SMP
+        for (i=0; i<NR_IPI; ++i) {
+            sum += stats->ipi_irqs[i];
+        }
+#endif
+#ifdef CONFIG_X86_MCE
+        sum += stats->mce_exception_count;
+        sum += stats->mce_poll_count;
+#endif
+#endif
+    }
+    END_LOCAL_IRQ_STATS_READ(stats);
+    return sum;
+};
+
+/*
+ * Generic tracepoint/notifier handling function.
+ */
+void sw_handle_trace_notifier_i(struct sw_trace_notifier_data *node)
+{
+    struct sw_collector_data *curr = NULL;
+    if (!node) {
+        return;
+    }
+    list_for_each_entry(curr, &node->list, list) {
+        pw_pr_debug("DEBUG: handling message\n");
+        sw_handle_per_cpu_msg(curr);
+    }
+};
+/*
+ * Generic tracepoint/notifier handling function.
+ */
+void sw_handle_trace_notifier_on_cpu_i(int cpu, struct sw_trace_notifier_data *node)
+{
+    struct sw_collector_data *curr = NULL;
+    if (!node) {
+        return;
+    }
+    list_for_each_entry(curr, &node->list, list) {
+        sw_handle_per_cpu_msg_on_cpu(cpu, curr);
+    }
+};
+void sw_handle_reset_messages_i(struct sw_trace_notifier_data *node)
+{
+    struct sw_collector_data *curr = NULL;
+    if (!node) {
+        return;
+    }
+    list_for_each_entry(curr, &node->list, list) {
+        pw_pr_debug("Handling message of unknown cpumask on cpu %d\n", RAW_CPU());
+        sw_schedule_work(&curr->cpumask, &sw_handle_per_cpu_msg, curr);
+    }
+}
+/*
+ * Tracepoint helpers.
+ */
+/*
+ * IRQ wakeup handling function.
+ */
+void sw_handle_irq_wakeup_i(struct sw_collector_data *node, int irq)
+{
+    int cpu = RAW_CPU();
+    sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu, node->per_msg_payload_size);
+    // char *dst_vals = (char *)(unsigned long)msg->p_payload;
+    char *dst_vals = msg->p_payload;
+
+    // msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+    msg->cpuidx = cpu;
+
+    /*
+     * IRQ handling ==> only return the irq number
+     */
+    *( (int *)dst_vals) = irq;
+
+    if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+        pw_pr_warn("WARNING: could NOT produce message!\n");
+    }
+};
+/*
+ * TIMER wakeup handling funtion.
+ */
+void sw_handle_timer_wakeup_i(struct sw_collector_data *node, pid_t pid, pid_t tid)
+{
+    int cpu = RAW_CPU();
+    sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu, node->per_msg_payload_size);
+    // char *dst_vals = (char *)(unsigned long)msg->p_payload;
+    char *dst_vals = msg->p_payload;
+
+    // msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+    msg->cpuidx = cpu;
+
+    /*
+     * TIMER handling ==> only return the pid, tid
+     */
+    *( (int *)dst_vals) = pid; dst_vals += sizeof(pid);
+    *( (int *)dst_vals) = tid;
+
+    if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+        pw_pr_warn("WARNING: could NOT produce message!\n");
+    }
+    pw_pr_debug("HANDLED timer expire for %d, %d\n", pid, tid);
+};
+/*
+ * Helper function for {hr}timer expires. Required for overhead tracking.
+ */
+void sw_handle_timer_wakeup_helper_i(struct sw_collector_data *curr, struct sw_trace_notifier_data *node, pid_t tid)
+{
+    pid_t pid = -1;
+    if (tid == 0) {
+        pid = 0;
+    } else {
+        struct task_struct *task =  pid_task(find_pid_ns(tid, &init_pid_ns), PIDTYPE_PID);
+        if (likely(task)) {
+            pid = task->tgid;
+        }
+    }
+    list_for_each_entry(curr, &node->list, list) {
+        sw_handle_timer_wakeup_i(curr, pid, tid);
+    }
+};
+/*
+ * SCHED wakeup handling funtion.
+ */
+void sw_handle_sched_wakeup_i(struct sw_collector_data *node, int source_cpu, int target_cpu)
+{
+    int cpu = source_cpu;
+    sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu, node->per_msg_payload_size);
+    // char *dst_vals = (char *)(unsigned long)msg->p_payload;
+    char *dst_vals = msg->p_payload;
+
+    // msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+    msg->cpuidx = source_cpu;
+
+    /*
+     * sched handling ==> only return the source, target CPUs
+     */
+    *( (int *)dst_vals) = source_cpu; dst_vals += sizeof(source_cpu);
+    *( (int *)dst_vals) = target_cpu;
+
+    if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_NONE)) {
+        pw_pr_warn("WARNING: could NOT produce message!\n");
+    }
+};
+/*
+ * APIC timer wakeup
+ */
+void sw_handle_apic_timer_wakeup_i(struct sw_collector_data *node)
+{
+    /*
+     * Send an empty message back to Ring-3
+     */
+    int cpu = RAW_CPU();
+    sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu, node->per_msg_payload_size);
+    // char *dst_vals = (char *)(unsigned long)msg->p_payload;
+
+    // msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+    msg->cpuidx = cpu;
+
+    if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+        pw_pr_warn("WARNING: could NOT produce message!\n");
+    }
+    pw_pr_debug("HANDLED APIC timer wakeup for cpu = %d\n", cpu);
+};
+/*
+ * Helper function for workqueue executions. Required for overhead tracking.
+ */
+void sw_handle_workqueue_wakeup_helper_i(int cpu, struct sw_collector_data *node)
+{
+    sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu, node->per_msg_payload_size);
+
+    // msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+    msg->cpuidx = cpu;
+
+    /*
+     * Workqueue wakeup ==> empty message.
+     */
+    if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+        pw_pr_error("WARNING: could NOT produce message!\n");
+    }
+};
+/*
+ * Helper function for sched_switch. Required for overhead tracking.
+ */
+void sw_handle_sched_switch_helper_i(void)
+{
+    static struct sw_trace_notifier_data *node = NULL;
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_SWITCH);
+        pw_pr_debug("SCHED SWITCH NODE = %p\n", node);
+    }
+    if (!node) {
+        return;
+    }
+    preempt_disable();
+    {
+        struct sw_collector_data *curr = NULL;
+        list_for_each_entry(curr, &node->list, list) {
+            unsigned long curr_jiff = jiffies, prev_jiff = curr->last_update_jiffies;
+            unsigned long delta_msecs = jiffies_to_msecs(curr_jiff) - jiffies_to_msecs(prev_jiff);
+            struct cpumask *mask = &curr->cpumask;
+            u16 timeout = curr->info->sampling_interval_msec;
+
+            if (!timeout) {
+                timeout = sw_min_polling_interval_msecs;
+            }
+            /* Has there been enough time since the last collection point? */
+            if (delta_msecs < timeout) {
+                continue;
+            }
+            /* Update timestamp and handle message */
+            if (cpumask_test_cpu(RAW_CPU(), mask) /* This msg must be handled on the current CPU */
+                    || cpumask_empty(mask) /* This msg may be handled by any CPU */) {
+                if (!CAS64(&curr->last_update_jiffies, prev_jiff, curr_jiff)) {
+                    /*
+                     * CAS failure should only be possible for messages that can be handled
+                     * on any CPU, in which case it indicates a different CPU already handled
+                     * this message.
+                     */
+                    continue;
+                }
+                sw_handle_per_cpu_msg_no_sched(curr);
+            }
+        }
+    }
+    preempt_enable();
+};
+
+/*
+ * Probe functions.
+ */
+/*
+ * 1. TPS
+ */
+/*
+ * Check IPI wakeups within the cpu_idle tracepoint.
+ */
+void sw_tps_apic_i(int cpu)
+{
+    static struct sw_trace_notifier_data *apic_timer_node = NULL;
+    if (unlikely(apic_timer_node == NULL)) {
+        apic_timer_node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_IPI);
+        pw_pr_debug("apic NODE = %p\n", apic_timer_node);
+    }
+    if (apic_timer_node) {
+        bool local_apic_timer_fired = false;
+        u64 curr_num_local_apic = sw_my_local_arch_irq_stats_cpu_i();
+        u64 *old_num_local_apic = &__get_cpu_var(sw_num_local_apic_timer_inters);
+
+        if (*old_num_local_apic && (*old_num_local_apic != curr_num_local_apic)) {
+            local_apic_timer_fired = true;
+        }
+        *old_num_local_apic = curr_num_local_apic;
+
+        if (local_apic_timer_fired && SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+            struct sw_collector_data *curr = NULL;
+            list_for_each_entry(curr, &apic_timer_node->list, list) {
+                sw_handle_apic_timer_wakeup_i(curr);
+            }
+        }
+    }
+};
+/*
+ * Perform any user-defined tasks within the
+ * cpu_idle tracepoint.
+ */
+void sw_tps_tps_i(int cpu)
+{
+    static struct sw_trace_notifier_data *tps_node = NULL;
+    if (unlikely(tps_node == NULL)) {
+        tps_node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_CPU_IDLE);
+        pw_pr_debug("TPS NODE = %p\n", tps_node);
+    }
+    sw_handle_trace_notifier_i(tps_node);
+};
+/*
+ * Perform any wakeup-related tasks within the
+ * cpu_idle tracepoint.
+ */
+void sw_tps_wakeup_i(int cpu)
+{
+    /*
+     * For now, assume we will always have to
+     * do some wakeup book keeping. Later, we'll
+     * need to detect if the user requested wakeups.
+     */
+    sw_wakeup_event_flag  = false;
+    RESET_VALID_WAKEUP_EVENT_COUNTER(cpu);
+};
+void sw_tps_i(void)
+{
+    /*
+     * Update: FIRST handle IPI wakeups
+     * THEN handle TPS
+     */
+    int cpu = RAW_CPU();
+    sw_tps_apic_i(cpu);
+    sw_tps_tps_i(cpu);
+    sw_tps_wakeup_i(cpu);
+};
+
+DEFINE_PROBE_FUNCTION(PROBE_TPS_PARAMS)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,38)
+    if (state == PWR_EVENT_EXIT) {
+        return;
+    }
+#endif
+    DO_PER_CPU_OVERHEAD_FUNC(sw_tps_i);
+};
+
+/*
+ * 2. TPF
+ */
+/*
+ * Helper function for overhead measurements.
+ */
+void sw_tpf_i(int cpu, struct sw_trace_notifier_data *node)
+{
+    sw_handle_trace_notifier_on_cpu_i((int)cpu, node);
+};
+
+DEFINE_PROBE_FUNCTION(PROBE_TPF_PARAMS)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    int cpu = RAW_CPU();
+#endif // version < 2.6.38
+    static struct sw_trace_notifier_data *node = NULL;
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_CPU_FREQUENCY);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+    DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
+};
+
+/*
+ * 3. IRQ handler entry
+ */
+DEFINE_PROBE_FUNCTION(PROBE_IRQ_PARAMS)
+{
+    int cpu = RAW_CPU();
+    static struct sw_trace_notifier_data *node = NULL;
+    struct sw_collector_data *curr = NULL;
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_IRQ_HANDLER_ENTRY);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+    if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+        return;
+    }
+    list_for_each_entry(curr, &node->list, list) {
+        DO_PER_CPU_OVERHEAD_FUNC(sw_handle_irq_wakeup_i, curr, irq);
+    }
+};
+/*
+ * 4. TIMER expire
+ */
+DEFINE_PROBE_FUNCTION(PROBE_TIMER_ARGS)
+{
+    int cpu = RAW_CPU();
+    static struct sw_trace_notifier_data *node = NULL;
+    struct sw_collector_data *curr = NULL;
+    pid_t tid = GET_TIMER_THREAD_ID(t);
+
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_TIMER_EXPIRE_ENTRY);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+
+    if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+        return;
+    }
+    DO_PER_CPU_OVERHEAD_FUNC(sw_handle_timer_wakeup_helper_i, curr, node, tid);
+};
+/*
+ * 5. HRTIMER expire
+ */
+DEFINE_PROBE_FUNCTION(PROBE_HRTIMER_PARAMS)
+{
+    int cpu = RAW_CPU();
+    static struct sw_trace_notifier_data *node = NULL;
+    struct sw_collector_data *curr = NULL;
+    pid_t tid = GET_TIMER_THREAD_ID(hrt);
+
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+
+    if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+        return;
+    }
+    DO_PER_CPU_OVERHEAD_FUNC(sw_handle_timer_wakeup_helper_i, curr, node, tid);
+};
+/*
+ * 6. SCHED wakeup
+ */
+DEFINE_PROBE_FUNCTION(PROBE_SCHED_WAKEUP_PARAMS)
+{
+    static struct sw_trace_notifier_data *node = NULL;
+    struct sw_collector_data *curr = NULL;
+    int target_cpu = task_cpu(task), source_cpu = RAW_CPU();
+    /*
+     * "Self-sched" samples are "don't care".
+     */
+    if (target_cpu == source_cpu) {
+        return;
+    }
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_WAKEUP);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+    /*
+     * Unlike other wakeup sources, we check the per-cpu flag
+     * of the TARGET cpu to decide if we should produce a sample.
+     */
+    if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(target_cpu)) {
+        return;
+    }
+    list_for_each_entry(curr, &node->list, list) {
+        // sw_handle_sched_wakeup_i(curr, source_cpu, target_cpu);
+        DO_PER_CPU_OVERHEAD_FUNC(sw_handle_sched_wakeup_i, curr, source_cpu, target_cpu);
+    }
+};
+/*
+ * 8. PROCESS fork
+ */
+/*
+ * Helper for PROCESS fork, PROCESS exit
+ */
+void sw_process_fork_exit_helper_i(struct sw_collector_data *node, struct task_struct *task, bool is_fork)
+{
+    int cpu = RAW_CPU();
+    pid_t pid = task->tgid, tid = task->pid;
+    const char *name = task->comm;
+    sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu, node->per_msg_payload_size);
+    char *dst_vals = msg->p_payload;
+
+    msg->cpuidx = cpu;
+
+    /*
+     * Fork/Exit ==> return pid, tid
+     * Fork ==> also return name
+     */
+    *( (int *)dst_vals) = pid; dst_vals += sizeof(pid);
+    *( (int *)dst_vals) = tid; dst_vals += sizeof(tid);
+    if (is_fork) {
+        memcpy(dst_vals, name, SW_MAX_PROC_NAME_SIZE);
+    }
+
+    if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+        pw_pr_warn("WARNING: could NOT produce message!\n");
+    }
+    pw_pr_debug("HANDLED process %s event for task: pid = %d, tid = %d, name = %s\n", is_fork ? "FORK" : "EXIT", pid, tid, name);
+};
+
+DEFINE_PROBE_FUNCTION(PROBE_PROCESS_FORK_PARAMS)
+{
+    static struct sw_trace_notifier_data *node = NULL;
+    struct sw_collector_data *curr = NULL;
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_PROCESS_FORK);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+    if (!node) {
+        return;
+    }
+    list_for_each_entry(curr, &node->list, list) {
+        DO_PER_CPU_OVERHEAD_FUNC(sw_process_fork_exit_helper_i, curr, child, true /* true ==> fork */);
+    }
+};
+/*
+ * 9. PROCESS exit
+ */
+DEFINE_PROBE_FUNCTION(PROBE_SCHED_PROCESS_EXIT_PARAMS)
+{
+    static struct sw_trace_notifier_data *node = NULL;
+    struct sw_collector_data *curr = NULL;
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_PROCESS_EXIT);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+    if (!node) {
+        return;
+    }
+    list_for_each_entry(curr, &node->list, list) {
+        DO_PER_CPU_OVERHEAD_FUNC(sw_process_fork_exit_helper_i, curr, task, false /* false ==> exit */);
+    }
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+/*
+ * 10. THERMAL_APIC entry
+ */
+DEFINE_PROBE_FUNCTION(PROBE_THERMAL_APIC_ENTRY_PARAMS)
+{
+    int cpu = RAW_CPU();
+    static struct sw_trace_notifier_data *node = NULL;
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_THERMAL_APIC_ENTRY);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+    DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
+};
+/*
+ * 10. THERMAL_APIC exit
+ */
+DEFINE_PROBE_FUNCTION(PROBE_THERMAL_APIC_EXIT_PARAMS)
+{
+    int cpu = RAW_CPU();
+    static struct sw_trace_notifier_data *node = NULL;
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_THERMAL_APIC_EXIT);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+    DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
+};
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+
+#if DO_ANDROID
+/*
+ * 11. WAKE lock / WAKEUP source activate.
+ */
+/*
+ * Helper function to produce wake lock/unlock messages.
+ */
+void sw_produce_wakelock_msg_i(int cpu, struct sw_collector_data *node,
+			       const char *name, int type, u64 timeout,
+			       int pid, int tid, const char *proc_name)
+{
+    sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu, node->per_msg_payload_size);
+    char *dst_vals = msg->p_payload;
+
+    msg->cpuidx = cpu;
+
+    /*
+     * Protocol:
+     * wakelock_timeout, wakelock_type, wakelock_name, proc_pid, proc_tid, proc_name
+     */
+    *((u64 *)dst_vals) = timeout; dst_vals += sizeof(timeout);
+    *((int *)dst_vals) = type; dst_vals += sizeof(type);
+    strncpy(dst_vals, name, SW_MAX_KERNEL_WAKELOCK_NAME_SIZE); dst_vals += SW_MAX_KERNEL_WAKELOCK_NAME_SIZE;
+
+    *((int *)dst_vals) = pid; dst_vals += sizeof(pid);
+    *((int *)dst_vals) = tid; dst_vals += sizeof(tid);
+    strncpy(dst_vals, proc_name, SW_MAX_PROC_NAME_SIZE); dst_vals += SW_MAX_PROC_NAME_SIZE;
+
+    if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+        pw_pr_warn("WARNING: could NOT produce message!\n");
+    }
+};
+/*
+ * Helper function to handle wake lock/unlock callbacks.
+ */
+void sw_handle_wakelock_i(int cpu, struct sw_trace_notifier_data *node, const char *name, int type, u64 timeout)
+{
+    int pid = PID(), tid = TID();
+    const char *proc_name = NAME();
+    struct sw_collector_data *curr = NULL;
+
+    if (!node) {
+        return;
+    }
+
+    list_for_each_entry(curr, &node->list, list) {
+        sw_produce_wakelock_msg_i(cpu, curr, name, type, timeout, pid, tid, proc_name);
+    }
+};
+DEFINE_PROBE_FUNCTION(PROBE_WAKE_LOCK_PARAMS)
+{
+    int cpu = RAW_CPU();
+    static struct sw_trace_notifier_data *node = NULL;
+    enum sw_kernel_wakelock_type type = SW_WAKE_LOCK;
+    u64 timeout = 0;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+    const char *name = lock->name;
+#endif
+
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_WAKE_LOCK);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+    /*
+     * Was this wakelock acquired with a timeout i.e.
+     * is this an auto expire wakelock?
+     */
+    if (lock->flags & (1U << 10)) {
+        type = SW_WAKE_LOCK_TIMEOUT;
+        timeout = jiffies_to_msecs(lock->expires - jiffies);
+    }
+#endif //LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+    DO_PER_CPU_OVERHEAD_FUNC(sw_handle_wakelock_i, cpu, node, name, (int)type, timeout);
+};
+/*
+ * 11. WAKE unlock / WAKEUP source deactivate.
+ */
+DEFINE_PROBE_FUNCTION(PROBE_WAKE_UNLOCK_PARAMS)
+{
+    int cpu = RAW_CPU();
+    static struct sw_trace_notifier_data *node = NULL;
+    enum sw_kernel_wakelock_type type = SW_WAKE_UNLOCK;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+    const char *name = lock->name;
+#endif
+
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_WAKE_UNLOCK);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+    DO_PER_CPU_OVERHEAD_FUNC(sw_handle_wakelock_i, cpu, node, name, (int)type, 0 /*timeout*/);
+};
+#endif // DO_ANDROID
+
+/*
+ * 12. WORKQUEUE
+ */
+DEFINE_PROBE_FUNCTION(PROBE_WORKQUEUE_PARAMS)
+{
+    int cpu = RAW_CPU();
+    static struct sw_trace_notifier_data *node = NULL;
+    struct sw_collector_data *curr = NULL;
+
+    if (unlikely(node == NULL)) {
+        node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_WORKQUEUE_EXECUTE_START);
+        pw_pr_debug("NODE = %p\n", node);
+    }
+
+    if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+        return;
+    }
+    list_for_each_entry(curr, &node->list, list) {
+        DO_PER_CPU_OVERHEAD_FUNC(sw_handle_workqueue_wakeup_helper_i, cpu, curr);
+    }
+};
+
+/*
+ * 13. SCHED switch
+ */
+DEFINE_PROBE_FUNCTION(PROBE_SCHED_SWITCH_PARAMS)
+{
+    DO_PER_CPU_OVERHEAD_FUNC(sw_handle_sched_switch_helper_i);
+};
+
+/*
+ * 1. SUSPEND notifier
+ */
+static void sw_send_pm_notification_i(enum sw_pm_action action)
+{
+    struct sw_driver_msg *msg = NULL;
+    size_t buffer_len = sizeof(*msg) + sizeof(action);
+    char *buffer = vmalloc(buffer_len);
+    if (!buffer) {
+        pw_pr_error("couldn't allocate memory when sending suspend notification!\n");
+        return;
+    }
+    msg = (struct sw_driver_msg *)buffer;
+    msg->tsc = sw_timestamp();
+    msg->cpuidx = RAW_CPU();
+    msg->plugin_id = 0; // "0" indicates a system message
+    msg->metric_id = 1; // "1" indicates a suspend/resume message (TODO)
+    msg->msg_id = 0; /* don't care; TODO: use the 'msg_id' to encode the 'action'? */
+    msg->payload_len = sizeof(action);
+    msg->p_payload = buffer + sizeof(*msg);
+    *((enum sw_pm_action *)msg->p_payload) = action;
+    if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+        pw_pr_error("couldn't produce generic message!\n");
+    }
+    printk(KERN_INFO "OK, enqueued suspend/resume notification at tsc = %llu, action = %d\n", msg->tsc, action);
+    vfree(buffer);
+}
+
+static u64 sw_pm_enter_tsc = 0;
+static bool sw_is_reset_i(void)
+{
+    /*
+     * TODO: rely on checking the IA32_FIXED_CTR2 instead?
+     */
+    u64 curr_tsc = sw_tscval();
+    bool is_reset = sw_pm_enter_tsc > curr_tsc;
+
+    pw_pr_force("DEBUG: curr tsc = %llu, prev tsc = %llu, is reset = %s\n", curr_tsc, sw_pm_enter_tsc, is_reset ? "true" : "false");
+
+    return is_reset;
+}
+void sw_probe_pm_helper_i(int id, int both_id, bool is_enter, enum sw_pm_action action)
+{
+    struct sw_trace_notifier_data *node = GET_COLLECTOR_NOTIFIER_NODE(id);
+    struct sw_trace_notifier_data *both_node = GET_COLLECTOR_NOTIFIER_NODE(both_id);
+    struct sw_trace_notifier_data *reset_node = GET_COLLECTOR_NOTIFIER_NODE(SW_NOTIFIER_ID_COUNTER_RESET);
+    if (is_enter) {
+        /*
+         * Entering HIBERNATION/SUSPEND
+         */
+        sw_pm_enter_tsc = sw_tscval();
+    } else {
+        /*
+         * Exitting HIBERNATION/SUSPEND
+         */
+        if (sw_is_reset_i() && reset_node) {
+            sw_handle_reset_messages_i(reset_node);
+        }
+    }
+    if (node) {
+        sw_handle_trace_notifier_i(node);
+    }
+    if (both_node) {
+        sw_handle_trace_notifier_i(both_node);
+    }
+    /* Send the suspend-resume notification */
+    sw_send_pm_notification_i(action);
+}
+
+int sw_probe_pm_notifier_i(struct notifier_block *block, unsigned long state, void *dummy)
+{
+    static const struct {
+        enum sw_pm_action action;
+        int node_id;
+        int both_id;
+        bool is_enter;
+    } pm_data[PM_POST_RESTORE] = {
+        [PM_HIBERNATION_PREPARE] = {SW_PM_ACTION_HIBERNATE_ENTER, SW_NOTIFIER_ID_HIBERNATE_ENTER, SW_NOTIFIER_ID_HIBERNATE, true},
+        [PM_POST_HIBERNATION] = {SW_PM_ACTION_HIBERNATE_EXIT, SW_NOTIFIER_ID_HIBERNATE_EXIT, SW_NOTIFIER_ID_HIBERNATE, false},
+        [PM_SUSPEND_PREPARE] = {SW_PM_ACTION_SUSPEND_ENTER, SW_NOTIFIER_ID_SUSPEND_ENTER, SW_NOTIFIER_ID_SUSPEND, true},
+        [PM_POST_SUSPEND] = {SW_PM_ACTION_SUSPEND_EXIT, SW_NOTIFIER_ID_SUSPEND_EXIT, SW_NOTIFIER_ID_SUSPEND, false},
+    };
+    enum sw_pm_action action = pm_data[state].action;
+    if (action != SW_PM_ACTION_NONE) {
+        int node_id = pm_data[state].node_id, both_id = pm_data[state].both_id;
+        bool is_enter = pm_data[state].is_enter;
+        printk(KERN_INFO "DEBUG: state = %lu, node_id = %d\n", state, node_id);
+        sw_probe_pm_helper_i(node_id, both_id, is_enter, action);
+    } else {
+        /* Not supported */
+        pw_pr_error("ERROR: unknown state %lu passed to SWA pm notifier!\n", state);
+    }
+    return NOTIFY_DONE;
+}
+
+void sw_store_topology_change_i(enum cpu_action type, int cpu, int core_id, int pkg_id)
+{
+    struct sw_topology_node *node = sw_kmalloc(sizeof(*node), GFP_ATOMIC);
+    if (!node) {
+        pw_pr_error("couldn't allocate a node for topology change tracking!\n");
+        return;
+    }
+    node->change.timestamp = sw_timestamp();
+    node->change.type = type;
+    node->change.cpu = cpu;
+    node->change.core = core_id;
+    node->change.pkg = pkg_id;
+
+    SW_LIST_ADD(&sw_topology_list, node, list);
+    ++sw_num_topology_entries;
+}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
+int sw_probe_hotplug_notifier_i(struct notifier_block *block, unsigned long action, void *pcpu)
+{
+    unsigned int cpu = (unsigned long)pcpu;
+    unsigned int pkg_id = topology_physical_package_id(cpu);
+    unsigned int core_id = topology_core_id(cpu);
+
+    switch (action) {
+        case CPU_UP_PREPARE:
+        case CPU_UP_PREPARE_FROZEN:
+            /* CPU is coming online -- store top change */
+            sw_store_topology_change_i(SW_CPU_ACTION_ONLINE_PREPARE, cpu, core_id, pkg_id);
+            pw_pr_debug("DEBUG: SoC Watch has cpu %d (phys = %d, core = %d) preparing to come online at tsc = %llu! Current cpu = %d\n", cpu, pkg_id, core_id, sw_timestamp(), RAW_CPU());
+            break;
+        case CPU_ONLINE:
+        case CPU_ONLINE_FROZEN:
+            /* CPU is online -- first store top change then take BEGIN snapshot */
+            sw_store_topology_change_i(SW_CPU_ACTION_ONLINE, cpu, core_id, pkg_id);
+            sw_process_snapshot_on_cpu(SW_WHEN_TYPE_BEGIN, cpu);
+            pw_pr_debug("DEBUG: SoC Watch has cpu %d (phys = %d, core = %d) online at tsc = %llu! Current cpu = %d\n", cpu, pkg_id, core_id, sw_timestamp(), RAW_CPU());
+            break;
+        case CPU_DOWN_PREPARE:
+        case CPU_DOWN_PREPARE_FROZEN:
+            /* CPU is going offline -- take END snapshot */
+            sw_process_snapshot_on_cpu(SW_WHEN_TYPE_END, cpu);
+            pw_pr_debug("DEBUG: SoC Watch has cpu %d preparing to go offline at tsc = %llu! Current cpu = %d\n", cpu, sw_timestamp(), RAW_CPU());
+            break;
+        case CPU_DEAD:
+        case CPU_DEAD_FROZEN:
+            /* CPU is offline -- store top change */
+            sw_store_topology_change_i(SW_CPU_ACTION_OFFLINE, cpu, core_id, pkg_id);
+            pw_pr_debug("DEBUG: SoC Watch has cpu %d offlined at tsc = %llu! Current cpu = %d\n", cpu, sw_timestamp(), RAW_CPU());
+            break;
+        default:
+            break;
+    }
+    return NOTIFY_OK;
+};
+#else
+void sw_probe_cpuhp_helper_i(unsigned int cpu, enum cpu_action action)
+{
+    unsigned int pkg_id = topology_physical_package_id(cpu);
+    unsigned int core_id = topology_core_id(cpu);
+
+    switch (action) {
+        case SW_CPU_ACTION_ONLINE_PREPARE:
+            /* CPU is coming online -- store top change */
+            sw_store_topology_change_i(action, cpu, core_id, pkg_id);
+            break;
+        case SW_CPU_ACTION_ONLINE:
+            /* CPU is online -- first store top change then take BEGIN snapshot */
+            sw_store_topology_change_i(action, cpu, core_id, pkg_id);
+            sw_process_snapshot_on_cpu(SW_WHEN_TYPE_BEGIN, cpu);
+            break;
+        case SW_CPU_ACTION_OFFLINE:
+            /* CPU is preparing to go offline -- take END snapshot then store top change */
+            sw_process_snapshot_on_cpu(SW_WHEN_TYPE_END, cpu);
+            sw_store_topology_change_i(action, cpu, core_id, pkg_id);
+            break;
+        default:
+            break;
+    }
+}
+int sw_probe_cpu_offline_i(unsigned int cpu)
+{
+    printk(KERN_INFO "DEBUG: offline notification for cpu %u at %llu\n", cpu, sw_tscval());
+    sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_OFFLINE);
+    return 0;
+}
+int sw_probe_cpu_online_i(unsigned int cpu)
+{
+    printk(KERN_INFO "DEBUG: online notification for cpu %u at %llu\n", cpu, sw_tscval());
+    sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_ONLINE_PREPARE);
+    sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_ONLINE);
+    return 0;
+}
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
+
+/*
+ * 2. CPUFREQ notifier
+ */
+int sw_probe_cpufreq_notifier_i(struct notifier_block *block, unsigned long state, void *data)
+{
+    struct cpufreq_freqs *freqs = data;
+    static struct sw_trace_notifier_data *node = NULL;
+    int cpu = freqs->cpu;
+
+    if (state == CPUFREQ_PRECHANGE) {
+        pw_pr_debug("CPU %d reports a CPUFREQ_PRECHANGE for target CPU %d at TSC = %llu\n", RAW_CPU(), cpu, sw_timestamp());
+        if (unlikely(node == NULL)) {
+            node = GET_COLLECTOR_NOTIFIER_NODE(SW_NOTIFIER_ID_CPUFREQ);
+            pw_pr_debug("NODE = %p\n", node);
+        }
+        /* Force an atomic context by disabling preemption */
+        get_cpu();
+        DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, cpu, node);
+        put_cpu();
+    }
+    return NOTIFY_DONE;
+}
+/*
+ * 1. TPS.
+ */
+int sw_register_trace_cpu_idle_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, power_start, sw_probe_power_start_i);
+#else // kernel version >= 2.6.38
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, cpu_idle, sw_probe_cpu_idle_i);
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_cpu_idle_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, power_start, sw_probe_power_start_i);
+#else // kernel version >= 2.6.38
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, cpu_idle, sw_probe_cpu_idle_i);
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    return PW_SUCCESS;
+};
+/*
+ * 2. TPF
+ */
+int sw_register_trace_cpu_frequency_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, power_frequency, sw_probe_power_frequency_i);
+#else // kernel version >= 2.6.38
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, cpu_frequency, sw_probe_cpu_frequency_i);
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_cpu_frequency_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, power_frequency, sw_probe_power_frequency_i);
+#else // kernel version >= 2.6.38
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, cpu_frequency, sw_probe_cpu_frequency_i);
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+    return PW_SUCCESS;
+};
+/*
+ * 3. IRQ handler entry
+ */
+int sw_register_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, irq_handler_entry, sw_probe_irq_handler_entry_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, irq_handler_entry, sw_probe_irq_handler_entry_i);
+    return PW_SUCCESS;
+};
+/*
+ * 4. TIMER expire.
+ */
+int sw_register_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, timer_expire_entry, sw_probe_timer_expire_entry_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, timer_expire_entry, sw_probe_timer_expire_entry_i);
+    return PW_SUCCESS;
+};
+/*
+ * 5. HRTIMER expire.
+ */
+int sw_register_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, hrtimer_expire_entry, sw_probe_hrtimer_expire_entry_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, hrtimer_expire_entry, sw_probe_hrtimer_expire_entry_i);
+    return PW_SUCCESS;
+};
+/*
+ * 6. SCHED wakeup
+ */
+int sw_register_trace_sched_wakeup_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_wakeup, sw_probe_sched_wakeup_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_sched_wakeup_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_wakeup, sw_probe_sched_wakeup_i);
+    return PW_SUCCESS;
+};
+/*
+ * 8. PROCESS fork
+ */
+int sw_register_trace_sched_process_fork_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_process_fork, sw_probe_sched_process_fork_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_sched_process_fork_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_process_fork, sw_probe_sched_process_fork_i);
+    return PW_SUCCESS;
+};
+/*
+ * 9. PROCESS exit
+ */
+int sw_register_trace_sched_process_exit_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_process_exit, sw_probe_sched_process_exit_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_sched_process_exit_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_process_exit, sw_probe_sched_process_exit_i);
+    return PW_SUCCESS;
+};
+/*
+ * 10. THERMAL_APIC entry
+ */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+int sw_register_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_entry, sw_probe_thermal_apic_entry_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_entry, sw_probe_thermal_apic_entry_i);
+    return PW_SUCCESS;
+};
+/*
+ * 10. THERMAL_APIC exit
+ */
+int sw_register_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_exit, sw_probe_thermal_apic_exit_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_exit, sw_probe_thermal_apic_exit_i);
+    return PW_SUCCESS;
+};
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+/*
+ * 11. WAKE lock / WAKEUP source activate.
+ */
+#if DO_ANDROID
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+int sw_register_trace_wake_lock_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, wake_lock, sw_probe_wake_lock_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_wake_lock_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wake_lock, sw_probe_wake_lock_i);
+    return PW_SUCCESS;
+};
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+int sw_register_trace_wakeup_source_activate_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_activate, sw_probe_wakeup_source_activate_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_wakeup_source_activate_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_activate, sw_probe_wakeup_source_activate_i);
+    return PW_SUCCESS;
+};
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+/*
+ * 11. WAKE unlock / WAKEUP source deactivate.
+ */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+int sw_register_trace_wake_unlock_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, wake_unlock, sw_probe_wake_unlock_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_wake_unlock_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wake_unlock, sw_probe_wake_unlock_i);
+    return PW_SUCCESS;
+};
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+int sw_register_trace_wakeup_source_deactivate_i(struct sw_trace_notifier_data *node)
+{
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_deactivate, sw_probe_wakeup_source_deactivate_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_wakeup_source_deactivate_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_deactivate, sw_probe_wakeup_source_deactivate_i);
+    return PW_SUCCESS;
+};
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+#endif // DO_ANDROID
+/*
+ * 12. WORKQUEUE execution.
+ */
+int sw_register_trace_workqueue_execution_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,35)
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execution, sw_probe_workqueue_execution_i);
+#else
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execute_start, sw_probe_workqueue_execute_start_i);
+#endif
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_workqueue_execution_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,35)
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execution, sw_probe_workqueue_execution_i);
+#else
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execute_start, sw_probe_workqueue_execute_start_i);
+#endif
+    return PW_SUCCESS;
+};
+/*
+ * 13. SCHED switch
+ */
+int sw_register_trace_sched_switch_i(struct sw_trace_notifier_data *node)
+{
+    /*
+     * Set polling tick time, in jiffies.
+     * Used by the context switch tracepoint to decide
+     * if enough time has elapsed since the last
+     * collection point to read resources again.
+     */
+    {
+        int cpu = 0;
+        for_each_present_cpu(cpu) {
+            *(&per_cpu(sw_pcpu_polling_jiff, cpu)) = jiffies;
+        }
+    }
+    DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_switch, sw_probe_sched_switch_i);
+    return PW_SUCCESS;
+};
+int sw_unregister_trace_sched_switch_i(struct sw_trace_notifier_data *node)
+{
+    DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_switch, sw_probe_sched_switch_i);
+    return PW_SUCCESS;
+};
+/*
+ * Notifier register/unregister functions.
+ */
+/*
+ * 1. SUSPEND notifier.
+ */
+struct notifier_block sw_pm_notifier = {
+    .notifier_call = &sw_probe_pm_notifier_i,
+};
+int sw_register_pm_notifier_i(struct sw_trace_notifier_data *node)
+{
+    register_pm_notifier(&sw_pm_notifier);
+    return PW_SUCCESS;
+};
+int sw_unregister_pm_notifier_i(struct sw_trace_notifier_data *node)
+{
+    unregister_pm_notifier(&sw_pm_notifier);
+    return PW_SUCCESS;
+};
+/*
+ * 2. CPUFREQ notifier.
+ */
+struct notifier_block sw_cpufreq_notifier = {
+    .notifier_call = &sw_probe_cpufreq_notifier_i,
+};
+int sw_register_cpufreq_notifier_i(struct sw_trace_notifier_data *node)
+{
+    cpufreq_register_notifier(&sw_cpufreq_notifier, CPUFREQ_TRANSITION_NOTIFIER);
+    return PW_SUCCESS;
+};
+int sw_unregister_cpufreq_notifier_i(struct sw_trace_notifier_data *node)
+{
+    cpufreq_unregister_notifier(&sw_cpufreq_notifier, CPUFREQ_TRANSITION_NOTIFIER);
+    return PW_SUCCESS;
+};
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
+/*
+ * 3. CPU hot plug notifier.
+ */
+struct notifier_block sw_cpu_hotplug_notifier = {
+    .notifier_call = &sw_probe_hotplug_notifier_i,
+};
+
+int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+    register_hotcpu_notifier(&sw_cpu_hotplug_notifier);
+    return PW_SUCCESS;
+};
+int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+    unregister_hotcpu_notifier(&sw_cpu_hotplug_notifier);
+    return PW_SUCCESS;
+};
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(4,10,0)
+static int sw_cpuhp_state = -1;
+int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+    sw_cpuhp_state = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN, "socwatch:online", &sw_probe_cpu_online_i, &sw_probe_cpu_offline_i);
+    if (sw_cpuhp_state < 0) {
+        pw_pr_error("couldn't register socwatch hotplug callbacks!\n");
+        return -EIO;
+    }
+    return 0;
+};
+int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+    if (sw_cpuhp_state >= 0) {
+        cpuhp_remove_state_nocalls((enum cpuhp_state)sw_cpuhp_state);
+    }
+    return 0;
+};
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
+
+/*
+ * Tracepoint extraction routines.
+ * Required for newer kernels (>=3.15)
+ */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0)
+static void sw_extract_tracepoint_callback(struct tracepoint *tp, void *priv)
+{
+    struct sw_trace_notifier_data *node = NULL;
+    int i=0;
+    int *numStructsFound = (int *)priv;
+    if (*numStructsFound == NUM_VALID_TRACEPOINTS) {
+        /*
+         * We've found all the tracepoints we need.
+         */
+        return;
+    }
+    if (tp) {
+        FOR_EACH_TRACEPOINT_NODE(i, node) {
+            if (node->tp == NULL && node->name) {
+                const char *name = sw_get_trace_notifier_kernel_name(node);
+                if (name && !strcmp(tp->name, name)) {
+                    node->tp = tp;
+                    ++*numStructsFound;
+                    pw_pr_debug("OK, found TP %s\n", tp->name);
+                }
+            }
+        }
+    }
+};
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0)
+
+/*
+ * Retrieve the list of tracepoint structs to use when registering and unregistering
+ * tracepoint handlers.
+ */
+int sw_extract_trace_notifier_providers(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0) && defined(CONFIG_TRACEPOINTS)
+    int numCallbacks = 0;
+    for_each_kernel_tracepoint(&sw_extract_tracepoint_callback, &numCallbacks);
+    /*
+     * Did we get the complete list?
+     */
+    if (numCallbacks != NUM_VALID_TRACEPOINTS) {
+        printk(KERN_WARNING "WARNING: Could NOT find tracepoint structs for some tracepoints!\n");
+    }
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0)
+    return PW_SUCCESS;
+};
+
+void sw_reset_trace_notifier_providers(void)
+{
+    /*
+     * Reset the wakeup flag. Not strictly required if we aren't probing
+     * any of the wakeup tracepoints.
+     */
+    {
+        int cpu = 0;
+        for_each_online_cpu(cpu) {
+            RESET_VALID_WAKEUP_EVENT_COUNTER(cpu);
+        }
+    }
+    /*
+     * Reset the wakeup event flag. Not strictly required if we
+     * aren't probing any of the wakeup tracepoints. Will be reset
+     * in the power_start tracepoint if user requested a c-state
+     * collection.
+     */
+    sw_wakeup_event_flag = true;
+};
+
+void sw_print_trace_notifier_provider_overheads(void)
+{
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_tps_i, "TPS");
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_tpf_i, "TPF");
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_irq_wakeup_i, "IRQ");
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_timer_wakeup_helper_i, "TIMER_EXPIRE");
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_sched_wakeup_i, "SCHED WAKEUP");
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_process_fork_exit_helper_i, "PROCESS FORK/EXIT");
+#if DO_ANDROID
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_wakelock_i, "WAKE LOCK/UNLOCK");
+#endif // DO_ANDROID
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_workqueue_wakeup_helper_i, "WORKQUEUE");
+    PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_sched_switch_helper_i, "SCHED SWITCH");
+};
+/*
+ * Add all trace/notifier providers.
+ */
+int sw_add_trace_notifier_providers(void)
+{
+    struct sw_trace_notifier_data *node = NULL;
+    int i = 0;
+    FOR_EACH_TRACEPOINT_NODE(i, node) {
+        if (sw_register_trace_notify_provider(node)) {
+            pw_pr_error("ERROR: couldn't add a trace provider!\n");
+            return -EIO;
+        }
+    }
+    FOR_EACH_NOTIFIER_NODE(i, node) {
+        if (sw_register_trace_notify_provider(node)) {
+            pw_pr_error("ERROR: couldn't add a notifier provider!\n");
+            return -EIO;
+        }
+    }
+    /*
+     * Add the cpu hot plug notifier.
+     */
+    {
+        if (sw_register_trace_notify_provider(&s_hotplug_notifier_data)) {
+            pw_pr_error("ERROR: couldn't add cpu notifier provider!\n");
+            return -EIO;
+        }
+    }
+    return PW_SUCCESS;
+}
+/*
+ * Remove previously added providers.
+ */
+void sw_remove_trace_notifier_providers(void) { /* NOP */ }
diff --git a/drivers/misc/intel/socwatch/sw_tracepoint_handlers.c b/drivers/misc/intel/socwatch/sw_tracepoint_handlers.c
new file mode 100644
index 000000000000..789c30777780
--- /dev/null
+++ b/drivers/misc/intel/socwatch/sw_tracepoint_handlers.c
@@ -0,0 +1,358 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1906 Fox Drive,
+  Champaign, IL 61820
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include "sw_structs.h"
+#include "sw_defines.h"
+#include "sw_types.h"
+#include "sw_tracepoint_handlers.h"
+#include "sw_trace_notifier_provider.h"
+#include "sw_mem.h"
+
+/* -------------------------------------------------
+ * Data structures and variable definitions.
+ * -------------------------------------------------
+ */
+struct sw_trace_list_node {
+    struct sw_trace_notifier_data *data;
+    int id;
+    SW_LIST_ENTRY(list, sw_trace_list_node);
+};
+static SW_DEFINE_LIST_HEAD(s_trace_list, sw_trace_list_node) = SW_LIST_HEAD_INITIALIZER(s_trace_list);
+static SW_DEFINE_LIST_HEAD(s_notifier_list, sw_trace_list_node) = SW_LIST_HEAD_INITIALIZER(s_notifier_list);
+static int s_trace_idx = -1, s_notifier_idx = -1;
+
+SW_DEFINE_LIST_HEAD(sw_topology_list, sw_topology_node) = SW_LIST_HEAD_INITIALIZER(sw_topology_list);
+size_t sw_num_topology_entries = 0;
+
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+int sw_extract_tracepoints(void)
+{
+    return sw_extract_trace_notifier_providers();
+}
+
+void sw_reset_trace_notifier_lists(void)
+{
+    sw_reset_trace_notifier_providers();
+}
+
+void sw_print_trace_notifier_overheads(void)
+{
+    sw_print_trace_notifier_provider_overheads();
+}
+
+static int sw_for_each_node_i(void *list_head,
+                              int (*func)(struct sw_trace_notifier_data *node, void *priv),
+                              void *priv, bool return_on_error)
+{
+    SW_LIST_HEAD_VAR(sw_trace_list_node) *head = list_head;
+    int retval = PW_SUCCESS;
+    struct sw_trace_list_node *lnode = NULL;
+    SW_LIST_FOR_EACH_ENTRY(lnode, head, list) {
+        if ((*func)(lnode->data, priv)) {
+            retval = -EIO;
+            if (return_on_error) {
+                break;
+            }
+        }
+    }
+    return retval;
+}
+
+int sw_for_each_tracepoint_node(int (*func)(struct sw_trace_notifier_data *node, void *priv), void *priv, bool return_on_error)
+{
+    if (func) {
+        return sw_for_each_node_i(&s_trace_list, func, priv, return_on_error);
+    }
+    return PW_SUCCESS;
+}
+
+int sw_for_each_notifier_node(int (*func)(struct sw_trace_notifier_data *node, void *priv), void *priv, bool return_on_error)
+{
+    if (func) {
+        return sw_for_each_node_i(&s_notifier_list, func, priv, return_on_error);
+    }
+    return PW_SUCCESS;
+}
+
+/*
+ * Retrieve the ID for the corresponding tracepoint/notifier.
+ */
+int sw_get_trace_notifier_id(struct sw_trace_notifier_data *tnode)
+{
+    struct sw_trace_list_node *lnode = NULL;
+    SW_LIST_HEAD_VAR(sw_trace_list_node) *head = (void *)&s_trace_list;
+    if (!tnode) {
+        pw_pr_error("ERROR: cannot get ID for NULL trace/notifier data!\n");
+        return -EIO;
+    }
+    if (!(tnode->type == SW_TRACE_COLLECTOR_TRACEPOINT || tnode->type == SW_TRACE_COLLECTOR_NOTIFIER)) {
+        pw_pr_error("ERROR: cannot get ID for invalid trace/notifier data!\n");
+        return -EIO;
+    }
+    if (!tnode->name || !tnode->name->abstract_name) {
+        pw_pr_error("ERROR: cannot get ID for trace/notifier data without valid name!\n");
+        return -EIO;
+    }
+#ifdef LINUX_VERSION_CODE
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0) && defined(CONFIG_TRACEPOINTS)
+    if (tnode->type == SW_TRACE_COLLECTOR_TRACEPOINT &&
+            tnode->name->kernel_name && !tnode->tp) {
+        /* No tracepoint structure found so no ID possible */
+        return -EIO;
+    }
+#endif
+#endif
+    if (tnode->type == SW_TRACE_COLLECTOR_NOTIFIER) {
+        head = (void *)&s_notifier_list;
+    }
+    SW_LIST_FOR_EACH_ENTRY(lnode, head, list) {
+        struct sw_trace_notifier_data *data = lnode->data;
+        if (!strcmp(data->name->abstract_name, tnode->name->abstract_name)) {
+            return lnode->id;
+        }
+    }
+    return -1;
+}
+/*
+ * Retrieve the "kernel" name for this tracepoint/notifier.
+ */
+const char *sw_get_trace_notifier_kernel_name(struct sw_trace_notifier_data *node)
+{
+    return node->name->kernel_name;
+};
+/*
+ * Retrieve the "abstract" name for this tracepoint/notifier.
+ */
+const char *sw_get_trace_notifier_abstract_name(struct sw_trace_notifier_data *node)
+{
+    return node->name->abstract_name;
+};
+
+/*
+ * Add a single TRACE/NOTIFY provider.
+ */
+int sw_register_trace_notify_provider(struct sw_trace_notifier_data *data)
+{
+    struct sw_trace_list_node *lnode = NULL;
+    if (!data) {
+        pw_pr_error("ERROR: cannot add NULL trace/notifier provider!\n");
+        return -EIO;
+    }
+    if (!(data->type == SW_TRACE_COLLECTOR_TRACEPOINT || data->type == SW_TRACE_COLLECTOR_NOTIFIER)) {
+        pw_pr_error("ERROR: cannot add invalid trace/notifier data!\n");
+        return -EIO;
+    }
+    /*
+     * Kernel name is allowed to be NULL, but abstract name MUST be present!
+     */
+    if (!data->name || !data->name->abstract_name) {
+        pw_pr_error("ERROR: cannot add trace/notifier provider without an abstract name!\n");
+        pw_pr_error("ERROR: data->name = %p\n", data->name);
+        return -EIO;
+    }
+    lnode = sw_kmalloc(sizeof(*lnode), GFP_KERNEL);
+    if (!lnode) {
+        pw_pr_error("ERROR: couldn't allocate a list node when adding a trace/notifier provider!\n");
+        return -ENOMEM;
+    }
+    lnode->data = data;
+    SW_LIST_ENTRY_INIT(lnode, list);
+    if (data->type == SW_TRACE_COLLECTOR_TRACEPOINT) {
+        lnode->id = ++s_trace_idx;
+        SW_LIST_ADD(&s_trace_list, lnode, list);
+    } else {
+        lnode->id = ++s_notifier_idx;
+        SW_LIST_ADD(&s_notifier_list, lnode, list);
+    }
+    return PW_SUCCESS;
+}
+/*
+ * Add all TRACE/NOTIFY providers.
+ */
+int sw_add_trace_notify(void)
+{
+    return sw_add_trace_notifier_providers();
+}
+
+static void sw_free_trace_notifier_list_i(void *list_head)
+{
+    SW_LIST_HEAD_VAR(sw_trace_list_node) *head = list_head;
+    while (!SW_LIST_EMPTY(head)) {
+        struct sw_trace_list_node *lnode = SW_LIST_GET_HEAD_ENTRY(head, sw_trace_list_node, list);
+        SW_LIST_UNLINK(lnode, list);
+        sw_kfree(lnode);
+    }
+}
+/*
+ * Remove TRACE/NOTIFY providers.
+ */
+void sw_remove_trace_notify(void)
+{
+    /*
+     * Free all nodes.
+     */
+    sw_free_trace_notifier_list_i(&s_trace_list);
+    sw_free_trace_notifier_list_i(&s_notifier_list);
+    /*
+     * Call our providers to deallocate resources.
+     */
+    sw_remove_trace_notifier_providers();
+    /*
+     * Clear out the topology list
+     */
+    sw_clear_topology_list();
+}
+
+#define REG_FLAG (void *)1
+#define UNREG_FLAG (void *)2
+static int sw_reg_unreg_node_i(struct sw_trace_notifier_data *node, void *is_reg)
+{
+    if (is_reg == REG_FLAG) {
+        /*
+         * Do we have anything to collect?
+         * Update: or were we asked to always register?
+         */
+        if (SW_LIST_EMPTY(&node->list) && !node->always_register) {
+            return PW_SUCCESS;
+        }
+        /*
+         * Sanity: ensure we have a register AND an unregister function before proceeding!
+         */
+        if (node->probe_register == NULL || node->probe_unregister == NULL) {
+            pw_pr_debug("WARNING: invalid trace/notifier register/unregister function for %s\n", sw_get_trace_notifier_kernel_name(node));
+            /*
+             * Don't flag this as an error -- some socwatch trace providers don't have a
+             * register/unregister function
+             */
+            return PW_SUCCESS;
+        }
+        if ((*node->probe_register)(node)) {
+            return -EIO;
+        }
+        node->was_registered = true;
+        return PW_SUCCESS;
+    } else if (is_reg == UNREG_FLAG) {
+        if (node->was_registered) {
+            /*
+             * No need to check for validity of probe unregister function -- 'sw_register_notifiers_i()'
+             * would already have done so!
+             */
+            WARN_ON((*node->probe_unregister)(node));
+            node->was_registered = false;
+            pw_pr_debug("OK, unregistered trace/notifier for %s\n", sw_get_trace_notifier_kernel_name(node));
+        }
+        return PW_SUCCESS;
+    }
+    pw_pr_error("ERROR: invalid reg/unreg flag value 0x%lx\n", (unsigned long)is_reg);
+    return -EIO;
+}
+/*
+ * Register all required tracepoints and notifiers.
+ */
+int sw_register_trace_notifiers(void)
+{
+    /*
+     * First, the tracepoints.
+     */
+    if (sw_for_each_tracepoint_node(&sw_reg_unreg_node_i, REG_FLAG, true /* return on error */)) {
+        pw_pr_error("ERROR registering some tracepoints\n");
+        return -EIO;
+    }
+    /*
+     * And then the notifiers.
+     */
+    if (sw_for_each_notifier_node(&sw_reg_unreg_node_i, REG_FLAG, true /* return on error */)) {
+        pw_pr_error("ERROR registering some tracepoints\n");
+        return -EIO;
+    }
+    return PW_SUCCESS;
+};
+/*
+ * Unregister all previously registered tracepoints and notifiers.
+ */
+int sw_unregister_trace_notifiers(void)
+{
+    /*
+     * First, the notifiers.
+     */
+    if (sw_for_each_notifier_node(&sw_reg_unreg_node_i, UNREG_FLAG, true /* return on error */)) {
+        pw_pr_error("ERROR registering some tracepoints\n");
+        return -EIO;
+    }
+    /*
+     * And then the tracepoints.
+     */
+    if (sw_for_each_tracepoint_node(&sw_reg_unreg_node_i, UNREG_FLAG, true /* return on error */)) {
+        pw_pr_error("ERROR registering some tracepoints\n");
+        return -EIO;
+    }
+    return PW_SUCCESS;
+};
+
+void sw_clear_topology_list(void)
+{
+    SW_LIST_HEAD_VAR(sw_topology_node) *head = &sw_topology_list;
+    while (!SW_LIST_EMPTY(head)) {
+        struct sw_topology_node *lnode = SW_LIST_GET_HEAD_ENTRY(head, sw_topology_node, list);
+        pw_pr_debug("Clearing topology node for cpu %d\n", lnode->change.cpu);
+        SW_LIST_UNLINK(lnode, list);
+        sw_kfree(lnode);
+    }
+    sw_num_topology_entries  = 0;
+}
-- 
2.18.0

